# Explainable-Natural-Language-Inference
Natural Language Inference with Transformer Ensembles and Explainability Techniques


Natural Language Inference (NLI) is a fundamental and quite challenging task in natural language processing, requiring efficient methods that are able to determine whether given hypotheses derive from given premises. In this paper, we apply explainability techniques to natural language inference methods as a mean to illustrate the decision-making procedure of methods. First, we investigate the performance and generalization capabilities of several Transformer-based models, including BERT, ALBERT, RoBERTa, and DeBERTa, across widely-used datasets like SNLI, GLUE Benchmark, and ANLI. Then, we employ stacking en-semble techniques, to leverage the strengths of multiple models and improve inference per-formance. Experimental results demonstrate significant improvements of the ensemble models in inference tasks, highlighting the effectiveness of stacking. Specifically, our best-performing ensemble models surpassed best performing individual transformer by 5.31% in accuracy on MNLI-m and MNLI-mm tasks. After that, we implement LIME and SHAP explainability techniques to shed light on the decision-making of the transformer models indicating how specific words and contextual information are utilized in the transformer inferences proce-dures. The results indicate that the model properly leverage contextual information and indi-vidual words to make decisions but, in some cases, find difficulties in inference scenarios with metaphorical connections which require deeper inferential reasoning.
