{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2933,"sourceType":"datasetVersion","datasetId":1670},{"sourceId":4548821,"sourceType":"datasetVersion","datasetId":2655798},{"sourceId":4550791,"sourceType":"datasetVersion","datasetId":2656775},{"sourceId":8083662,"sourceType":"datasetVersion","datasetId":4771616},{"sourceId":8083668,"sourceType":"datasetVersion","datasetId":4771621},{"sourceId":8083678,"sourceType":"datasetVersion","datasetId":4771629},{"sourceId":8084913,"sourceType":"datasetVersion","datasetId":4772442}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Load the SNLI test data (including true labels)\nsnli_test_path = \"/kaggle/input/stanford-natural-language-inference-corpus/snli_1.0_test.csv\"\nsnli_test_df = pd.read_csv(snli_test_path)\n\n# Define file paths for SNLI prediction files\nsnli_predictions_paths = {\n    \"deberta\": \"/kaggle/input/deberta-nli/deberta_snli_predictions.csv\",\n    \"roberta\": \"/kaggle/input/roberta/roberta_snli_predictions.csv\",\n    \"albert\": \"/kaggle/input/albert/albert_snli_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_snli = \"/kaggle/working/combined_snli_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_snli_df = pd.DataFrame(columns=columns)\n\nlabel_mapping = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n\n# Load and merge the predictions\nfor model, path in snli_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_snli_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_snli_df['True_Label'] = snli_test_df['gold_label'].map(label_mapping)\n\n# Convert True_Label to integer type\ncombined_snli_df['True_Label'] = combined_snli_df['True_Label'].astype('Int64')\n\n# Save the combined DataFrame to CSV\ncombined_snli_df.to_csv(output_csv_path_snli, index=False)\n\nprint(f\"Combined SNLI predictions with true labels saved to {output_csv_path_snli}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T12:14:52.937546Z","iopub.execute_input":"2024-08-20T12:14:52.937969Z","iopub.status.idle":"2024-08-20T12:14:53.219695Z","shell.execute_reply.started":"2024-08-20T12:14:52.937936Z","shell.execute_reply":"2024-08-20T12:14:53.218704Z"},"trusted":true},"execution_count":90,"outputs":[{"name":"stdout","text":"Combined SNLI predictions with true labels saved to /kaggle/working/combined_snli_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_snli_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.221453Z","iopub.execute_input":"2024-08-20T12:14:53.221747Z","iopub.status.idle":"2024-08-20T12:14:53.238615Z","shell.execute_reply.started":"2024-08-20T12:14:53.221720Z","shell.execute_reply":"2024-08-20T12:14:53.237498Z"},"trusted":true},"execution_count":91,"outputs":[{"execution_count":91,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.034767         0.962592               0.002641   \n1            0.001921         0.319032               0.679047   \n2            0.998783         0.000764               0.000453   \n3            0.001001         0.997708               0.001291   \n4            0.001080         0.301363               0.697557   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.012451         0.927093               0.060457   \n1            0.752766         0.242251               0.004983   \n2            0.000254         0.004494               0.995253   \n3            0.005844         0.990736               0.003419   \n4            0.278348         0.718575               0.003076   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.008653        0.947434              0.043913           1  \n1           0.740332        0.256434              0.003235           0  \n2           0.004677        0.060481              0.934843           2  \n3           0.034056        0.956687              0.009257           1  \n4           0.498761        0.499270              0.001969           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.034767</td>\n      <td>0.962592</td>\n      <td>0.002641</td>\n      <td>0.012451</td>\n      <td>0.927093</td>\n      <td>0.060457</td>\n      <td>0.008653</td>\n      <td>0.947434</td>\n      <td>0.043913</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001921</td>\n      <td>0.319032</td>\n      <td>0.679047</td>\n      <td>0.752766</td>\n      <td>0.242251</td>\n      <td>0.004983</td>\n      <td>0.740332</td>\n      <td>0.256434</td>\n      <td>0.003235</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.998783</td>\n      <td>0.000764</td>\n      <td>0.000453</td>\n      <td>0.000254</td>\n      <td>0.004494</td>\n      <td>0.995253</td>\n      <td>0.004677</td>\n      <td>0.060481</td>\n      <td>0.934843</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001001</td>\n      <td>0.997708</td>\n      <td>0.001291</td>\n      <td>0.005844</td>\n      <td>0.990736</td>\n      <td>0.003419</td>\n      <td>0.034056</td>\n      <td>0.956687</td>\n      <td>0.009257</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.001080</td>\n      <td>0.301363</td>\n      <td>0.697557</td>\n      <td>0.278348</td>\n      <td>0.718575</td>\n      <td>0.003076</td>\n      <td>0.498761</td>\n      <td>0.499270</td>\n      <td>0.001969</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the ANLI Round 1 test data (including true labels)\nmnli_matched_test_path = \"/kaggle/input/nli-dataset-for-sentence-understanding/mnli_validation_matched.csv\"\nmnli_matched_test_df = pd.read_csv(mnli_matched_test_path)\n\n# Define file paths for ANLI Round 1 prediction files\nmnli_matched_predictions_paths = {\n    \"deberta\": \"/kaggle/input/validation/deberta_mnli_matched_val_predictions.csv\",\n    \"roberta\": \"/kaggle/input/validation/roberta_mnli_matched_val_predictions.csv\",\n    \"albert\": \"/kaggle/input/validation/albert_mnli_matched_val_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_mnli_matched = \"/kaggle/working/combined_mnli_matched_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_mnli_matched_df = pd.DataFrame(columns=columns)\n\n# Load and merge the predictions\nfor model, path in mnli_matched_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_mnli_matched_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_mnli_matched_df['True_Label'] = mnli_matched_test_df['label']\n\n# Save the combined DataFrame to CSV\ncombined_mnli_matched_df.to_csv(output_csv_path_mnli_matched, index=False)\n\nprint(f\"Combined MNLI-matched predictions with true labels saved to {output_csv_path_mnli_matched}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.239939Z","iopub.execute_input":"2024-08-20T12:14:53.240309Z","iopub.status.idle":"2024-08-20T12:14:53.438155Z","shell.execute_reply.started":"2024-08-20T12:14:53.240271Z","shell.execute_reply":"2024-08-20T12:14:53.437073Z"},"trusted":true},"execution_count":92,"outputs":[{"name":"stdout","text":"Combined MNLI-matched predictions with true labels saved to /kaggle/working/combined_mnli_matched_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_mnli_matched_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.440490Z","iopub.execute_input":"2024-08-20T12:14:53.440798Z","iopub.status.idle":"2024-08-20T12:14:53.456894Z","shell.execute_reply.started":"2024-08-20T12:14:53.440771Z","shell.execute_reply":"2024-08-20T12:14:53.455916Z"},"trusted":true},"execution_count":93,"outputs":[{"execution_count":93,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.005164         0.993364               0.001472   \n1            0.999153         0.000526               0.000321   \n2            0.000989         0.044792               0.954219   \n3            0.994965         0.004808               0.000228   \n4            0.999657         0.000220               0.000123   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.017844         0.950246               0.031909   \n1            0.001413         0.002030               0.996557   \n2            0.954781         0.042249               0.002970   \n3            0.000343         0.003511               0.996146   \n4            0.000079         0.000496               0.999425   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.010844        0.983012              0.006144           1  \n1           0.005388        0.007536              0.987076           2  \n2           0.853862        0.143483              0.002655           0  \n3           0.004128        0.070757              0.925115           2  \n4           0.003864        0.029262              0.966875           2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.005164</td>\n      <td>0.993364</td>\n      <td>0.001472</td>\n      <td>0.017844</td>\n      <td>0.950246</td>\n      <td>0.031909</td>\n      <td>0.010844</td>\n      <td>0.983012</td>\n      <td>0.006144</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.999153</td>\n      <td>0.000526</td>\n      <td>0.000321</td>\n      <td>0.001413</td>\n      <td>0.002030</td>\n      <td>0.996557</td>\n      <td>0.005388</td>\n      <td>0.007536</td>\n      <td>0.987076</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000989</td>\n      <td>0.044792</td>\n      <td>0.954219</td>\n      <td>0.954781</td>\n      <td>0.042249</td>\n      <td>0.002970</td>\n      <td>0.853862</td>\n      <td>0.143483</td>\n      <td>0.002655</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.994965</td>\n      <td>0.004808</td>\n      <td>0.000228</td>\n      <td>0.000343</td>\n      <td>0.003511</td>\n      <td>0.996146</td>\n      <td>0.004128</td>\n      <td>0.070757</td>\n      <td>0.925115</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.999657</td>\n      <td>0.000220</td>\n      <td>0.000123</td>\n      <td>0.000079</td>\n      <td>0.000496</td>\n      <td>0.999425</td>\n      <td>0.003864</td>\n      <td>0.029262</td>\n      <td>0.966875</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the ANLI Round 1 test data (including true labels)\nmnli_mismatched_test_path = \"/kaggle/input/nli-dataset-for-sentence-understanding/mnli_validation_mismatched.csv\"\nmnli_mismatched_test_df = pd.read_csv(mnli_mismatched_test_path)\n\n# Define file paths for ANLI Round 1 prediction files\nmnli_mismatched_predictions_paths = {\n    \"deberta\": \"/kaggle/input/validation/deberta_mnli_mismatched_val_predictions.csv\",\n    \"roberta\": \"/kaggle/input/validation/roberta_mnli_mismatched_val_predictions.csv\",\n    \"albert\": \"/kaggle/input/validation/albert_mnli_mismatched_val_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_mnli_mismatched = \"/kaggle/working/combined_mnli_mismatched_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_mnli_mismatched_df = pd.DataFrame(columns=columns)\n\n# Load and merge the predictions\nfor model, path in mnli_mismatched_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_mnli_mismatched_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_mnli_mismatched_df['True_Label'] = mnli_mismatched_test_df['label']\n\n# Save the combined DataFrame to CSV\ncombined_mnli_mismatched_df.to_csv(output_csv_path_mnli_mismatched, index=False)\n\nprint(f\"Combined MNLI-mismatched predictions with true labels saved to {output_csv_path_mnli_mismatched}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.458400Z","iopub.execute_input":"2024-08-20T12:14:53.458758Z","iopub.status.idle":"2024-08-20T12:14:53.657816Z","shell.execute_reply.started":"2024-08-20T12:14:53.458729Z","shell.execute_reply":"2024-08-20T12:14:53.656827Z"},"trusted":true},"execution_count":94,"outputs":[{"name":"stdout","text":"Combined MNLI-mismatched predictions with true labels saved to /kaggle/working/combined_mnli_mismatched_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_mnli_mismatched_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.659122Z","iopub.execute_input":"2024-08-20T12:14:53.659497Z","iopub.status.idle":"2024-08-20T12:14:53.674860Z","shell.execute_reply.started":"2024-08-20T12:14:53.659460Z","shell.execute_reply":"2024-08-20T12:14:53.673804Z"},"trusted":true},"execution_count":95,"outputs":[{"execution_count":95,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.999667         0.000160               0.000173   \n1            0.998119         0.000962               0.000919   \n2            0.000552         0.004809               0.994639   \n3            0.827653         0.171961               0.000386   \n4            0.000292         0.002875               0.996833   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.000068         0.000402               0.999529   \n1            0.000183         0.001511               0.998306   \n2            0.986062         0.012020               0.001918   \n3            0.000478         0.270953               0.728569   \n4            0.975167         0.021904               0.002929   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.000894        0.003787              0.995318           2  \n1           0.006421        0.010224              0.983355           2  \n2           0.975041        0.023354              0.001605           0  \n3           0.001722        0.796122              0.202156           2  \n4           0.965952        0.032748              0.001300           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.999667</td>\n      <td>0.000160</td>\n      <td>0.000173</td>\n      <td>0.000068</td>\n      <td>0.000402</td>\n      <td>0.999529</td>\n      <td>0.000894</td>\n      <td>0.003787</td>\n      <td>0.995318</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.998119</td>\n      <td>0.000962</td>\n      <td>0.000919</td>\n      <td>0.000183</td>\n      <td>0.001511</td>\n      <td>0.998306</td>\n      <td>0.006421</td>\n      <td>0.010224</td>\n      <td>0.983355</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000552</td>\n      <td>0.004809</td>\n      <td>0.994639</td>\n      <td>0.986062</td>\n      <td>0.012020</td>\n      <td>0.001918</td>\n      <td>0.975041</td>\n      <td>0.023354</td>\n      <td>0.001605</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.827653</td>\n      <td>0.171961</td>\n      <td>0.000386</td>\n      <td>0.000478</td>\n      <td>0.270953</td>\n      <td>0.728569</td>\n      <td>0.001722</td>\n      <td>0.796122</td>\n      <td>0.202156</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000292</td>\n      <td>0.002875</td>\n      <td>0.996833</td>\n      <td>0.975167</td>\n      <td>0.021904</td>\n      <td>0.002929</td>\n      <td>0.965952</td>\n      <td>0.032748</td>\n      <td>0.001300</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the ANLI Round 1 test data (including true labels)\nanli_r1_test_path = \"/kaggle/input/anli-a-large-scale-nli-benchmark-dataset/test_r1.csv\"\nanli_r1_test_df = pd.read_csv(anli_r1_test_path)\n\n# Define file paths for ANLI Round 1 prediction files\nanli_r1_predictions_paths = {\n    \"deberta\": \"/kaggle/input/deberta-nli/deberta_anli_r1_predictions.csv\",\n    \"roberta\": \"/kaggle/input/roberta/roberta_anli_r1_predictions.csv\",\n    \"albert\": \"/kaggle/input/albert/albert_anli_r1_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_anli_r1 = \"/kaggle/working/combined_anli_r1_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_anli_r1_df = pd.DataFrame(columns=columns)\n\n# Load and merge the predictions\nfor model, path in anli_r1_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_anli_r1_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_anli_r1_df['True_Label'] = anli_r1_test_df['label']\n\n# Save the combined DataFrame to CSV\ncombined_anli_r1_df.to_csv(output_csv_path_anli_r1, index=False)\n\nprint(f\"Combined ANLI Round 1 predictions with true labels saved to {output_csv_path_anli_r1}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.676178Z","iopub.execute_input":"2024-08-20T12:14:53.676510Z","iopub.status.idle":"2024-08-20T12:14:53.725603Z","shell.execute_reply.started":"2024-08-20T12:14:53.676474Z","shell.execute_reply":"2024-08-20T12:14:53.724581Z"},"trusted":true},"execution_count":96,"outputs":[{"name":"stdout","text":"Combined ANLI Round 1 predictions with true labels saved to /kaggle/working/combined_anli_r1_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_anli_r1_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.726950Z","iopub.execute_input":"2024-08-20T12:14:53.727627Z","iopub.status.idle":"2024-08-20T12:14:53.742999Z","shell.execute_reply.started":"2024-08-20T12:14:53.727581Z","shell.execute_reply":"2024-08-20T12:14:53.741971Z"},"trusted":true},"execution_count":97,"outputs":[{"execution_count":97,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.015388         0.976305               0.008307   \n1            0.224603         0.501549               0.273848   \n2            0.006642         0.976690               0.016669   \n3            0.966494         0.032235               0.001272   \n4            0.880736         0.028293               0.090971   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.996714         0.000376               0.002910   \n1            0.875720         0.000724               0.123556   \n2            0.999484         0.000330               0.000186   \n3            0.000686         0.998181               0.001133   \n4            0.000378         0.000197               0.999425   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.322974        0.667628              0.009398           0  \n1           0.998526        0.000604              0.000869           0  \n2           0.783352        0.212241              0.004407           0  \n3           0.002134        0.989523              0.008343           1  \n4           0.023283        0.013253              0.963464           2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.015388</td>\n      <td>0.976305</td>\n      <td>0.008307</td>\n      <td>0.996714</td>\n      <td>0.000376</td>\n      <td>0.002910</td>\n      <td>0.322974</td>\n      <td>0.667628</td>\n      <td>0.009398</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.224603</td>\n      <td>0.501549</td>\n      <td>0.273848</td>\n      <td>0.875720</td>\n      <td>0.000724</td>\n      <td>0.123556</td>\n      <td>0.998526</td>\n      <td>0.000604</td>\n      <td>0.000869</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.006642</td>\n      <td>0.976690</td>\n      <td>0.016669</td>\n      <td>0.999484</td>\n      <td>0.000330</td>\n      <td>0.000186</td>\n      <td>0.783352</td>\n      <td>0.212241</td>\n      <td>0.004407</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.966494</td>\n      <td>0.032235</td>\n      <td>0.001272</td>\n      <td>0.000686</td>\n      <td>0.998181</td>\n      <td>0.001133</td>\n      <td>0.002134</td>\n      <td>0.989523</td>\n      <td>0.008343</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.880736</td>\n      <td>0.028293</td>\n      <td>0.090971</td>\n      <td>0.000378</td>\n      <td>0.000197</td>\n      <td>0.999425</td>\n      <td>0.023283</td>\n      <td>0.013253</td>\n      <td>0.963464</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the ANLI Round 2 test data (including true labels)\nanli_r2_test_path = \"/kaggle/input/anli-a-large-scale-nli-benchmark-dataset/test_r2.csv\"\nanli_r2_test_df = pd.read_csv(anli_r2_test_path)\n\n# Define file paths for ANLI Round 2 prediction files\nanli_r2_predictions_paths = {\n    \"deberta\": \"/kaggle/input/deberta-nli/deberta_anli_r2_predictions.csv\",\n    \"roberta\": \"/kaggle/input/roberta/roberta_anli_r2_predictions.csv\",\n    \"albert\": \"/kaggle/input/albert/albert_anli_r2_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_anli_r2 = \"/kaggle/working/combined_anli_r2_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_anli_r2_df = pd.DataFrame(columns=columns)\n\n# Load and merge the predictions\nfor model, path in anli_r2_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_anli_r2_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_anli_r2_df['True_Label'] = anli_r2_test_df['label']\n\n# Save the combined DataFrame to CSV\ncombined_anli_r2_df.to_csv(output_csv_path_anli_r2, index=False)\n\nprint(f\"Combined ANLI Round 2 predictions with true labels saved to {output_csv_path_anli_r2}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.744250Z","iopub.execute_input":"2024-08-20T12:14:53.744588Z","iopub.status.idle":"2024-08-20T12:14:53.792276Z","shell.execute_reply.started":"2024-08-20T12:14:53.744550Z","shell.execute_reply":"2024-08-20T12:14:53.791322Z"},"trusted":true},"execution_count":98,"outputs":[{"name":"stdout","text":"Combined ANLI Round 2 predictions with true labels saved to /kaggle/working/combined_anli_r2_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_anli_r2_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.795900Z","iopub.execute_input":"2024-08-20T12:14:53.796270Z","iopub.status.idle":"2024-08-20T12:14:53.810973Z","shell.execute_reply.started":"2024-08-20T12:14:53.796245Z","shell.execute_reply":"2024-08-20T12:14:53.809823Z"},"trusted":true},"execution_count":99,"outputs":[{"execution_count":99,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.001309         0.029617               0.969075   \n1            0.724144         0.273676               0.002180   \n2            0.071604         0.917894               0.010503   \n3            0.066162         0.929179               0.004659   \n4            0.906199         0.089873               0.003928   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.999506         0.000264               0.000230   \n1            0.026951         0.054230               0.918819   \n2            0.001282         0.998108               0.000610   \n3            0.007091         0.992694               0.000215   \n4            0.006259         0.989432               0.004309   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.863365        0.133388              0.003246           0  \n1           0.072900        0.904344              0.022756           1  \n2           0.027402        0.972218              0.000380           0  \n3           0.632171        0.365194              0.002635           1  \n4           0.064109        0.234642              0.701249           2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.001309</td>\n      <td>0.029617</td>\n      <td>0.969075</td>\n      <td>0.999506</td>\n      <td>0.000264</td>\n      <td>0.000230</td>\n      <td>0.863365</td>\n      <td>0.133388</td>\n      <td>0.003246</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.724144</td>\n      <td>0.273676</td>\n      <td>0.002180</td>\n      <td>0.026951</td>\n      <td>0.054230</td>\n      <td>0.918819</td>\n      <td>0.072900</td>\n      <td>0.904344</td>\n      <td>0.022756</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.071604</td>\n      <td>0.917894</td>\n      <td>0.010503</td>\n      <td>0.001282</td>\n      <td>0.998108</td>\n      <td>0.000610</td>\n      <td>0.027402</td>\n      <td>0.972218</td>\n      <td>0.000380</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.066162</td>\n      <td>0.929179</td>\n      <td>0.004659</td>\n      <td>0.007091</td>\n      <td>0.992694</td>\n      <td>0.000215</td>\n      <td>0.632171</td>\n      <td>0.365194</td>\n      <td>0.002635</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.906199</td>\n      <td>0.089873</td>\n      <td>0.003928</td>\n      <td>0.006259</td>\n      <td>0.989432</td>\n      <td>0.004309</td>\n      <td>0.064109</td>\n      <td>0.234642</td>\n      <td>0.701249</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the ANLI Round 3 test data (including true labels)\nanli_r3_test_path = \"/kaggle/input/anli-a-large-scale-nli-benchmark-dataset/test_r3.csv\"\nanli_r3_test_df = pd.read_csv(anli_r3_test_path)\n\n# Define file paths for ANLI Round 2 prediction files\nanli_r3_predictions_paths = {\n    \"deberta\": \"/kaggle/input/deberta-nli/deberta_anli_r3_predictions.csv\",\n    \"roberta\": \"/kaggle/input/roberta/roberta_anli_r3_predictions.csv\",\n    \"albert\": \"/kaggle/input/albert/albert_anli_r3_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_anli_r3 = \"/kaggle/working/combined_anli_r3_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_anli_r3_df = pd.DataFrame(columns=columns)\n\n# Load and merge the predictions\nfor model, path in anli_r3_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_anli_r3_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_anli_r3_df['True_Label'] = anli_r3_test_df['label']\n\n# Save the combined DataFrame to CSV\ncombined_anli_r3_df.to_csv(output_csv_path_anli_r3, index=False)\n\nprint(f\"Combined ANLI Round 3 predictions with true labels saved to {output_csv_path_anli_r3}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.812139Z","iopub.execute_input":"2024-08-20T12:14:53.812464Z","iopub.status.idle":"2024-08-20T12:14:53.863919Z","shell.execute_reply.started":"2024-08-20T12:14:53.812437Z","shell.execute_reply":"2024-08-20T12:14:53.862911Z"},"trusted":true},"execution_count":100,"outputs":[{"name":"stdout","text":"Combined ANLI Round 3 predictions with true labels saved to /kaggle/working/combined_anli_r3_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_anli_r3_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.865201Z","iopub.execute_input":"2024-08-20T12:14:53.865510Z","iopub.status.idle":"2024-08-20T12:14:53.880951Z","shell.execute_reply.started":"2024-08-20T12:14:53.865483Z","shell.execute_reply":"2024-08-20T12:14:53.879799Z"},"trusted":true},"execution_count":101,"outputs":[{"execution_count":101,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.005921         0.960529               0.033551   \n1            0.009586         0.934714               0.055700   \n2            0.003428         0.976393               0.020179   \n3            0.004633         0.023985               0.971382   \n4            0.017428         0.633695               0.348877   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.022959         0.976533               0.000509   \n1            0.999611         0.000205               0.000185   \n2            0.002020         0.997897               0.000083   \n3            0.974441         0.024459               0.001100   \n4            0.984416         0.011166               0.004419   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.001848        0.998084              0.000067           0  \n1           0.951772        0.048075              0.000153           0  \n2           0.001014        0.998984              0.000002           0  \n3           0.996749        0.000989              0.002262           0  \n4           0.000518        0.128416              0.871066           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.005921</td>\n      <td>0.960529</td>\n      <td>0.033551</td>\n      <td>0.022959</td>\n      <td>0.976533</td>\n      <td>0.000509</td>\n      <td>0.001848</td>\n      <td>0.998084</td>\n      <td>0.000067</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.009586</td>\n      <td>0.934714</td>\n      <td>0.055700</td>\n      <td>0.999611</td>\n      <td>0.000205</td>\n      <td>0.000185</td>\n      <td>0.951772</td>\n      <td>0.048075</td>\n      <td>0.000153</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.003428</td>\n      <td>0.976393</td>\n      <td>0.020179</td>\n      <td>0.002020</td>\n      <td>0.997897</td>\n      <td>0.000083</td>\n      <td>0.001014</td>\n      <td>0.998984</td>\n      <td>0.000002</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004633</td>\n      <td>0.023985</td>\n      <td>0.971382</td>\n      <td>0.974441</td>\n      <td>0.024459</td>\n      <td>0.001100</td>\n      <td>0.996749</td>\n      <td>0.000989</td>\n      <td>0.002262</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.017428</td>\n      <td>0.633695</td>\n      <td>0.348877</td>\n      <td>0.984416</td>\n      <td>0.011166</td>\n      <td>0.004419</td>\n      <td>0.000518</td>\n      <td>0.128416</td>\n      <td>0.871066</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Check for missing values\nmissing_values_anli1 = combined_anli_r1_df.isnull().sum()\n\nmissing_values_anli2 = combined_anli_r2_df.isnull().sum()\n\nmissing_values_anli3 = combined_anli_r3_df.isnull().sum()\n\nmissing_values_snli = combined_snli_df.isnull().sum()\n\nmissing_values_mnli_matched = combined_mnli_matched_df.isnull().sum()\n\nmissing_values_mnli_mismatched = combined_mnli_mismatched_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.882304Z","iopub.execute_input":"2024-08-20T12:14:53.882684Z","iopub.status.idle":"2024-08-20T12:14:53.895729Z","shell.execute_reply.started":"2024-08-20T12:14:53.882643Z","shell.execute_reply":"2024-08-20T12:14:53.894773Z"},"trusted":true},"execution_count":102,"outputs":[]},{"cell_type":"code","source":"missing_values_anli1","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.897098Z","iopub.execute_input":"2024-08-20T12:14:53.897856Z","iopub.status.idle":"2024-08-20T12:14:53.909073Z","shell.execute_reply.started":"2024-08-20T12:14:53.897821Z","shell.execute_reply":"2024-08-20T12:14:53.907873Z"},"trusted":true},"execution_count":103,"outputs":[{"execution_count":103,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"missing_values_anli2","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.910445Z","iopub.execute_input":"2024-08-20T12:14:53.911112Z","iopub.status.idle":"2024-08-20T12:14:53.919599Z","shell.execute_reply.started":"2024-08-20T12:14:53.911074Z","shell.execute_reply":"2024-08-20T12:14:53.918715Z"},"trusted":true},"execution_count":104,"outputs":[{"execution_count":104,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"missing_values_anli3","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.921206Z","iopub.execute_input":"2024-08-20T12:14:53.921805Z","iopub.status.idle":"2024-08-20T12:14:53.931012Z","shell.execute_reply.started":"2024-08-20T12:14:53.921772Z","shell.execute_reply":"2024-08-20T12:14:53.930087Z"},"trusted":true},"execution_count":105,"outputs":[{"execution_count":105,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"missing_values_snli","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.932223Z","iopub.execute_input":"2024-08-20T12:14:53.932515Z","iopub.status.idle":"2024-08-20T12:14:53.942096Z","shell.execute_reply.started":"2024-08-20T12:14:53.932490Z","shell.execute_reply":"2024-08-20T12:14:53.941099Z"},"trusted":true},"execution_count":106,"outputs":[{"execution_count":106,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment         0\nDeberta_Neutral            0\nDeberta_Contradiction      0\nRoberta_Entailment         0\nRoberta_Neutral            0\nRoberta_Contradiction      0\nAlbert_Entailment          0\nAlbert_Neutral             0\nAlbert_Contradiction       0\nTrue_Label               176\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"missing_values_mnli_matched","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.943596Z","iopub.execute_input":"2024-08-20T12:14:53.944230Z","iopub.status.idle":"2024-08-20T12:14:53.954735Z","shell.execute_reply.started":"2024-08-20T12:14:53.944194Z","shell.execute_reply":"2024-08-20T12:14:53.953764Z"},"trusted":true},"execution_count":107,"outputs":[{"execution_count":107,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"missing_values_mnli_mismatched","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.956197Z","iopub.execute_input":"2024-08-20T12:14:53.956551Z","iopub.status.idle":"2024-08-20T12:14:53.966955Z","shell.execute_reply.started":"2024-08-20T12:14:53.956518Z","shell.execute_reply":"2024-08-20T12:14:53.966065Z"},"trusted":true},"execution_count":108,"outputs":[{"execution_count":108,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"combined_snli_df.dropna(subset=['True_Label'], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.968077Z","iopub.execute_input":"2024-08-20T12:14:53.968370Z","iopub.status.idle":"2024-08-20T12:14:53.978375Z","shell.execute_reply.started":"2024-08-20T12:14:53.968345Z","shell.execute_reply":"2024-08-20T12:14:53.977407Z"},"trusted":true},"execution_count":109,"outputs":[]},{"cell_type":"code","source":"# Verify missing values again after removal\nmissing_values_snli_after_removal = combined_snli_df.isnull().sum()\nprint(missing_values_snli_after_removal)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.979619Z","iopub.execute_input":"2024-08-20T12:14:53.980631Z","iopub.status.idle":"2024-08-20T12:14:53.989583Z","shell.execute_reply.started":"2024-08-20T12:14:53.980589Z","shell.execute_reply":"2024-08-20T12:14:53.988567Z"},"trusted":true},"execution_count":110,"outputs":[{"name":"stdout","text":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_snli_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:53.990618Z","iopub.execute_input":"2024-08-20T12:14:53.990881Z","iopub.status.idle":"2024-08-20T12:14:54.004956Z","shell.execute_reply.started":"2024-08-20T12:14:53.990857Z","shell.execute_reply":"2024-08-20T12:14:54.003995Z"},"trusted":true},"execution_count":111,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 9824 entries, 0 to 9999\nData columns (total 10 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Deberta_Entailment     9824 non-null   float64\n 1   Deberta_Neutral        9824 non-null   float64\n 2   Deberta_Contradiction  9824 non-null   float64\n 3   Roberta_Entailment     9824 non-null   float64\n 4   Roberta_Neutral        9824 non-null   float64\n 5   Roberta_Contradiction  9824 non-null   float64\n 6   Albert_Entailment      9824 non-null   float64\n 7   Albert_Neutral         9824 non-null   float64\n 8   Albert_Contradiction   9824 non-null   float64\n 9   True_Label             9824 non-null   Int64  \ndtypes: Int64(1), float64(9)\nmemory usage: 853.8 KB\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_snli_df","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:54.006047Z","iopub.execute_input":"2024-08-20T12:14:54.006455Z","iopub.status.idle":"2024-08-20T12:14:54.026488Z","shell.execute_reply.started":"2024-08-20T12:14:54.006427Z","shell.execute_reply":"2024-08-20T12:14:54.025607Z"},"trusted":true},"execution_count":112,"outputs":[{"execution_count":112,"output_type":"execute_result","data":{"text/plain":"      Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0               0.034767         0.962592               0.002641   \n1               0.001921         0.319032               0.679047   \n2               0.998783         0.000764               0.000453   \n3               0.001001         0.997708               0.001291   \n4               0.001080         0.301363               0.697557   \n...                  ...              ...                    ...   \n9995            0.998825         0.001033               0.000142   \n9996            0.000704         0.009793               0.989503   \n9997            0.999171         0.000493               0.000336   \n9998            0.000267         0.002178               0.997556   \n9999            0.003641         0.995224               0.001135   \n\n      Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0               0.012451         0.927093               0.060457   \n1               0.752766         0.242251               0.004983   \n2               0.000254         0.004494               0.995253   \n3               0.005844         0.990736               0.003419   \n4               0.278348         0.718575               0.003076   \n...                  ...              ...                    ...   \n9995            0.001264         0.028942               0.969794   \n9996            0.780946         0.217053               0.002001   \n9997            0.000054         0.000765               0.999181   \n9998            0.983402         0.015884               0.000714   \n9999            0.000904         0.995495               0.003601   \n\n      Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0              0.008653        0.947434              0.043913           1  \n1              0.740332        0.256434              0.003235           0  \n2              0.004677        0.060481              0.934843           2  \n3              0.034056        0.956687              0.009257           1  \n4              0.498761        0.499270              0.001969           0  \n...                 ...             ...                   ...         ...  \n9995           0.006420        0.057240              0.936340           2  \n9996           0.894637        0.104095              0.001267           0  \n9997           0.000838        0.002670              0.996493           2  \n9998           0.984347        0.015223              0.000430           0  \n9999           0.005047        0.990218              0.004735           1  \n\n[9824 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.034767</td>\n      <td>0.962592</td>\n      <td>0.002641</td>\n      <td>0.012451</td>\n      <td>0.927093</td>\n      <td>0.060457</td>\n      <td>0.008653</td>\n      <td>0.947434</td>\n      <td>0.043913</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001921</td>\n      <td>0.319032</td>\n      <td>0.679047</td>\n      <td>0.752766</td>\n      <td>0.242251</td>\n      <td>0.004983</td>\n      <td>0.740332</td>\n      <td>0.256434</td>\n      <td>0.003235</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.998783</td>\n      <td>0.000764</td>\n      <td>0.000453</td>\n      <td>0.000254</td>\n      <td>0.004494</td>\n      <td>0.995253</td>\n      <td>0.004677</td>\n      <td>0.060481</td>\n      <td>0.934843</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001001</td>\n      <td>0.997708</td>\n      <td>0.001291</td>\n      <td>0.005844</td>\n      <td>0.990736</td>\n      <td>0.003419</td>\n      <td>0.034056</td>\n      <td>0.956687</td>\n      <td>0.009257</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.001080</td>\n      <td>0.301363</td>\n      <td>0.697557</td>\n      <td>0.278348</td>\n      <td>0.718575</td>\n      <td>0.003076</td>\n      <td>0.498761</td>\n      <td>0.499270</td>\n      <td>0.001969</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>0.998825</td>\n      <td>0.001033</td>\n      <td>0.000142</td>\n      <td>0.001264</td>\n      <td>0.028942</td>\n      <td>0.969794</td>\n      <td>0.006420</td>\n      <td>0.057240</td>\n      <td>0.936340</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>0.000704</td>\n      <td>0.009793</td>\n      <td>0.989503</td>\n      <td>0.780946</td>\n      <td>0.217053</td>\n      <td>0.002001</td>\n      <td>0.894637</td>\n      <td>0.104095</td>\n      <td>0.001267</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>0.999171</td>\n      <td>0.000493</td>\n      <td>0.000336</td>\n      <td>0.000054</td>\n      <td>0.000765</td>\n      <td>0.999181</td>\n      <td>0.000838</td>\n      <td>0.002670</td>\n      <td>0.996493</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>0.000267</td>\n      <td>0.002178</td>\n      <td>0.997556</td>\n      <td>0.983402</td>\n      <td>0.015884</td>\n      <td>0.000714</td>\n      <td>0.984347</td>\n      <td>0.015223</td>\n      <td>0.000430</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>0.003641</td>\n      <td>0.995224</td>\n      <td>0.001135</td>\n      <td>0.000904</td>\n      <td>0.995495</td>\n      <td>0.003601</td>\n      <td>0.005047</td>\n      <td>0.990218</td>\n      <td>0.004735</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>9824 rows  10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\ndef calculate_margin(row):\n    # Assuming the row only contains the probabilities\n    sorted_probs = np.sort(row)  # Sort probabilities in ascending order\n    if len(sorted_probs) > 1:\n        return sorted_probs[-1] - sorted_probs[-2]  # Difference between the highest and second highest\n    else:\n        return 0  # This handles the edge case where there is only one probability value\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:54.028048Z","iopub.execute_input":"2024-08-20T12:14:54.028636Z","iopub.status.idle":"2024-08-20T12:14:54.035476Z","shell.execute_reply.started":"2024-08-20T12:14:54.028600Z","shell.execute_reply":"2024-08-20T12:14:54.034571Z"},"trusted":true},"execution_count":113,"outputs":[]},{"cell_type":"code","source":"# Applying to a sample DataFrame with made-up column names\ncombined_snli_df['confidence_margin_entailment'] = combined_snli_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_snli_df['confidence_margin_neutral'] = combined_snli_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_snli_df['confidence_margin_contradiction'] = combined_snli_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n\n\n# Applying to a sample DataFrame with made-up column names\ncombined_mnli_matched_df['confidence_margin_entailment'] = combined_mnli_matched_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_mnli_matched_df['confidence_margin_neutral'] = combined_mnli_matched_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_mnli_matched_df['confidence_margin_contradiction'] = combined_mnli_matched_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n\n# Applying to a sample DataFrame with made-up column names\ncombined_mnli_mismatched_df['confidence_margin_entailment'] = combined_mnli_mismatched_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_mnli_mismatched_df['confidence_margin_neutral'] = combined_mnli_mismatched_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_mnli_mismatched_df['confidence_margin_contradiction'] = combined_mnli_mismatched_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n\n# Applying to a sample DataFrame with made-up column names\ncombined_anli_r1_df['confidence_margin_entailment'] = combined_anli_r1_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_anli_r1_df['confidence_margin_neutral'] = combined_anli_r1_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_anli_r1_df['confidence_margin_contradiction'] = combined_anli_r1_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n\n\n# Applying to a sample DataFrame with made-up column names\ncombined_anli_r2_df['confidence_margin_entailment'] = combined_anli_r2_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_anli_r2_df['confidence_margin_neutral'] = combined_anli_r2_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_anli_r2_df['confidence_margin_contradiction'] = combined_anli_r2_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n\n\n# Applying to a sample DataFrame with made-up column names\ncombined_anli_r3_df['confidence_margin_entailment'] = combined_anli_r3_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_anli_r3_df['confidence_margin_neutral'] = combined_anli_r3_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_anli_r3_df['confidence_margin_contradiction'] = combined_anli_r3_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:54.036985Z","iopub.execute_input":"2024-08-20T12:14:54.037368Z","iopub.status.idle":"2024-08-20T12:14:56.971311Z","shell.execute_reply.started":"2024-08-20T12:14:54.037335Z","shell.execute_reply":"2024-08-20T12:14:56.970177Z"},"trusted":true},"execution_count":114,"outputs":[]},{"cell_type":"code","source":"combined_anli_r3_df","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:56.973058Z","iopub.execute_input":"2024-08-20T12:14:56.973600Z","iopub.status.idle":"2024-08-20T12:14:56.995370Z","shell.execute_reply.started":"2024-08-20T12:14:56.973559Z","shell.execute_reply":"2024-08-20T12:14:56.994328Z"},"trusted":true},"execution_count":115,"outputs":[{"execution_count":115,"output_type":"execute_result","data":{"text/plain":"      Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0               0.005921         0.960529               0.033551   \n1               0.009586         0.934714               0.055700   \n2               0.003428         0.976393               0.020179   \n3               0.004633         0.023985               0.971382   \n4               0.017428         0.633695               0.348877   \n...                  ...              ...                    ...   \n1195            0.150312         0.806051               0.043637   \n1196            0.971834         0.026294               0.001872   \n1197            0.973818         0.025074               0.001109   \n1198            0.341781         0.226539               0.431681   \n1199            0.472737         0.477215               0.050049   \n\n      Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0               0.022959         0.976533               0.000509   \n1               0.999611         0.000205               0.000185   \n2               0.002020         0.997897               0.000083   \n3               0.974441         0.024459               0.001100   \n4               0.984416         0.011166               0.004419   \n...                  ...              ...                    ...   \n1195            0.032452         0.068553               0.898994   \n1196            0.009070         0.824654               0.166276   \n1197            0.000352         0.000972               0.998677   \n1198            0.006147         0.073669               0.920184   \n1199            0.000420         0.991832               0.007748   \n\n      Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \\\n0              0.001848        0.998084              0.000067           0   \n1              0.951772        0.048075              0.000153           0   \n2              0.001014        0.998984              0.000002           0   \n3              0.996749        0.000989              0.002262           0   \n4              0.000518        0.128416              0.871066           0   \n...                 ...             ...                   ...         ...   \n1195           0.122045        0.254093              0.623862           2   \n1196           0.001115        0.003229              0.995656           2   \n1197           0.310862        0.618914              0.070225           2   \n1198           0.054172        0.317935              0.627893           2   \n1199           0.015091        0.121354              0.863554           2   \n\n      confidence_margin_entailment  confidence_margin_neutral  \\\n0                         0.017038                   0.021552   \n1                         0.047839                   0.886640   \n2                         0.001408                   0.001087   \n3                         0.022308                   0.000474   \n4                         0.966988                   0.505279   \n...                            ...                        ...   \n1195                      0.028268                   0.551958   \n1196                      0.962764                   0.798360   \n1197                      0.662956                   0.593840   \n1198                      0.287609                   0.091397   \n1199                      0.457646                   0.514617   \n\n      confidence_margin_contradiction  \n0                            0.033042  \n1                            0.055515  \n2                            0.020096  \n3                            0.969120  \n4                            0.522189  \n...                               ...  \n1195                         0.275132  \n1196                         0.829379  \n1197                         0.928452  \n1198                         0.292291  \n1199                         0.813506  \n\n[1200 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n      <th>confidence_margin_entailment</th>\n      <th>confidence_margin_neutral</th>\n      <th>confidence_margin_contradiction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.005921</td>\n      <td>0.960529</td>\n      <td>0.033551</td>\n      <td>0.022959</td>\n      <td>0.976533</td>\n      <td>0.000509</td>\n      <td>0.001848</td>\n      <td>0.998084</td>\n      <td>0.000067</td>\n      <td>0</td>\n      <td>0.017038</td>\n      <td>0.021552</td>\n      <td>0.033042</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.009586</td>\n      <td>0.934714</td>\n      <td>0.055700</td>\n      <td>0.999611</td>\n      <td>0.000205</td>\n      <td>0.000185</td>\n      <td>0.951772</td>\n      <td>0.048075</td>\n      <td>0.000153</td>\n      <td>0</td>\n      <td>0.047839</td>\n      <td>0.886640</td>\n      <td>0.055515</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.003428</td>\n      <td>0.976393</td>\n      <td>0.020179</td>\n      <td>0.002020</td>\n      <td>0.997897</td>\n      <td>0.000083</td>\n      <td>0.001014</td>\n      <td>0.998984</td>\n      <td>0.000002</td>\n      <td>0</td>\n      <td>0.001408</td>\n      <td>0.001087</td>\n      <td>0.020096</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004633</td>\n      <td>0.023985</td>\n      <td>0.971382</td>\n      <td>0.974441</td>\n      <td>0.024459</td>\n      <td>0.001100</td>\n      <td>0.996749</td>\n      <td>0.000989</td>\n      <td>0.002262</td>\n      <td>0</td>\n      <td>0.022308</td>\n      <td>0.000474</td>\n      <td>0.969120</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.017428</td>\n      <td>0.633695</td>\n      <td>0.348877</td>\n      <td>0.984416</td>\n      <td>0.011166</td>\n      <td>0.004419</td>\n      <td>0.000518</td>\n      <td>0.128416</td>\n      <td>0.871066</td>\n      <td>0</td>\n      <td>0.966988</td>\n      <td>0.505279</td>\n      <td>0.522189</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1195</th>\n      <td>0.150312</td>\n      <td>0.806051</td>\n      <td>0.043637</td>\n      <td>0.032452</td>\n      <td>0.068553</td>\n      <td>0.898994</td>\n      <td>0.122045</td>\n      <td>0.254093</td>\n      <td>0.623862</td>\n      <td>2</td>\n      <td>0.028268</td>\n      <td>0.551958</td>\n      <td>0.275132</td>\n    </tr>\n    <tr>\n      <th>1196</th>\n      <td>0.971834</td>\n      <td>0.026294</td>\n      <td>0.001872</td>\n      <td>0.009070</td>\n      <td>0.824654</td>\n      <td>0.166276</td>\n      <td>0.001115</td>\n      <td>0.003229</td>\n      <td>0.995656</td>\n      <td>2</td>\n      <td>0.962764</td>\n      <td>0.798360</td>\n      <td>0.829379</td>\n    </tr>\n    <tr>\n      <th>1197</th>\n      <td>0.973818</td>\n      <td>0.025074</td>\n      <td>0.001109</td>\n      <td>0.000352</td>\n      <td>0.000972</td>\n      <td>0.998677</td>\n      <td>0.310862</td>\n      <td>0.618914</td>\n      <td>0.070225</td>\n      <td>2</td>\n      <td>0.662956</td>\n      <td>0.593840</td>\n      <td>0.928452</td>\n    </tr>\n    <tr>\n      <th>1198</th>\n      <td>0.341781</td>\n      <td>0.226539</td>\n      <td>0.431681</td>\n      <td>0.006147</td>\n      <td>0.073669</td>\n      <td>0.920184</td>\n      <td>0.054172</td>\n      <td>0.317935</td>\n      <td>0.627893</td>\n      <td>2</td>\n      <td>0.287609</td>\n      <td>0.091397</td>\n      <td>0.292291</td>\n    </tr>\n    <tr>\n      <th>1199</th>\n      <td>0.472737</td>\n      <td>0.477215</td>\n      <td>0.050049</td>\n      <td>0.000420</td>\n      <td>0.991832</td>\n      <td>0.007748</td>\n      <td>0.015091</td>\n      <td>0.121354</td>\n      <td>0.863554</td>\n      <td>2</td>\n      <td>0.457646</td>\n      <td>0.514617</td>\n      <td>0.813506</td>\n    </tr>\n  </tbody>\n</table>\n<p>1200 rows  13 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot histograms for the confidence margins\ndef plot_confidence_margins(df):\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 3, 1)\n    plt.hist(df['confidence_margin_entailment'], bins=50, alpha=0.75, label='Entailment')\n    plt.title('Confidence Margin Histogram - Entailment')\n    plt.xlabel('Margin')\n    plt.ylabel('Frequency')\n    \n    plt.subplot(1, 3, 2)\n    plt.hist(df['confidence_margin_neutral'], bins=50, alpha=0.75, label='Neutral', color='green')\n    plt.title('Confidence Margin Histogram - Neutral')\n    plt.xlabel('Margin')\n    plt.ylabel('Frequency')\n    \n    plt.subplot(1, 3, 3)\n    plt.hist(df['confidence_margin_contradiction'], bins=50, alpha=0.75, label='Contradiction', color='red')\n    plt.title('Confidence Margin Histogram - Contradiction')\n    plt.xlabel('Margin')\n    plt.ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_confidence_margins(combined_snli_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:56.996666Z","iopub.execute_input":"2024-08-20T12:14:56.996995Z","iopub.status.idle":"2024-08-20T12:14:57.969692Z","shell.execute_reply.started":"2024-08-20T12:14:56.996968Z","shell.execute_reply":"2024-08-20T12:14:57.968681Z"},"trusted":true},"execution_count":116,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 3 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# Define thresholds based on the distribution\nconfidence_threshold = 0.8\n\n# Apply threshold and create a new copy of the DataFrame\nhigh_confidence_snli = combined_snli_df[\n    (combined_snli_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_snli_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_snli_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()\n\n\n# Apply threshold and create a new copy of the DataFrame\nhigh_confidence_mnli_matched = combined_mnli_matched_df[\n    (combined_mnli_matched_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_mnli_matched_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_mnli_matched_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()\n\n# Apply threshold and create a new copy of the DataFrame\nhigh_confidence_mnli_mismatched = combined_mnli_mismatched_df[\n    (combined_mnli_mismatched_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_mnli_mismatched_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_mnli_mismatched_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()\n\n# Filter high confidence samples for ANLI round 1\nhigh_confidence_anli_r1 = combined_anli_r1_df[\n    (combined_anli_r1_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_anli_r1_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_anli_r1_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()\n\n# Filter high confidence samples for ANLI round 2\nhigh_confidence_anli_r2 = combined_anli_r2_df[\n    (combined_anli_r2_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_anli_r2_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_anli_r2_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()\n\n# Filter high confidence samples for ANLI round 3\nhigh_confidence_anli_r3 = combined_anli_r3_df[\n    (combined_anli_r3_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_anli_r3_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_anli_r3_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:57.975435Z","iopub.execute_input":"2024-08-20T12:14:57.975762Z","iopub.status.idle":"2024-08-20T12:14:58.001368Z","shell.execute_reply.started":"2024-08-20T12:14:57.975735Z","shell.execute_reply":"2024-08-20T12:14:58.000389Z"},"trusted":true},"execution_count":117,"outputs":[]},{"cell_type":"code","source":"# Majority vote logic with weights for numeric labels\ndef majority_vote_with_threshold(row):\n    votes = [0, 0, 0]  # Index 0 for entailment, 1 for neutral, 2 for contradiction\n    if row['confidence_margin_entailment'] >= confidence_threshold:\n        votes[0] += row['confidence_margin_entailment']\n    if row['confidence_margin_neutral'] >= confidence_threshold:\n        votes[1] += row['confidence_margin_neutral']\n    if row['confidence_margin_contradiction'] >= confidence_threshold:\n        votes[2] += row['confidence_margin_contradiction']\n    \n    # Return the index of the highest vote\n    return np.argmax(votes)\n\n# Apply the majority vote logic using .loc\nhigh_confidence_snli.loc[:, 'majority_vote'] = high_confidence_snli.apply(majority_vote_with_threshold, axis=1)\n# Apply the majority vote logic using .loc\nhigh_confidence_mnli_matched.loc[:, 'majority_vote'] = high_confidence_mnli_matched.apply(majority_vote_with_threshold, axis=1)\n# Apply majority vote logic\nhigh_confidence_mnli_mismatched.loc[:, 'majority_vote'] = high_confidence_mnli_mismatched.apply(majority_vote_with_threshold, axis=1)\n# Apply majority vote logic\nhigh_confidence_anli_r1.loc[:, 'majority_vote'] = high_confidence_anli_r1.apply(majority_vote_with_threshold, axis=1)\n# Apply majority vote logic\nhigh_confidence_anli_r2.loc[:, 'majority_vote'] = high_confidence_anli_r2.apply(majority_vote_with_threshold, axis=1)\n# Apply majority vote logic\nhigh_confidence_anli_r3.loc[:, 'majority_vote'] = high_confidence_anli_r3.apply(majority_vote_with_threshold, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:58.002912Z","iopub.execute_input":"2024-08-20T12:14:58.003404Z","iopub.status.idle":"2024-08-20T12:14:59.748455Z","shell.execute_reply.started":"2024-08-20T12:14:58.003369Z","shell.execute_reply":"2024-08-20T12:14:59.747604Z"},"trusted":true},"execution_count":118,"outputs":[]},{"cell_type":"code","source":"high_confidence_anli_r3","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:59.749518Z","iopub.execute_input":"2024-08-20T12:14:59.749826Z","iopub.status.idle":"2024-08-20T12:14:59.771755Z","shell.execute_reply.started":"2024-08-20T12:14:59.749799Z","shell.execute_reply":"2024-08-20T12:14:59.770686Z"},"trusted":true},"execution_count":119,"outputs":[{"execution_count":119,"output_type":"execute_result","data":{"text/plain":"      Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n1               0.009586         0.934714               0.055700   \n3               0.004633         0.023985               0.971382   \n4               0.017428         0.633695               0.348877   \n5               0.002461         0.150678               0.846861   \n8               0.131793         0.776056               0.092151   \n...                  ...              ...                    ...   \n1193            0.957103         0.040533               0.002363   \n1194            0.945850         0.052813               0.001337   \n1196            0.971834         0.026294               0.001872   \n1197            0.973818         0.025074               0.001109   \n1199            0.472737         0.477215               0.050049   \n\n      Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n1               0.999611         0.000205               0.000185   \n3               0.974441         0.024459               0.001100   \n4               0.984416         0.011166               0.004419   \n5               0.999220         0.000554               0.000227   \n8               0.000384         0.002570               0.997046   \n...                  ...              ...                    ...   \n1193            0.000066         0.000316               0.999618   \n1194            0.000607         0.000853               0.998540   \n1196            0.009070         0.824654               0.166276   \n1197            0.000352         0.000972               0.998677   \n1199            0.000420         0.991832               0.007748   \n\n      Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \\\n1              0.951772        0.048075              0.000153           0   \n3              0.996749        0.000989              0.002262           0   \n4              0.000518        0.128416              0.871066           0   \n5              0.985221        0.014745              0.000034           0   \n8              0.049409        0.904356              0.046236           0   \n...                 ...             ...                   ...         ...   \n1193           0.009265        0.016116              0.974619           2   \n1194           0.000205        0.000291              0.999504           2   \n1196           0.001115        0.003229              0.995656           2   \n1197           0.310862        0.618914              0.070225           2   \n1199           0.015091        0.121354              0.863554           2   \n\n      confidence_margin_entailment  confidence_margin_neutral  \\\n1                         0.047839                   0.886640   \n3                         0.022308                   0.000474   \n4                         0.966988                   0.505279   \n5                         0.013998                   0.135933   \n8                         0.082384                   0.128299   \n...                            ...                        ...   \n1193                      0.947838                   0.024417   \n1194                      0.945243                   0.051960   \n1196                      0.962764                   0.798360   \n1197                      0.662956                   0.593840   \n1199                      0.457646                   0.514617   \n\n      confidence_margin_contradiction  majority_vote  \n1                            0.055515              1  \n3                            0.969120              2  \n4                            0.522189              0  \n5                            0.846635              2  \n8                            0.904895              2  \n...                               ...            ...  \n1193                         0.024999              0  \n1194                         0.000965              0  \n1196                         0.829379              0  \n1197                         0.928452              2  \n1199                         0.813506              2  \n\n[530 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n      <th>confidence_margin_entailment</th>\n      <th>confidence_margin_neutral</th>\n      <th>confidence_margin_contradiction</th>\n      <th>majority_vote</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.009586</td>\n      <td>0.934714</td>\n      <td>0.055700</td>\n      <td>0.999611</td>\n      <td>0.000205</td>\n      <td>0.000185</td>\n      <td>0.951772</td>\n      <td>0.048075</td>\n      <td>0.000153</td>\n      <td>0</td>\n      <td>0.047839</td>\n      <td>0.886640</td>\n      <td>0.055515</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004633</td>\n      <td>0.023985</td>\n      <td>0.971382</td>\n      <td>0.974441</td>\n      <td>0.024459</td>\n      <td>0.001100</td>\n      <td>0.996749</td>\n      <td>0.000989</td>\n      <td>0.002262</td>\n      <td>0</td>\n      <td>0.022308</td>\n      <td>0.000474</td>\n      <td>0.969120</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.017428</td>\n      <td>0.633695</td>\n      <td>0.348877</td>\n      <td>0.984416</td>\n      <td>0.011166</td>\n      <td>0.004419</td>\n      <td>0.000518</td>\n      <td>0.128416</td>\n      <td>0.871066</td>\n      <td>0</td>\n      <td>0.966988</td>\n      <td>0.505279</td>\n      <td>0.522189</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.002461</td>\n      <td>0.150678</td>\n      <td>0.846861</td>\n      <td>0.999220</td>\n      <td>0.000554</td>\n      <td>0.000227</td>\n      <td>0.985221</td>\n      <td>0.014745</td>\n      <td>0.000034</td>\n      <td>0</td>\n      <td>0.013998</td>\n      <td>0.135933</td>\n      <td>0.846635</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.131793</td>\n      <td>0.776056</td>\n      <td>0.092151</td>\n      <td>0.000384</td>\n      <td>0.002570</td>\n      <td>0.997046</td>\n      <td>0.049409</td>\n      <td>0.904356</td>\n      <td>0.046236</td>\n      <td>0</td>\n      <td>0.082384</td>\n      <td>0.128299</td>\n      <td>0.904895</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1193</th>\n      <td>0.957103</td>\n      <td>0.040533</td>\n      <td>0.002363</td>\n      <td>0.000066</td>\n      <td>0.000316</td>\n      <td>0.999618</td>\n      <td>0.009265</td>\n      <td>0.016116</td>\n      <td>0.974619</td>\n      <td>2</td>\n      <td>0.947838</td>\n      <td>0.024417</td>\n      <td>0.024999</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1194</th>\n      <td>0.945850</td>\n      <td>0.052813</td>\n      <td>0.001337</td>\n      <td>0.000607</td>\n      <td>0.000853</td>\n      <td>0.998540</td>\n      <td>0.000205</td>\n      <td>0.000291</td>\n      <td>0.999504</td>\n      <td>2</td>\n      <td>0.945243</td>\n      <td>0.051960</td>\n      <td>0.000965</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1196</th>\n      <td>0.971834</td>\n      <td>0.026294</td>\n      <td>0.001872</td>\n      <td>0.009070</td>\n      <td>0.824654</td>\n      <td>0.166276</td>\n      <td>0.001115</td>\n      <td>0.003229</td>\n      <td>0.995656</td>\n      <td>2</td>\n      <td>0.962764</td>\n      <td>0.798360</td>\n      <td>0.829379</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1197</th>\n      <td>0.973818</td>\n      <td>0.025074</td>\n      <td>0.001109</td>\n      <td>0.000352</td>\n      <td>0.000972</td>\n      <td>0.998677</td>\n      <td>0.310862</td>\n      <td>0.618914</td>\n      <td>0.070225</td>\n      <td>2</td>\n      <td>0.662956</td>\n      <td>0.593840</td>\n      <td>0.928452</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1199</th>\n      <td>0.472737</td>\n      <td>0.477215</td>\n      <td>0.050049</td>\n      <td>0.000420</td>\n      <td>0.991832</td>\n      <td>0.007748</td>\n      <td>0.015091</td>\n      <td>0.121354</td>\n      <td>0.863554</td>\n      <td>2</td>\n      <td>0.457646</td>\n      <td>0.514617</td>\n      <td>0.813506</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>530 rows  14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"high_confidence_mnli_mismatched","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:59.772914Z","iopub.execute_input":"2024-08-20T12:14:59.773210Z","iopub.status.idle":"2024-08-20T12:14:59.797459Z","shell.execute_reply.started":"2024-08-20T12:14:59.773185Z","shell.execute_reply":"2024-08-20T12:14:59.796464Z"},"trusted":true},"execution_count":120,"outputs":[{"execution_count":120,"output_type":"execute_result","data":{"text/plain":"      Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0               0.999667         0.000160               0.000173   \n1               0.998119         0.000962               0.000919   \n2               0.000552         0.004809               0.994639   \n3               0.827653         0.171961               0.000386   \n4               0.000292         0.002875               0.996833   \n...                  ...              ...                    ...   \n9824            0.000760         0.013837               0.985402   \n9825            0.000145         0.003906               0.995949   \n9826            0.996226         0.003656               0.000118   \n9828            0.997044         0.001226               0.001730   \n9829            0.999685         0.000112               0.000202   \n\n      Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0               0.000068         0.000402               0.999529   \n1               0.000183         0.001511               0.998306   \n2               0.986062         0.012020               0.001918   \n3               0.000478         0.270953               0.728569   \n4               0.975167         0.021904               0.002929   \n...                  ...              ...                    ...   \n9824            0.972984         0.025382               0.001634   \n9825            0.952827         0.045891               0.001281   \n9826            0.000353         0.104717               0.894930   \n9828            0.000364         0.000717               0.998919   \n9829            0.000085         0.000289               0.999626   \n\n      Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \\\n0              0.000894        0.003787              0.995318           2   \n1              0.006421        0.010224              0.983355           2   \n2              0.975041        0.023354              0.001605           0   \n3              0.001722        0.796122              0.202156           2   \n4              0.965952        0.032748              0.001300           0   \n...                 ...             ...                   ...         ...   \n9824           0.925212        0.069991              0.004798           0   \n9825           0.965645        0.032757              0.001598           0   \n9826           0.004991        0.039946              0.955062           2   \n9828           0.019078        0.039258              0.941663           2   \n9829           0.000420        0.001810              0.997770           2   \n\n      confidence_margin_entailment  confidence_margin_neutral  \\\n0                         0.998773                   0.003385   \n1                         0.991698                   0.008714   \n2                         0.011021                   0.011334   \n3                         0.825931                   0.525168   \n4                         0.009215                   0.010844   \n...                            ...                        ...   \n9824                      0.047772                   0.044608   \n9825                      0.012818                   0.013134   \n9826                      0.991234                   0.064771   \n9828                      0.977966                   0.038033   \n9829                      0.999265                   0.001521   \n\n      confidence_margin_contradiction  majority_vote  \n0                            0.004211              0  \n1                            0.014951              0  \n2                            0.992721              2  \n3                            0.526413              0  \n4                            0.993904              2  \n...                               ...            ...  \n9824                         0.980605              2  \n9825                         0.994351              2  \n9826                         0.060132              0  \n9828                         0.057256              0  \n9829                         0.001856              0  \n\n[5910 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n      <th>confidence_margin_entailment</th>\n      <th>confidence_margin_neutral</th>\n      <th>confidence_margin_contradiction</th>\n      <th>majority_vote</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.999667</td>\n      <td>0.000160</td>\n      <td>0.000173</td>\n      <td>0.000068</td>\n      <td>0.000402</td>\n      <td>0.999529</td>\n      <td>0.000894</td>\n      <td>0.003787</td>\n      <td>0.995318</td>\n      <td>2</td>\n      <td>0.998773</td>\n      <td>0.003385</td>\n      <td>0.004211</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.998119</td>\n      <td>0.000962</td>\n      <td>0.000919</td>\n      <td>0.000183</td>\n      <td>0.001511</td>\n      <td>0.998306</td>\n      <td>0.006421</td>\n      <td>0.010224</td>\n      <td>0.983355</td>\n      <td>2</td>\n      <td>0.991698</td>\n      <td>0.008714</td>\n      <td>0.014951</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000552</td>\n      <td>0.004809</td>\n      <td>0.994639</td>\n      <td>0.986062</td>\n      <td>0.012020</td>\n      <td>0.001918</td>\n      <td>0.975041</td>\n      <td>0.023354</td>\n      <td>0.001605</td>\n      <td>0</td>\n      <td>0.011021</td>\n      <td>0.011334</td>\n      <td>0.992721</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.827653</td>\n      <td>0.171961</td>\n      <td>0.000386</td>\n      <td>0.000478</td>\n      <td>0.270953</td>\n      <td>0.728569</td>\n      <td>0.001722</td>\n      <td>0.796122</td>\n      <td>0.202156</td>\n      <td>2</td>\n      <td>0.825931</td>\n      <td>0.525168</td>\n      <td>0.526413</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000292</td>\n      <td>0.002875</td>\n      <td>0.996833</td>\n      <td>0.975167</td>\n      <td>0.021904</td>\n      <td>0.002929</td>\n      <td>0.965952</td>\n      <td>0.032748</td>\n      <td>0.001300</td>\n      <td>0</td>\n      <td>0.009215</td>\n      <td>0.010844</td>\n      <td>0.993904</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9824</th>\n      <td>0.000760</td>\n      <td>0.013837</td>\n      <td>0.985402</td>\n      <td>0.972984</td>\n      <td>0.025382</td>\n      <td>0.001634</td>\n      <td>0.925212</td>\n      <td>0.069991</td>\n      <td>0.004798</td>\n      <td>0</td>\n      <td>0.047772</td>\n      <td>0.044608</td>\n      <td>0.980605</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9825</th>\n      <td>0.000145</td>\n      <td>0.003906</td>\n      <td>0.995949</td>\n      <td>0.952827</td>\n      <td>0.045891</td>\n      <td>0.001281</td>\n      <td>0.965645</td>\n      <td>0.032757</td>\n      <td>0.001598</td>\n      <td>0</td>\n      <td>0.012818</td>\n      <td>0.013134</td>\n      <td>0.994351</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9826</th>\n      <td>0.996226</td>\n      <td>0.003656</td>\n      <td>0.000118</td>\n      <td>0.000353</td>\n      <td>0.104717</td>\n      <td>0.894930</td>\n      <td>0.004991</td>\n      <td>0.039946</td>\n      <td>0.955062</td>\n      <td>2</td>\n      <td>0.991234</td>\n      <td>0.064771</td>\n      <td>0.060132</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9828</th>\n      <td>0.997044</td>\n      <td>0.001226</td>\n      <td>0.001730</td>\n      <td>0.000364</td>\n      <td>0.000717</td>\n      <td>0.998919</td>\n      <td>0.019078</td>\n      <td>0.039258</td>\n      <td>0.941663</td>\n      <td>2</td>\n      <td>0.977966</td>\n      <td>0.038033</td>\n      <td>0.057256</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9829</th>\n      <td>0.999685</td>\n      <td>0.000112</td>\n      <td>0.000202</td>\n      <td>0.000085</td>\n      <td>0.000289</td>\n      <td>0.999626</td>\n      <td>0.000420</td>\n      <td>0.001810</td>\n      <td>0.997770</td>\n      <td>2</td>\n      <td>0.999265</td>\n      <td>0.001521</td>\n      <td>0.001856</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5910 rows  14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install  torch-geometric","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:14:59.798607Z","iopub.execute_input":"2024-08-20T12:14:59.798869Z","iopub.status.idle":"2024-08-20T12:15:12.163271Z","shell.execute_reply.started":"2024-08-20T12:14:59.798845Z","shell.execute_reply":"2024-08-20T12:15:12.161890Z"},"trusted":true},"execution_count":121,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch-geometric in /opt/conda/lib/python3.10/site-packages (2.5.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.11.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2024.3.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.31.0)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\n\n\n# Features and Labels\nX_snli = high_confidence_snli.drop(['True_Label','majority_vote'], axis=1).values\ny_snli = high_confidence_snli['True_Label'].values\n\n# Features and Labels\nX_mnli_matched = high_confidence_mnli_matched.drop(['True_Label','majority_vote'], axis=1).values\ny_mnli_matched = high_confidence_mnli_matched['True_Label'].values\n\n# Features and Labels\nX_mnli_mismatched = high_confidence_mnli_mismatched.drop(['True_Label','majority_vote'], axis=1).values\ny_mnli_mismatched = high_confidence_mnli_mismatched['True_Label'].values\n\n# Features and Labels\nX_anli_r1 = high_confidence_anli_r1.drop(['True_Label','majority_vote'], axis=1).values\ny_anli_r1 = high_confidence_anli_r1['True_Label'].values\n\n# Features and Labels\nX_anli_r2 = high_confidence_anli_r2.drop(['True_Label','majority_vote'], axis=1).values\ny_anli_r2 = high_confidence_anli_r2['True_Label'].values\n\n# Features and Labels\nX_anli_r3 = high_confidence_anli_r3.drop(['True_Label','majority_vote'], axis=1).values\ny_anli_r3 = high_confidence_anli_r3['True_Label'].values\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:15:12.166268Z","iopub.execute_input":"2024-08-20T12:15:12.167338Z","iopub.status.idle":"2024-08-20T12:15:12.184288Z","shell.execute_reply.started":"2024-08-20T12:15:12.167297Z","shell.execute_reply":"2024-08-20T12:15:12.183082Z"},"trusted":true},"execution_count":122,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GCNConv\nfrom torch_geometric.data import Data\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\n\n# Function to create graph data\ndef create_graph(data, labels):\n    num_nodes = data.shape[0]\n    edge_index = torch.tensor([range(num_nodes), range(num_nodes)], dtype=torch.long)  # Self-loops as edges\n    graph_data = Data(x=torch.tensor(data, dtype=torch.float), edge_index=edge_index, y=torch.tensor(labels, dtype=torch.long))\n    return graph_data\n\n# Early Stopping Class\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_loss = None\n        self.wait = 0\n        self.stop_training = False\n\n    def __call__(self, loss):\n        if self.best_loss is None or loss < self.best_loss - self.min_delta:\n            self.best_loss = loss\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.stop_training = True\n\n# Simple GCN Model\nclass SimpleGCN(torch.nn.Module):\n    def __init__(self, input_dim):\n        super(SimpleGCN, self).__init__()\n        self.conv1 = GCNConv(input_dim, 32)  # Input to hidden layer\n        self.conv2 = GCNConv(32, 3)  # Output layer for classification\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=0.3, training=self.training)  # Dropout\n        x = self.conv2(x, edge_index)\n        return F.log_softmax(x, dim=1)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:15:12.186168Z","iopub.execute_input":"2024-08-20T12:15:12.186691Z","iopub.status.idle":"2024-08-20T12:15:12.201962Z","shell.execute_reply.started":"2024-08-20T12:15:12.186654Z","shell.execute_reply":"2024-08-20T12:15:12.201048Z"},"trusted":true},"execution_count":123,"outputs":[]},{"cell_type":"code","source":"\n\n# Modified training and evaluation function to include recall and F1-score\ndef train_and_evaluate(train_data, test_data, name, num_epochs=100, patience=10, learning_rate=0.005, weight_decay=5e-4):\n    input_dim = train_data.x.shape[1]  # Determine the input dimension\n    model = SimpleGCN(input_dim)  # Instantiate model with correct input dimension\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)  # Learning rate and weight decay\n    criterion = torch.nn.CrossEntropyLoss()  # Loss function\n\n    # Initialize early stopping\n    early_stopping = EarlyStopping(patience=patience)\n\n    # Variables for tracking metrics\n    train_losses = []\n    val_losses = []\n    train_recalls = []\n    val_recalls = []\n    train_f1_scores = []\n    val_f1_scores = []\n\n    # Training with validation and early stopping\n    for epoch in range(num_epochs):\n        model.train()  # Set to training mode\n        optimizer.zero_grad()  # Reset gradients\n        out = model(train_data)  # Forward pass\n        loss = criterion(out, train_data.y)  # Calculate loss\n        loss.backward()  # Backward pass\n        optimizer.step()  # Update weights\n        train_losses.append(loss.item())  # Store training loss\n\n        # Calculate training metrics\n        train_pred = out.argmax(dim=1).detach().cpu().numpy()  # Get predicted labels\n        train_y_true = train_data.y.detach().cpu().numpy()\n        train_recall = recall_score(train_y_true, train_pred, average='macro')\n        train_f1 = f1_score(train_y_true, train_pred, average='macro')\n        train_recalls.append(train_recall)\n        train_f1_scores.append(train_f1)\n\n        # Validation\n        model.eval()  # Set to evaluation mode\n        with torch.no_grad():\n            val_out = model(test_data)  # Forward pass for validation\n            val_loss = criterion(val_out, test_data.y)  # Validation loss\n            val_losses.append(val_loss.item())  # Store validation loss\n            val_pred = val_out.argmax(dim=1).detach().cpu().numpy()  # Get predicted labels\n            val_y_true = test_data.y.detach().cpu().numpy()\n            val_recall = recall_score(val_y_true, val_pred, average='macro')\n            val_f1 = f1_score(val_y_true, val_pred, average='macro')\n            val_recalls.append(val_recall)\n            val_f1_scores.append(val_f1)\n\n        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, Val Recall: {val_recall:.4f}, Val F1-Score: {val_f1:.4f}\")\n\n        # Check for early stopping\n        early_stopping(val_loss.item())\n        if early_stopping.stop_training:\n            print(\"Early stopping triggered\")\n            break\n\n    # Final metrics on validation data\n    final_val_loss = val_losses[-1]\n    final_val_recall = val_recalls[-1]\n    final_val_f1 = val_f1_scores[-1]\n\n    print(f\"Final Validation Loss on {name}: {final_val_loss:.4f}\")\n    print(f\"Final Validation Recall on {name}: {final_val_recall:.4f}\")\n    print(f\"Final Validation F1-Score on {name}: {final_val_f1:.4f}\")\n\n    # Return model, final validation loss, recall, and F1-score\n    return model, final_val_loss, final_val_recall, final_val_f1\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:16:25.329108Z","iopub.execute_input":"2024-08-20T12:16:25.329504Z","iopub.status.idle":"2024-08-20T12:16:25.344913Z","shell.execute_reply.started":"2024-08-20T12:16:25.329473Z","shell.execute_reply":"2024-08-20T12:16:25.343878Z"},"trusted":true},"execution_count":126,"outputs":[]},{"cell_type":"code","source":"X_train_snli, X_test_snli, y_train_snli, y_test_snli = train_test_split(X_snli, y_snli, test_size=0.2, random_state=42)\n\n# Create graph data for SNLI\ntrain_graph_snli = create_graph(X_train_snli, y_train_snli)\ntest_graph_snli = create_graph(X_test_snli, y_test_snli)\n\n# Call the function to train and evaluate for SNLI\nmodel_snli, val_loss_snli, val_accuracy_snli = train_and_evaluate(\n    train_graph_snli, test_graph_snli, 'SNLI'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:16:27.994994Z","iopub.execute_input":"2024-08-20T12:16:27.995869Z","iopub.status.idle":"2024-08-20T12:16:29.262721Z","shell.execute_reply.started":"2024-08-20T12:16:27.995838Z","shell.execute_reply":"2024-08-20T12:16:29.261560Z"},"trusted":true},"execution_count":127,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.0271, Val Loss: 0.9472, Val Recall: 0.3333, Val F1-Score: 0.2105\nEpoch 2/100, Train Loss: 0.9474, Val Loss: 0.8716, Val Recall: 0.3338, Val F1-Score: 0.2116\nEpoch 3/100, Train Loss: 0.8672, Val Loss: 0.7999, Val Recall: 0.6437, Val F1-Score: 0.6270\nEpoch 4/100, Train Loss: 0.7971, Val Loss: 0.7322, Val Recall: 0.6575, Val F1-Score: 0.6412\nEpoch 5/100, Train Loss: 0.7304, Val Loss: 0.6682, Val Recall: 0.6609, Val F1-Score: 0.6448\nEpoch 6/100, Train Loss: 0.6667, Val Loss: 0.6097, Val Recall: 0.6614, Val F1-Score: 0.6455\nEpoch 7/100, Train Loss: 0.6062, Val Loss: 0.5588, Val Recall: 0.6614, Val F1-Score: 0.6455\nEpoch 8/100, Train Loss: 0.5572, Val Loss: 0.5138, Val Recall: 0.6614, Val F1-Score: 0.6456\nEpoch 9/100, Train Loss: 0.5131, Val Loss: 0.4732, Val Recall: 0.6614, Val F1-Score: 0.6455\nEpoch 10/100, Train Loss: 0.4731, Val Loss: 0.4366, Val Recall: 0.6614, Val F1-Score: 0.6455\nEpoch 11/100, Train Loss: 0.4379, Val Loss: 0.4040, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 12/100, Train Loss: 0.4016, Val Loss: 0.3751, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 13/100, Train Loss: 0.3742, Val Loss: 0.3497, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 14/100, Train Loss: 0.3505, Val Loss: 0.3275, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 15/100, Train Loss: 0.3253, Val Loss: 0.3083, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 16/100, Train Loss: 0.3059, Val Loss: 0.2920, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 17/100, Train Loss: 0.2876, Val Loss: 0.2781, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 18/100, Train Loss: 0.2736, Val Loss: 0.2664, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 19/100, Train Loss: 0.2598, Val Loss: 0.2566, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 20/100, Train Loss: 0.2489, Val Loss: 0.2483, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 21/100, Train Loss: 0.2395, Val Loss: 0.2412, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 22/100, Train Loss: 0.2308, Val Loss: 0.2353, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 23/100, Train Loss: 0.2253, Val Loss: 0.2303, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 24/100, Train Loss: 0.2175, Val Loss: 0.2262, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 25/100, Train Loss: 0.2110, Val Loss: 0.2226, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 26/100, Train Loss: 0.2070, Val Loss: 0.2195, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 27/100, Train Loss: 0.2004, Val Loss: 0.2168, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 28/100, Train Loss: 0.1945, Val Loss: 0.2144, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 29/100, Train Loss: 0.1921, Val Loss: 0.2122, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 30/100, Train Loss: 0.1919, Val Loss: 0.2101, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 31/100, Train Loss: 0.1856, Val Loss: 0.2082, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 32/100, Train Loss: 0.1863, Val Loss: 0.2062, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 33/100, Train Loss: 0.1829, Val Loss: 0.2043, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 34/100, Train Loss: 0.1768, Val Loss: 0.2024, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 35/100, Train Loss: 0.1785, Val Loss: 0.2005, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 36/100, Train Loss: 0.1735, Val Loss: 0.1985, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 37/100, Train Loss: 0.1712, Val Loss: 0.1966, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 38/100, Train Loss: 0.1710, Val Loss: 0.1946, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 39/100, Train Loss: 0.1692, Val Loss: 0.1927, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 40/100, Train Loss: 0.1647, Val Loss: 0.1908, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 41/100, Train Loss: 0.1673, Val Loss: 0.1890, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 42/100, Train Loss: 0.1646, Val Loss: 0.1873, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 43/100, Train Loss: 0.1617, Val Loss: 0.1857, Val Recall: 0.6614, Val F1-Score: 0.6454\nEpoch 44/100, Train Loss: 0.1601, Val Loss: 0.1841, Val Recall: 0.6901, Val F1-Score: 0.6997\nEpoch 45/100, Train Loss: 0.1601, Val Loss: 0.1826, Val Recall: 0.6959, Val F1-Score: 0.7096\nEpoch 46/100, Train Loss: 0.1539, Val Loss: 0.1812, Val Recall: 0.7074, Val F1-Score: 0.7285\nEpoch 47/100, Train Loss: 0.1551, Val Loss: 0.1799, Val Recall: 0.7131, Val F1-Score: 0.7375\nEpoch 48/100, Train Loss: 0.1534, Val Loss: 0.1786, Val Recall: 0.7131, Val F1-Score: 0.7375\nEpoch 49/100, Train Loss: 0.1507, Val Loss: 0.1775, Val Recall: 0.7189, Val F1-Score: 0.7463\nEpoch 50/100, Train Loss: 0.1514, Val Loss: 0.1764, Val Recall: 0.7246, Val F1-Score: 0.7548\nEpoch 51/100, Train Loss: 0.1500, Val Loss: 0.1754, Val Recall: 0.7361, Val F1-Score: 0.7711\nEpoch 52/100, Train Loss: 0.1530, Val Loss: 0.1746, Val Recall: 0.7419, Val F1-Score: 0.7789\nEpoch 53/100, Train Loss: 0.1522, Val Loss: 0.1738, Val Recall: 0.7419, Val F1-Score: 0.7789\nEpoch 54/100, Train Loss: 0.1456, Val Loss: 0.1732, Val Recall: 0.7534, Val F1-Score: 0.7940\nEpoch 55/100, Train Loss: 0.1466, Val Loss: 0.1727, Val Recall: 0.7528, Val F1-Score: 0.7902\nEpoch 56/100, Train Loss: 0.1464, Val Loss: 0.1722, Val Recall: 0.7522, Val F1-Score: 0.7880\nEpoch 57/100, Train Loss: 0.1494, Val Loss: 0.1718, Val Recall: 0.7522, Val F1-Score: 0.7880\nEpoch 58/100, Train Loss: 0.1456, Val Loss: 0.1715, Val Recall: 0.7579, Val F1-Score: 0.7951\nEpoch 59/100, Train Loss: 0.1459, Val Loss: 0.1712, Val Recall: 0.7631, Val F1-Score: 0.7997\nEpoch 60/100, Train Loss: 0.1444, Val Loss: 0.1709, Val Recall: 0.7625, Val F1-Score: 0.7976\nEpoch 61/100, Train Loss: 0.1431, Val Loss: 0.1707, Val Recall: 0.7677, Val F1-Score: 0.8020\nEpoch 62/100, Train Loss: 0.1449, Val Loss: 0.1706, Val Recall: 0.7672, Val F1-Score: 0.7999\nEpoch 63/100, Train Loss: 0.1474, Val Loss: 0.1705, Val Recall: 0.7661, Val F1-Score: 0.7959\nEpoch 64/100, Train Loss: 0.1449, Val Loss: 0.1705, Val Recall: 0.7654, Val F1-Score: 0.7939\nEpoch 65/100, Train Loss: 0.1428, Val Loss: 0.1704, Val Recall: 0.7654, Val F1-Score: 0.7939\nEpoch 66/100, Train Loss: 0.1416, Val Loss: 0.1704, Val Recall: 0.7654, Val F1-Score: 0.7939\nEpoch 67/100, Train Loss: 0.1378, Val Loss: 0.1703, Val Recall: 0.7654, Val F1-Score: 0.7939\nEpoch 68/100, Train Loss: 0.1441, Val Loss: 0.1703, Val Recall: 0.7712, Val F1-Score: 0.8000\nEpoch 69/100, Train Loss: 0.1429, Val Loss: 0.1703, Val Recall: 0.7712, Val F1-Score: 0.8000\nEpoch 70/100, Train Loss: 0.1448, Val Loss: 0.1704, Val Recall: 0.7712, Val F1-Score: 0.8000\nEpoch 71/100, Train Loss: 0.1415, Val Loss: 0.1704, Val Recall: 0.7712, Val F1-Score: 0.8000\nEarly stopping triggered\nFinal Validation Loss on SNLI: 0.1704\nFinal Validation Recall on SNLI: 0.7712\nFinal Validation F1-Score on SNLI: 0.8000\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[127], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m test_graph_snli \u001b[38;5;241m=\u001b[39m create_graph(X_test_snli, y_test_snli)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the function to train and evaluate for SNLI\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model_snli, val_loss_snli, val_accuracy_snli \u001b[38;5;241m=\u001b[39m train_and_evaluate(\n\u001b[1;32m      9\u001b[0m     train_graph_snli, test_graph_snli, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSNLI\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"],"ename":"ValueError","evalue":"too many values to unpack (expected 3)","output_type":"error"}]},{"cell_type":"code","source":"X_train_mnli_matched, X_test_mnli_matched, y_train_mnli_matched, y_test_mnli_matched = train_test_split(X_mnli_matched, y_mnli_matched, test_size=0.2, random_state=42)\n\n# Create graph data for MNLI Matched\ntrain_graph_mnli_matched = create_graph(X_train_mnli_matched, y_train_mnli_matched)\ntest_graph_mnli_matched = create_graph(X_test_mnli_matched, y_test_mnli_matched)\n\n# Call the function to train and evaluate for MNLI Matched\nmodel_mnli_matched, val_loss_mnli_matched, val_accuracy_mnli_matched = train_and_evaluate(\n    train_graph_mnli_matched, test_graph_mnli_matched, 'MNLI Matched'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:16:32.270283Z","iopub.execute_input":"2024-08-20T12:16:32.270917Z","iopub.status.idle":"2024-08-20T12:16:33.611789Z","shell.execute_reply.started":"2024-08-20T12:16:32.270882Z","shell.execute_reply":"2024-08-20T12:16:33.610541Z"},"trusted":true},"execution_count":128,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.1823, Val Loss: 1.0781, Val Recall: 0.3249, Val F1-Score: 0.0211\nEpoch 2/100, Train Loss: 1.1048, Val Loss: 1.0022, Val Recall: 0.2596, Val F1-Score: 0.0460\nEpoch 3/100, Train Loss: 1.0186, Val Loss: 0.9329, Val Recall: 0.5089, Val F1-Score: 0.5090\nEpoch 4/100, Train Loss: 0.9472, Val Loss: 0.8695, Val Recall: 0.6567, Val F1-Score: 0.6472\nEpoch 5/100, Train Loss: 0.8852, Val Loss: 0.8120, Val Recall: 0.6584, Val F1-Score: 0.6484\nEpoch 6/100, Train Loss: 0.8292, Val Loss: 0.7575, Val Recall: 0.6596, Val F1-Score: 0.6493\nEpoch 7/100, Train Loss: 0.7739, Val Loss: 0.7065, Val Recall: 0.6602, Val F1-Score: 0.6496\nEpoch 8/100, Train Loss: 0.7252, Val Loss: 0.6589, Val Recall: 0.6602, Val F1-Score: 0.6496\nEpoch 9/100, Train Loss: 0.6740, Val Loss: 0.6150, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 10/100, Train Loss: 0.6296, Val Loss: 0.5740, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 11/100, Train Loss: 0.5914, Val Loss: 0.5355, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 12/100, Train Loss: 0.5544, Val Loss: 0.4994, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 13/100, Train Loss: 0.5184, Val Loss: 0.4655, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 14/100, Train Loss: 0.4855, Val Loss: 0.4339, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 15/100, Train Loss: 0.4534, Val Loss: 0.4046, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 16/100, Train Loss: 0.4248, Val Loss: 0.3777, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 17/100, Train Loss: 0.4014, Val Loss: 0.3528, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 18/100, Train Loss: 0.3752, Val Loss: 0.3300, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 19/100, Train Loss: 0.3476, Val Loss: 0.3092, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 20/100, Train Loss: 0.3363, Val Loss: 0.2904, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 21/100, Train Loss: 0.3139, Val Loss: 0.2736, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 22/100, Train Loss: 0.3002, Val Loss: 0.2587, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 23/100, Train Loss: 0.2788, Val Loss: 0.2454, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 24/100, Train Loss: 0.2685, Val Loss: 0.2339, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 25/100, Train Loss: 0.2557, Val Loss: 0.2237, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 26/100, Train Loss: 0.2481, Val Loss: 0.2148, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 27/100, Train Loss: 0.2345, Val Loss: 0.2071, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 28/100, Train Loss: 0.2257, Val Loss: 0.2004, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 29/100, Train Loss: 0.2186, Val Loss: 0.1946, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 30/100, Train Loss: 0.2138, Val Loss: 0.1896, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 31/100, Train Loss: 0.2065, Val Loss: 0.1853, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 32/100, Train Loss: 0.1999, Val Loss: 0.1817, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 33/100, Train Loss: 0.1947, Val Loss: 0.1786, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 34/100, Train Loss: 0.1943, Val Loss: 0.1760, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 35/100, Train Loss: 0.1903, Val Loss: 0.1738, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 36/100, Train Loss: 0.1838, Val Loss: 0.1718, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 37/100, Train Loss: 0.1829, Val Loss: 0.1700, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 38/100, Train Loss: 0.1822, Val Loss: 0.1686, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 39/100, Train Loss: 0.1746, Val Loss: 0.1673, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 40/100, Train Loss: 0.1756, Val Loss: 0.1662, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 41/100, Train Loss: 0.1743, Val Loss: 0.1653, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 42/100, Train Loss: 0.1732, Val Loss: 0.1645, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 43/100, Train Loss: 0.1684, Val Loss: 0.1638, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 44/100, Train Loss: 0.1655, Val Loss: 0.1631, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 45/100, Train Loss: 0.1655, Val Loss: 0.1623, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 46/100, Train Loss: 0.1627, Val Loss: 0.1614, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 47/100, Train Loss: 0.1649, Val Loss: 0.1607, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 48/100, Train Loss: 0.1650, Val Loss: 0.1600, Val Recall: 0.6613, Val F1-Score: 0.6507\nEpoch 49/100, Train Loss: 0.1662, Val Loss: 0.1593, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 50/100, Train Loss: 0.1584, Val Loss: 0.1588, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 51/100, Train Loss: 0.1615, Val Loss: 0.1583, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 52/100, Train Loss: 0.1635, Val Loss: 0.1578, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 53/100, Train Loss: 0.1613, Val Loss: 0.1573, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 54/100, Train Loss: 0.1584, Val Loss: 0.1569, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 55/100, Train Loss: 0.1582, Val Loss: 0.1565, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 56/100, Train Loss: 0.1550, Val Loss: 0.1562, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 57/100, Train Loss: 0.1549, Val Loss: 0.1558, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 58/100, Train Loss: 0.1558, Val Loss: 0.1555, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 59/100, Train Loss: 0.1540, Val Loss: 0.1553, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 60/100, Train Loss: 0.1569, Val Loss: 0.1551, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 61/100, Train Loss: 0.1519, Val Loss: 0.1549, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 62/100, Train Loss: 0.1557, Val Loss: 0.1547, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 63/100, Train Loss: 0.1532, Val Loss: 0.1546, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 64/100, Train Loss: 0.1508, Val Loss: 0.1545, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 65/100, Train Loss: 0.1502, Val Loss: 0.1544, Val Recall: 0.6619, Val F1-Score: 0.6513\nEpoch 66/100, Train Loss: 0.1541, Val Loss: 0.1543, Val Recall: 0.6709, Val F1-Score: 0.6691\nEpoch 67/100, Train Loss: 0.1505, Val Loss: 0.1542, Val Recall: 0.6709, Val F1-Score: 0.6691\nEpoch 68/100, Train Loss: 0.1521, Val Loss: 0.1541, Val Recall: 0.6709, Val F1-Score: 0.6691\nEpoch 69/100, Train Loss: 0.1492, Val Loss: 0.1541, Val Recall: 0.6709, Val F1-Score: 0.6691\nEpoch 70/100, Train Loss: 0.1518, Val Loss: 0.1540, Val Recall: 0.6709, Val F1-Score: 0.6691\nEpoch 71/100, Train Loss: 0.1545, Val Loss: 0.1540, Val Recall: 0.6709, Val F1-Score: 0.6691\nEpoch 72/100, Train Loss: 0.1503, Val Loss: 0.1538, Val Recall: 0.6709, Val F1-Score: 0.6691\nEpoch 73/100, Train Loss: 0.1476, Val Loss: 0.1538, Val Recall: 0.6703, Val F1-Score: 0.6685\nEpoch 74/100, Train Loss: 0.1474, Val Loss: 0.1537, Val Recall: 0.6697, Val F1-Score: 0.6678\nEpoch 75/100, Train Loss: 0.1420, Val Loss: 0.1536, Val Recall: 0.6697, Val F1-Score: 0.6678\nEpoch 76/100, Train Loss: 0.1456, Val Loss: 0.1536, Val Recall: 0.6697, Val F1-Score: 0.6678\nEpoch 77/100, Train Loss: 0.1508, Val Loss: 0.1535, Val Recall: 0.6697, Val F1-Score: 0.6678\nEpoch 78/100, Train Loss: 0.1478, Val Loss: 0.1535, Val Recall: 0.6697, Val F1-Score: 0.6678\nEpoch 79/100, Train Loss: 0.1436, Val Loss: 0.1535, Val Recall: 0.6787, Val F1-Score: 0.6843\nEpoch 80/100, Train Loss: 0.1432, Val Loss: 0.1534, Val Recall: 0.6877, Val F1-Score: 0.7001\nEarly stopping triggered\nFinal Validation Loss on MNLI Matched: 0.1534\nFinal Validation Recall on MNLI Matched: 0.6877\nFinal Validation F1-Score on MNLI Matched: 0.7001\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[128], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m test_graph_mnli_matched \u001b[38;5;241m=\u001b[39m create_graph(X_test_mnli_matched, y_test_mnli_matched)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the function to train and evaluate for MNLI Matched\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model_mnli_matched, val_loss_mnli_matched, val_accuracy_mnli_matched \u001b[38;5;241m=\u001b[39m train_and_evaluate(\n\u001b[1;32m      9\u001b[0m     train_graph_mnli_matched, test_graph_mnli_matched, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNLI Matched\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"],"ename":"ValueError","evalue":"too many values to unpack (expected 3)","output_type":"error"}]},{"cell_type":"code","source":"X_train_mnli_mismatched, X_test_mnli_mismatched, y_train_mnli_mismatched, y_test_mnli_mismatched = train_test_split(X_mnli_mismatched, y_mnli_mismatched, test_size=0.2, random_state=42)\n\n# Create graph data for MNLI mismatched\ntrain_graph_mnli_mismatched = create_graph(X_train_mnli_mismatched, y_train_mnli_mismatched)\ntest_graph_mnli_mismatched = create_graph(X_test_mnli_mismatched, y_test_mnli_mismatched)\n\n# Call the function to train and evaluate for MNLI Matched\nmodel_mnli_mismatched, val_loss_mnli_mismatched, val_accuracy_mnli_mismatched = train_and_evaluate(\n    train_graph_mnli_mismatched, test_graph_mnli_mismatched, 'MNLI Mismatched'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:16:41.106556Z","iopub.execute_input":"2024-08-20T12:16:41.106948Z","iopub.status.idle":"2024-08-20T12:16:42.409520Z","shell.execute_reply.started":"2024-08-20T12:16:41.106915Z","shell.execute_reply":"2024-08-20T12:16:42.408068Z"},"trusted":true},"execution_count":129,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.0860, Val Loss: 1.0070, Val Recall: 0.0472, Val F1-Score: 0.0464\nEpoch 2/100, Train Loss: 1.0211, Val Loss: 0.9474, Val Recall: 0.3736, Val F1-Score: 0.2998\nEpoch 3/100, Train Loss: 0.9613, Val Loss: 0.8934, Val Recall: 0.6643, Val F1-Score: 0.6538\nEpoch 4/100, Train Loss: 0.9063, Val Loss: 0.8427, Val Recall: 0.6655, Val F1-Score: 0.6549\nEpoch 5/100, Train Loss: 0.8582, Val Loss: 0.7951, Val Recall: 0.6655, Val F1-Score: 0.6549\nEpoch 6/100, Train Loss: 0.8079, Val Loss: 0.7505, Val Recall: 0.6655, Val F1-Score: 0.6549\nEpoch 7/100, Train Loss: 0.7670, Val Loss: 0.7085, Val Recall: 0.6655, Val F1-Score: 0.6549\nEpoch 8/100, Train Loss: 0.7234, Val Loss: 0.6686, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 9/100, Train Loss: 0.6821, Val Loss: 0.6307, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 10/100, Train Loss: 0.6480, Val Loss: 0.5947, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 11/100, Train Loss: 0.6119, Val Loss: 0.5612, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 12/100, Train Loss: 0.5794, Val Loss: 0.5294, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 13/100, Train Loss: 0.5539, Val Loss: 0.4987, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 14/100, Train Loss: 0.5239, Val Loss: 0.4691, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 15/100, Train Loss: 0.4927, Val Loss: 0.4407, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 16/100, Train Loss: 0.4661, Val Loss: 0.4136, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 17/100, Train Loss: 0.4375, Val Loss: 0.3881, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 18/100, Train Loss: 0.4164, Val Loss: 0.3640, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 19/100, Train Loss: 0.3886, Val Loss: 0.3416, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 20/100, Train Loss: 0.3700, Val Loss: 0.3209, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 21/100, Train Loss: 0.3482, Val Loss: 0.3017, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 22/100, Train Loss: 0.3306, Val Loss: 0.2841, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 23/100, Train Loss: 0.3132, Val Loss: 0.2679, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 24/100, Train Loss: 0.2926, Val Loss: 0.2532, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 25/100, Train Loss: 0.2863, Val Loss: 0.2398, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 26/100, Train Loss: 0.2674, Val Loss: 0.2278, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 27/100, Train Loss: 0.2554, Val Loss: 0.2169, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 28/100, Train Loss: 0.2476, Val Loss: 0.2071, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 29/100, Train Loss: 0.2346, Val Loss: 0.1983, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 30/100, Train Loss: 0.2248, Val Loss: 0.1902, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 31/100, Train Loss: 0.2164, Val Loss: 0.1831, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 32/100, Train Loss: 0.2112, Val Loss: 0.1768, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 33/100, Train Loss: 0.2063, Val Loss: 0.1712, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 34/100, Train Loss: 0.2001, Val Loss: 0.1662, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 35/100, Train Loss: 0.1915, Val Loss: 0.1618, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 36/100, Train Loss: 0.1871, Val Loss: 0.1579, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 37/100, Train Loss: 0.1863, Val Loss: 0.1545, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 38/100, Train Loss: 0.1844, Val Loss: 0.1515, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 39/100, Train Loss: 0.1734, Val Loss: 0.1488, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 40/100, Train Loss: 0.1706, Val Loss: 0.1464, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 41/100, Train Loss: 0.1711, Val Loss: 0.1443, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 42/100, Train Loss: 0.1672, Val Loss: 0.1423, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 43/100, Train Loss: 0.1661, Val Loss: 0.1405, Val Recall: 0.6649, Val F1-Score: 0.6543\nEpoch 44/100, Train Loss: 0.1586, Val Loss: 0.1388, Val Recall: 0.6649, Val F1-Score: 0.6546\nEpoch 45/100, Train Loss: 0.1586, Val Loss: 0.1372, Val Recall: 0.6649, Val F1-Score: 0.6546\nEpoch 46/100, Train Loss: 0.1598, Val Loss: 0.1359, Val Recall: 0.6649, Val F1-Score: 0.6546\nEpoch 47/100, Train Loss: 0.1460, Val Loss: 0.1346, Val Recall: 0.6649, Val F1-Score: 0.6546\nEpoch 48/100, Train Loss: 0.1558, Val Loss: 0.1334, Val Recall: 0.6649, Val F1-Score: 0.6546\nEpoch 49/100, Train Loss: 0.1478, Val Loss: 0.1324, Val Recall: 0.6739, Val F1-Score: 0.6720\nEpoch 50/100, Train Loss: 0.1529, Val Loss: 0.1314, Val Recall: 0.6739, Val F1-Score: 0.6720\nEpoch 51/100, Train Loss: 0.1538, Val Loss: 0.1306, Val Recall: 0.6739, Val F1-Score: 0.6720\nEpoch 52/100, Train Loss: 0.1506, Val Loss: 0.1299, Val Recall: 0.6829, Val F1-Score: 0.6885\nEpoch 53/100, Train Loss: 0.1489, Val Loss: 0.1293, Val Recall: 0.6829, Val F1-Score: 0.6885\nEpoch 54/100, Train Loss: 0.1523, Val Loss: 0.1289, Val Recall: 0.6829, Val F1-Score: 0.6885\nEpoch 55/100, Train Loss: 0.1428, Val Loss: 0.1286, Val Recall: 0.6829, Val F1-Score: 0.6885\nEpoch 56/100, Train Loss: 0.1447, Val Loss: 0.1284, Val Recall: 0.6829, Val F1-Score: 0.6885\nEpoch 57/100, Train Loss: 0.1463, Val Loss: 0.1283, Val Recall: 0.6829, Val F1-Score: 0.6885\nEpoch 58/100, Train Loss: 0.1456, Val Loss: 0.1282, Val Recall: 0.6829, Val F1-Score: 0.6885\nEpoch 59/100, Train Loss: 0.1444, Val Loss: 0.1282, Val Recall: 0.6829, Val F1-Score: 0.6885\nEpoch 60/100, Train Loss: 0.1459, Val Loss: 0.1282, Val Recall: 0.6829, Val F1-Score: 0.6885\nEpoch 61/100, Train Loss: 0.1479, Val Loss: 0.1283, Val Recall: 0.6823, Val F1-Score: 0.6874\nEpoch 62/100, Train Loss: 0.1436, Val Loss: 0.1284, Val Recall: 0.6823, Val F1-Score: 0.6874\nEpoch 63/100, Train Loss: 0.1486, Val Loss: 0.1284, Val Recall: 0.6823, Val F1-Score: 0.6874\nEpoch 64/100, Train Loss: 0.1497, Val Loss: 0.1284, Val Recall: 0.6823, Val F1-Score: 0.6874\nEpoch 65/100, Train Loss: 0.1393, Val Loss: 0.1284, Val Recall: 0.6823, Val F1-Score: 0.6874\nEpoch 66/100, Train Loss: 0.1449, Val Loss: 0.1283, Val Recall: 0.6823, Val F1-Score: 0.6874\nEpoch 67/100, Train Loss: 0.1435, Val Loss: 0.1283, Val Recall: 0.6823, Val F1-Score: 0.6874\nEarly stopping triggered\nFinal Validation Loss on MNLI Mismatched: 0.1283\nFinal Validation Recall on MNLI Mismatched: 0.6823\nFinal Validation F1-Score on MNLI Mismatched: 0.6874\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[129], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m test_graph_mnli_mismatched \u001b[38;5;241m=\u001b[39m create_graph(X_test_mnli_mismatched, y_test_mnli_mismatched)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the function to train and evaluate for MNLI Matched\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model_mnli_mismatched, val_loss_mnli_mismatched, val_accuracy_mnli_mismatched \u001b[38;5;241m=\u001b[39m train_and_evaluate(\n\u001b[1;32m      9\u001b[0m     train_graph_mnli_mismatched, test_graph_mnli_mismatched, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMNLI Mismatched\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"],"ename":"ValueError","evalue":"too many values to unpack (expected 3)","output_type":"error"}]},{"cell_type":"code","source":"X_train_anli_r1, X_test_anli_r1, y_train_anli_r1, y_test_anli_r1 = train_test_split(X_anli_r1, y_anli_r1, test_size=0.2, random_state=42)\n\n# Create graph data for ANLI r1\ntrain_graph_anli_r1 = create_graph(X_train_anli_r1, y_train_anli_r1)\ntest_graph_anli_r1 = create_graph(X_test_anli_r1, y_test_anli_r1)\n\n# Call the function to train and evaluate for ANLI r1\nmodel_anli_r1, val_loss_anli_r1, val_accuracy_anli_r1 = train_and_evaluate(\n    train_graph_anli_r1, test_graph_anli_r1, 'ANLI r1'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:16:48.228548Z","iopub.execute_input":"2024-08-20T12:16:48.229542Z","iopub.status.idle":"2024-08-20T12:16:49.037488Z","shell.execute_reply.started":"2024-08-20T12:16:48.229504Z","shell.execute_reply":"2024-08-20T12:16:49.036288Z"},"trusted":true},"execution_count":130,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.2441, Val Loss: 1.1447, Val Recall: 0.3213, Val F1-Score: 0.2868\nEpoch 2/100, Train Loss: 1.1640, Val Loss: 1.0880, Val Recall: 0.3288, Val F1-Score: 0.2952\nEpoch 3/100, Train Loss: 1.1209, Val Loss: 1.0346, Val Recall: 0.3955, Val F1-Score: 0.3532\nEpoch 4/100, Train Loss: 1.1010, Val Loss: 0.9843, Val Recall: 0.5707, Val F1-Score: 0.4910\nEpoch 5/100, Train Loss: 1.0399, Val Loss: 0.9370, Val Recall: 0.7360, Val F1-Score: 0.7420\nEpoch 6/100, Train Loss: 0.9892, Val Loss: 0.8928, Val Recall: 0.7676, Val F1-Score: 0.7971\nEpoch 7/100, Train Loss: 0.9720, Val Loss: 0.8509, Val Recall: 0.7676, Val F1-Score: 0.7971\nEpoch 8/100, Train Loss: 0.9338, Val Loss: 0.8114, Val Recall: 0.7676, Val F1-Score: 0.7971\nEpoch 9/100, Train Loss: 0.8951, Val Loss: 0.7744, Val Recall: 0.7676, Val F1-Score: 0.7971\nEpoch 10/100, Train Loss: 0.8686, Val Loss: 0.7400, Val Recall: 0.7676, Val F1-Score: 0.7971\nEpoch 11/100, Train Loss: 0.8495, Val Loss: 0.7079, Val Recall: 0.7676, Val F1-Score: 0.7971\nEpoch 12/100, Train Loss: 0.8154, Val Loss: 0.6780, Val Recall: 0.7676, Val F1-Score: 0.7971\nEpoch 13/100, Train Loss: 0.7891, Val Loss: 0.6504, Val Recall: 0.7943, Val F1-Score: 0.8194\nEpoch 14/100, Train Loss: 0.7828, Val Loss: 0.6249, Val Recall: 0.7867, Val F1-Score: 0.8064\nEpoch 15/100, Train Loss: 0.7379, Val Loss: 0.6016, Val Recall: 0.7867, Val F1-Score: 0.8064\nEpoch 16/100, Train Loss: 0.7243, Val Loss: 0.5801, Val Recall: 0.7867, Val F1-Score: 0.8064\nEpoch 17/100, Train Loss: 0.6941, Val Loss: 0.5603, Val Recall: 0.7867, Val F1-Score: 0.8064\nEpoch 18/100, Train Loss: 0.6740, Val Loss: 0.5424, Val Recall: 0.7867, Val F1-Score: 0.8064\nEpoch 19/100, Train Loss: 0.6653, Val Loss: 0.5260, Val Recall: 0.7867, Val F1-Score: 0.8064\nEpoch 20/100, Train Loss: 0.6608, Val Loss: 0.5110, Val Recall: 0.7867, Val F1-Score: 0.8064\nEpoch 21/100, Train Loss: 0.6539, Val Loss: 0.4975, Val Recall: 0.8000, Val F1-Score: 0.8222\nEpoch 22/100, Train Loss: 0.6380, Val Loss: 0.4853, Val Recall: 0.8000, Val F1-Score: 0.8196\nEpoch 23/100, Train Loss: 0.6382, Val Loss: 0.4746, Val Recall: 0.7924, Val F1-Score: 0.8073\nEpoch 24/100, Train Loss: 0.6090, Val Loss: 0.4647, Val Recall: 0.8045, Val F1-Score: 0.8130\nEpoch 25/100, Train Loss: 0.6300, Val Loss: 0.4562, Val Recall: 0.7982, Val F1-Score: 0.7998\nEpoch 26/100, Train Loss: 0.6146, Val Loss: 0.4486, Val Recall: 0.8115, Val F1-Score: 0.8102\nEpoch 27/100, Train Loss: 0.6208, Val Loss: 0.4417, Val Recall: 0.8115, Val F1-Score: 0.8102\nEpoch 28/100, Train Loss: 0.5892, Val Loss: 0.4355, Val Recall: 0.7964, Val F1-Score: 0.7885\nEpoch 29/100, Train Loss: 0.6054, Val Loss: 0.4299, Val Recall: 0.7964, Val F1-Score: 0.7885\nEpoch 30/100, Train Loss: 0.5916, Val Loss: 0.4247, Val Recall: 0.7964, Val F1-Score: 0.7885\nEpoch 31/100, Train Loss: 0.5575, Val Loss: 0.4198, Val Recall: 0.7888, Val F1-Score: 0.7781\nEpoch 32/100, Train Loss: 0.6103, Val Loss: 0.4150, Val Recall: 0.8084, Val F1-Score: 0.7950\nEpoch 33/100, Train Loss: 0.5794, Val Loss: 0.4104, Val Recall: 0.7755, Val F1-Score: 0.7677\nEpoch 34/100, Train Loss: 0.5753, Val Loss: 0.4062, Val Recall: 0.7830, Val F1-Score: 0.7782\nEpoch 35/100, Train Loss: 0.5764, Val Loss: 0.4023, Val Recall: 0.7830, Val F1-Score: 0.7782\nEpoch 36/100, Train Loss: 0.5639, Val Loss: 0.3987, Val Recall: 0.7830, Val F1-Score: 0.7782\nEpoch 37/100, Train Loss: 0.5745, Val Loss: 0.3953, Val Recall: 0.7830, Val F1-Score: 0.7782\nEpoch 38/100, Train Loss: 0.5779, Val Loss: 0.3923, Val Recall: 0.7830, Val F1-Score: 0.7782\nEpoch 39/100, Train Loss: 0.5723, Val Loss: 0.3901, Val Recall: 0.7830, Val F1-Score: 0.7782\nEpoch 40/100, Train Loss: 0.5360, Val Loss: 0.3882, Val Recall: 0.7830, Val F1-Score: 0.7782\nEpoch 41/100, Train Loss: 0.5441, Val Loss: 0.3863, Val Recall: 0.7830, Val F1-Score: 0.7782\nEpoch 42/100, Train Loss: 0.5621, Val Loss: 0.3848, Val Recall: 0.7830, Val F1-Score: 0.7782\nEpoch 43/100, Train Loss: 0.5493, Val Loss: 0.3840, Val Recall: 0.8027, Val F1-Score: 0.7952\nEpoch 44/100, Train Loss: 0.5725, Val Loss: 0.3837, Val Recall: 0.8027, Val F1-Score: 0.7952\nEpoch 45/100, Train Loss: 0.5588, Val Loss: 0.3835, Val Recall: 0.8027, Val F1-Score: 0.7952\nEpoch 46/100, Train Loss: 0.5530, Val Loss: 0.3830, Val Recall: 0.8027, Val F1-Score: 0.7952\nEpoch 47/100, Train Loss: 0.5243, Val Loss: 0.3827, Val Recall: 0.8027, Val F1-Score: 0.7952\nEpoch 48/100, Train Loss: 0.5581, Val Loss: 0.3828, Val Recall: 0.8102, Val F1-Score: 0.8051\nEpoch 49/100, Train Loss: 0.5262, Val Loss: 0.3831, Val Recall: 0.8102, Val F1-Score: 0.8051\nEpoch 50/100, Train Loss: 0.5452, Val Loss: 0.3834, Val Recall: 0.8102, Val F1-Score: 0.8051\nEpoch 51/100, Train Loss: 0.5576, Val Loss: 0.3834, Val Recall: 0.8102, Val F1-Score: 0.8051\nEpoch 52/100, Train Loss: 0.5289, Val Loss: 0.3829, Val Recall: 0.8102, Val F1-Score: 0.8051\nEpoch 53/100, Train Loss: 0.5352, Val Loss: 0.3822, Val Recall: 0.8102, Val F1-Score: 0.8051\nEpoch 54/100, Train Loss: 0.5394, Val Loss: 0.3817, Val Recall: 0.8102, Val F1-Score: 0.8051\nEpoch 55/100, Train Loss: 0.5475, Val Loss: 0.3812, Val Recall: 0.8102, Val F1-Score: 0.8051\nEpoch 56/100, Train Loss: 0.5328, Val Loss: 0.3804, Val Recall: 0.8102, Val F1-Score: 0.8051\nEpoch 57/100, Train Loss: 0.5444, Val Loss: 0.3797, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 58/100, Train Loss: 0.5233, Val Loss: 0.3787, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 59/100, Train Loss: 0.5574, Val Loss: 0.3781, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 60/100, Train Loss: 0.5145, Val Loss: 0.3776, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 61/100, Train Loss: 0.5442, Val Loss: 0.3770, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 62/100, Train Loss: 0.5382, Val Loss: 0.3766, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 63/100, Train Loss: 0.5450, Val Loss: 0.3769, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 64/100, Train Loss: 0.5381, Val Loss: 0.3773, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 65/100, Train Loss: 0.5200, Val Loss: 0.3779, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 66/100, Train Loss: 0.5380, Val Loss: 0.3790, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 67/100, Train Loss: 0.5173, Val Loss: 0.3801, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 68/100, Train Loss: 0.5589, Val Loss: 0.3809, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 69/100, Train Loss: 0.5596, Val Loss: 0.3816, Val Recall: 0.7906, Val F1-Score: 0.7881\nEpoch 70/100, Train Loss: 0.5601, Val Loss: 0.3824, Val Recall: 0.7906, Val F1-Score: 0.7881\nEarly stopping triggered\nFinal Validation Loss on ANLI r1: 0.3824\nFinal Validation Recall on ANLI r1: 0.7906\nFinal Validation F1-Score on ANLI r1: 0.7881\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[130], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m test_graph_anli_r1 \u001b[38;5;241m=\u001b[39m create_graph(X_test_anli_r1, y_test_anli_r1)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the function to train and evaluate for ANLI r1\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model_anli_r1, val_loss_anli_r1, val_accuracy_anli_r1 \u001b[38;5;241m=\u001b[39m train_and_evaluate(\n\u001b[1;32m      9\u001b[0m     train_graph_anli_r1, test_graph_anli_r1, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANLI r1\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"],"ename":"ValueError","evalue":"too many values to unpack (expected 3)","output_type":"error"}]},{"cell_type":"code","source":"X_train_anli_r2, X_test_anli_r2, y_train_anli_r2, y_test_anli_r2 = train_test_split(X_anli_r2, y_anli_r2, test_size=0.2, random_state=42)\n\n# Create graph data for ANLI r2\ntrain_graph_anli_r2 = create_graph(X_train_anli_r2, y_train_anli_r2)\ntest_graph_anli_r2 = create_graph(X_test_anli_r2, y_test_anli_r2)\n\n# Call the function to train and evaluate for ANLI r2\nmodel_anli_r2, val_loss_anli_r2, val_accuracy_anli_r2 = train_and_evaluate(\n    train_graph_anli_r2, test_graph_anli_r2, 'ANLI r2'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:16:51.297362Z","iopub.execute_input":"2024-08-20T12:16:51.298220Z","iopub.status.idle":"2024-08-20T12:16:51.816368Z","shell.execute_reply.started":"2024-08-20T12:16:51.298187Z","shell.execute_reply":"2024-08-20T12:16:51.815197Z"},"trusted":true},"execution_count":131,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.0565, Val Loss: 0.9952, Val Recall: 0.5221, Val F1-Score: 0.5303\nEpoch 2/100, Train Loss: 1.0377, Val Loss: 0.9714, Val Recall: 0.5759, Val F1-Score: 0.5804\nEpoch 3/100, Train Loss: 1.0116, Val Loss: 0.9490, Val Recall: 0.6116, Val F1-Score: 0.6111\nEpoch 4/100, Train Loss: 0.9853, Val Loss: 0.9278, Val Recall: 0.6178, Val F1-Score: 0.6161\nEpoch 5/100, Train Loss: 0.9662, Val Loss: 0.9078, Val Recall: 0.6472, Val F1-Score: 0.6433\nEpoch 6/100, Train Loss: 0.9243, Val Loss: 0.8889, Val Recall: 0.6327, Val F1-Score: 0.6242\nEpoch 7/100, Train Loss: 0.9384, Val Loss: 0.8711, Val Recall: 0.6446, Val F1-Score: 0.6351\nEpoch 8/100, Train Loss: 0.9065, Val Loss: 0.8541, Val Recall: 0.6446, Val F1-Score: 0.6351\nEpoch 9/100, Train Loss: 0.8827, Val Loss: 0.8375, Val Recall: 0.6446, Val F1-Score: 0.6351\nEpoch 10/100, Train Loss: 0.8686, Val Loss: 0.8213, Val Recall: 0.6446, Val F1-Score: 0.6351\nEpoch 11/100, Train Loss: 0.8605, Val Loss: 0.8054, Val Recall: 0.6446, Val F1-Score: 0.6351\nEpoch 12/100, Train Loss: 0.8344, Val Loss: 0.7896, Val Recall: 0.6679, Val F1-Score: 0.6641\nEpoch 13/100, Train Loss: 0.8270, Val Loss: 0.7742, Val Recall: 0.6736, Val F1-Score: 0.6730\nEpoch 14/100, Train Loss: 0.8186, Val Loss: 0.7595, Val Recall: 0.6881, Val F1-Score: 0.6904\nEpoch 15/100, Train Loss: 0.8010, Val Loss: 0.7457, Val Recall: 0.7026, Val F1-Score: 0.7071\nEpoch 16/100, Train Loss: 0.7776, Val Loss: 0.7330, Val Recall: 0.7026, Val F1-Score: 0.7071\nEpoch 17/100, Train Loss: 0.7920, Val Loss: 0.7218, Val Recall: 0.7171, Val F1-Score: 0.7231\nEpoch 18/100, Train Loss: 0.7817, Val Loss: 0.7115, Val Recall: 0.7083, Val F1-Score: 0.7127\nEpoch 19/100, Train Loss: 0.7569, Val Loss: 0.7021, Val Recall: 0.7228, Val F1-Score: 0.7279\nEpoch 20/100, Train Loss: 0.7791, Val Loss: 0.6938, Val Recall: 0.7228, Val F1-Score: 0.7279\nEpoch 21/100, Train Loss: 0.7486, Val Loss: 0.6866, Val Recall: 0.7228, Val F1-Score: 0.7279\nEpoch 22/100, Train Loss: 0.7543, Val Loss: 0.6805, Val Recall: 0.7228, Val F1-Score: 0.7279\nEpoch 23/100, Train Loss: 0.7680, Val Loss: 0.6756, Val Recall: 0.7347, Val F1-Score: 0.7376\nEpoch 24/100, Train Loss: 0.7482, Val Loss: 0.6713, Val Recall: 0.7347, Val F1-Score: 0.7376\nEpoch 25/100, Train Loss: 0.7296, Val Loss: 0.6678, Val Recall: 0.7347, Val F1-Score: 0.7376\nEpoch 26/100, Train Loss: 0.7366, Val Loss: 0.6650, Val Recall: 0.7347, Val F1-Score: 0.7376\nEpoch 27/100, Train Loss: 0.7231, Val Loss: 0.6628, Val Recall: 0.7347, Val F1-Score: 0.7376\nEpoch 28/100, Train Loss: 0.7317, Val Loss: 0.6609, Val Recall: 0.7347, Val F1-Score: 0.7376\nEpoch 29/100, Train Loss: 0.7197, Val Loss: 0.6596, Val Recall: 0.7259, Val F1-Score: 0.7262\nEpoch 30/100, Train Loss: 0.7292, Val Loss: 0.6584, Val Recall: 0.7259, Val F1-Score: 0.7262\nEpoch 31/100, Train Loss: 0.7211, Val Loss: 0.6572, Val Recall: 0.7259, Val F1-Score: 0.7262\nEpoch 32/100, Train Loss: 0.7037, Val Loss: 0.6560, Val Recall: 0.7259, Val F1-Score: 0.7262\nEpoch 33/100, Train Loss: 0.7223, Val Loss: 0.6550, Val Recall: 0.7259, Val F1-Score: 0.7262\nEpoch 34/100, Train Loss: 0.7265, Val Loss: 0.6542, Val Recall: 0.7259, Val F1-Score: 0.7262\nEpoch 35/100, Train Loss: 0.6901, Val Loss: 0.6536, Val Recall: 0.7259, Val F1-Score: 0.7262\nEpoch 36/100, Train Loss: 0.6806, Val Loss: 0.6534, Val Recall: 0.7259, Val F1-Score: 0.7262\nEpoch 37/100, Train Loss: 0.7018, Val Loss: 0.6534, Val Recall: 0.7347, Val F1-Score: 0.7361\nEpoch 38/100, Train Loss: 0.7103, Val Loss: 0.6537, Val Recall: 0.7347, Val F1-Score: 0.7361\nEpoch 39/100, Train Loss: 0.7046, Val Loss: 0.6541, Val Recall: 0.7347, Val F1-Score: 0.7361\nEpoch 40/100, Train Loss: 0.7000, Val Loss: 0.6547, Val Recall: 0.7347, Val F1-Score: 0.7361\nEpoch 41/100, Train Loss: 0.7027, Val Loss: 0.6557, Val Recall: 0.7347, Val F1-Score: 0.7361\nEpoch 42/100, Train Loss: 0.6971, Val Loss: 0.6568, Val Recall: 0.7347, Val F1-Score: 0.7361\nEpoch 43/100, Train Loss: 0.6868, Val Loss: 0.6581, Val Recall: 0.7347, Val F1-Score: 0.7361\nEpoch 44/100, Train Loss: 0.6892, Val Loss: 0.6593, Val Recall: 0.7347, Val F1-Score: 0.7361\nEpoch 45/100, Train Loss: 0.7007, Val Loss: 0.6606, Val Recall: 0.7347, Val F1-Score: 0.7361\nEarly stopping triggered\nFinal Validation Loss on ANLI r2: 0.6606\nFinal Validation Recall on ANLI r2: 0.7347\nFinal Validation F1-Score on ANLI r2: 0.7361\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[131], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m test_graph_anli_r2 \u001b[38;5;241m=\u001b[39m create_graph(X_test_anli_r2, y_test_anli_r2)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the function to train and evaluate for ANLI r2\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model_anli_r2, val_loss_anli_r2, val_accuracy_anli_r2 \u001b[38;5;241m=\u001b[39m train_and_evaluate(\n\u001b[1;32m      9\u001b[0m     train_graph_anli_r2, test_graph_anli_r2, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANLI r2\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"],"ename":"ValueError","evalue":"too many values to unpack (expected 3)","output_type":"error"}]},{"cell_type":"code","source":"X_train_anli_r3, X_test_anli_r3, y_train_anli_r3, y_test_anli_r3 = train_test_split(X_anli_r3, y_anli_r3, test_size=0.2, random_state=42)\n\n# Create graph data for ANLI r3\ntrain_graph_anli_r3 = create_graph(X_train_anli_r3, y_train_anli_r3)\ntest_graph_anli_r3 = create_graph(X_test_anli_r3, y_test_anli_r3)\n\n# Call the function to train and evaluate for ANLI r3\nmodel_anli_r3, val_loss_anli_r3, val_accuracy_anli_r3 = train_and_evaluate(\n    train_graph_anli_r3, test_graph_anli_r3, 'ANLI r3'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:16:54.305968Z","iopub.execute_input":"2024-08-20T12:16:54.306958Z","iopub.status.idle":"2024-08-20T12:16:55.111849Z","shell.execute_reply.started":"2024-08-20T12:16:54.306916Z","shell.execute_reply":"2024-08-20T12:16:55.110624Z"},"trusted":true},"execution_count":132,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.2870, Val Loss: 1.2102, Val Recall: 0.2776, Val F1-Score: 0.2218\nEpoch 2/100, Train Loss: 1.2423, Val Loss: 1.1755, Val Recall: 0.3464, Val F1-Score: 0.2635\nEpoch 3/100, Train Loss: 1.2040, Val Loss: 1.1431, Val Recall: 0.3528, Val F1-Score: 0.2648\nEpoch 4/100, Train Loss: 1.1819, Val Loss: 1.1129, Val Recall: 0.3528, Val F1-Score: 0.2645\nEpoch 5/100, Train Loss: 1.1277, Val Loss: 1.0843, Val Recall: 0.4136, Val F1-Score: 0.3523\nEpoch 6/100, Train Loss: 1.1132, Val Loss: 1.0571, Val Recall: 0.4210, Val F1-Score: 0.3638\nEpoch 7/100, Train Loss: 1.0782, Val Loss: 1.0315, Val Recall: 0.5114, Val F1-Score: 0.4779\nEpoch 8/100, Train Loss: 1.0572, Val Loss: 1.0075, Val Recall: 0.6792, Val F1-Score: 0.6713\nEpoch 9/100, Train Loss: 1.0410, Val Loss: 0.9848, Val Recall: 0.7014, Val F1-Score: 0.6968\nEpoch 10/100, Train Loss: 0.9922, Val Loss: 0.9630, Val Recall: 0.6744, Val F1-Score: 0.6663\nEpoch 11/100, Train Loss: 0.9855, Val Loss: 0.9419, Val Recall: 0.6818, Val F1-Score: 0.6738\nEpoch 12/100, Train Loss: 0.9524, Val Loss: 0.9218, Val Recall: 0.6588, Val F1-Score: 0.6448\nEpoch 13/100, Train Loss: 0.9512, Val Loss: 0.9026, Val Recall: 0.6766, Val F1-Score: 0.6628\nEpoch 14/100, Train Loss: 0.9187, Val Loss: 0.8844, Val Recall: 0.6870, Val F1-Score: 0.6716\nEpoch 15/100, Train Loss: 0.9106, Val Loss: 0.8670, Val Recall: 0.6964, Val F1-Score: 0.6737\nEpoch 16/100, Train Loss: 0.9040, Val Loss: 0.8505, Val Recall: 0.6744, Val F1-Score: 0.6483\nEpoch 17/100, Train Loss: 0.8720, Val Loss: 0.8347, Val Recall: 0.6744, Val F1-Score: 0.6483\nEpoch 18/100, Train Loss: 0.8747, Val Loss: 0.8192, Val Recall: 0.6859, Val F1-Score: 0.6644\nEpoch 19/100, Train Loss: 0.8351, Val Loss: 0.8038, Val Recall: 0.6859, Val F1-Score: 0.6644\nEpoch 20/100, Train Loss: 0.8501, Val Loss: 0.7889, Val Recall: 0.6859, Val F1-Score: 0.6644\nEpoch 21/100, Train Loss: 0.8158, Val Loss: 0.7744, Val Recall: 0.6859, Val F1-Score: 0.6644\nEpoch 22/100, Train Loss: 0.7922, Val Loss: 0.7603, Val Recall: 0.6933, Val F1-Score: 0.6724\nEpoch 23/100, Train Loss: 0.7954, Val Loss: 0.7467, Val Recall: 0.6933, Val F1-Score: 0.6724\nEpoch 24/100, Train Loss: 0.7777, Val Loss: 0.7335, Val Recall: 0.6933, Val F1-Score: 0.6724\nEpoch 25/100, Train Loss: 0.7546, Val Loss: 0.7210, Val Recall: 0.6933, Val F1-Score: 0.6724\nEpoch 26/100, Train Loss: 0.7542, Val Loss: 0.7094, Val Recall: 0.7048, Val F1-Score: 0.6880\nEpoch 27/100, Train Loss: 0.7781, Val Loss: 0.6993, Val Recall: 0.7048, Val F1-Score: 0.6880\nEpoch 28/100, Train Loss: 0.7467, Val Loss: 0.6905, Val Recall: 0.7048, Val F1-Score: 0.6880\nEpoch 29/100, Train Loss: 0.7084, Val Loss: 0.6824, Val Recall: 0.7048, Val F1-Score: 0.6880\nEpoch 30/100, Train Loss: 0.7393, Val Loss: 0.6756, Val Recall: 0.7163, Val F1-Score: 0.7034\nEpoch 31/100, Train Loss: 0.7107, Val Loss: 0.6696, Val Recall: 0.7163, Val F1-Score: 0.7034\nEpoch 32/100, Train Loss: 0.7098, Val Loss: 0.6644, Val Recall: 0.7174, Val F1-Score: 0.7099\nEpoch 33/100, Train Loss: 0.6994, Val Loss: 0.6600, Val Recall: 0.7174, Val F1-Score: 0.7099\nEpoch 34/100, Train Loss: 0.7176, Val Loss: 0.6564, Val Recall: 0.7330, Val F1-Score: 0.7303\nEpoch 35/100, Train Loss: 0.7070, Val Loss: 0.6535, Val Recall: 0.7256, Val F1-Score: 0.7224\nEpoch 36/100, Train Loss: 0.6957, Val Loss: 0.6508, Val Recall: 0.7371, Val F1-Score: 0.7349\nEpoch 37/100, Train Loss: 0.7063, Val Loss: 0.6482, Val Recall: 0.7371, Val F1-Score: 0.7349\nEpoch 38/100, Train Loss: 0.6839, Val Loss: 0.6458, Val Recall: 0.7371, Val F1-Score: 0.7349\nEpoch 39/100, Train Loss: 0.7032, Val Loss: 0.6438, Val Recall: 0.7297, Val F1-Score: 0.7269\nEpoch 40/100, Train Loss: 0.6814, Val Loss: 0.6421, Val Recall: 0.7182, Val F1-Score: 0.7130\nEpoch 41/100, Train Loss: 0.6852, Val Loss: 0.6405, Val Recall: 0.7286, Val F1-Score: 0.7220\nEpoch 42/100, Train Loss: 0.6885, Val Loss: 0.6392, Val Recall: 0.7349, Val F1-Score: 0.7237\nEpoch 43/100, Train Loss: 0.6759, Val Loss: 0.6383, Val Recall: 0.7245, Val F1-Score: 0.7156\nEpoch 44/100, Train Loss: 0.6833, Val Loss: 0.6376, Val Recall: 0.7245, Val F1-Score: 0.7156\nEpoch 45/100, Train Loss: 0.6939, Val Loss: 0.6373, Val Recall: 0.7245, Val F1-Score: 0.7153\nEpoch 46/100, Train Loss: 0.6817, Val Loss: 0.6372, Val Recall: 0.7245, Val F1-Score: 0.7153\nEpoch 47/100, Train Loss: 0.6873, Val Loss: 0.6372, Val Recall: 0.7245, Val F1-Score: 0.7153\nEpoch 48/100, Train Loss: 0.6753, Val Loss: 0.6373, Val Recall: 0.7245, Val F1-Score: 0.7153\nEpoch 49/100, Train Loss: 0.7019, Val Loss: 0.6375, Val Recall: 0.7245, Val F1-Score: 0.7153\nEpoch 50/100, Train Loss: 0.6778, Val Loss: 0.6378, Val Recall: 0.7245, Val F1-Score: 0.7153\nEpoch 51/100, Train Loss: 0.6630, Val Loss: 0.6383, Val Recall: 0.7171, Val F1-Score: 0.7072\nEpoch 52/100, Train Loss: 0.6786, Val Loss: 0.6391, Val Recall: 0.7171, Val F1-Score: 0.7072\nEpoch 53/100, Train Loss: 0.6726, Val Loss: 0.6395, Val Recall: 0.7171, Val F1-Score: 0.7072\nEpoch 54/100, Train Loss: 0.6547, Val Loss: 0.6397, Val Recall: 0.7171, Val F1-Score: 0.7072\nEarly stopping triggered\nFinal Validation Loss on ANLI r3: 0.6397\nFinal Validation Recall on ANLI r3: 0.7171\nFinal Validation F1-Score on ANLI r3: 0.7072\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[132], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m test_graph_anli_r3 \u001b[38;5;241m=\u001b[39m create_graph(X_test_anli_r3, y_test_anli_r3)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Call the function to train and evaluate for ANLI r3\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model_anli_r3, val_loss_anli_r3, val_accuracy_anli_r3 \u001b[38;5;241m=\u001b[39m train_and_evaluate(\n\u001b[1;32m      9\u001b[0m     train_graph_anli_r3, test_graph_anli_r3, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mANLI r3\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     10\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"],"ename":"ValueError","evalue":"too many values to unpack (expected 3)","output_type":"error"}]},{"cell_type":"code","source":"import torch\n\n# Convert NumPy arrays to PyTorch tensors\nX_train_snli_tensor = torch.tensor(X_train_snli)\nX_train_mnli_matched_tensor = torch.tensor(X_train_mnli_matched)\nX_train_mnli_mismatched_tensor = torch.tensor(X_train_mnli_mismatched)\nX_train_anli_r1_tensor = torch.tensor(X_train_anli_r1)\nX_train_anli_r2_tensor = torch.tensor(X_train_anli_r2)\nX_train_anli_r3_tensor = torch.tensor(X_train_anli_r3)\n\ny_train_snli_tensor = torch.tensor(y_train_snli)\ny_train_mnli_matched_tensor = torch.tensor(y_train_mnli_matched)\ny_train_mnli_mismatched_tensor = torch.tensor(y_train_mnli_mismatched)\ny_train_anli_r1_tensor = torch.tensor(y_train_anli_r1)\ny_train_anli_r2_tensor = torch.tensor(y_train_anli_r2)\ny_train_anli_r3_tensor = torch.tensor(y_train_anli_r3)\n\n# Concatenate X_train and y_train for all tasks\nX_train_all = torch.cat([X_train_snli_tensor, X_train_mnli_matched_tensor, X_train_mnli_mismatched_tensor, X_train_anli_r1_tensor, X_train_anli_r2_tensor, X_train_anli_r3_tensor], dim=0)\ny_train_all = torch.cat([y_train_snli_tensor, y_train_mnli_matched_tensor, y_train_mnli_mismatched_tensor, y_train_anli_r1_tensor, y_train_anli_r2_tensor, y_train_anli_r3_tensor], dim=0)\n\n# Convert NumPy arrays to PyTorch tensors for test data\nX_test_snli_tensor = torch.tensor(X_test_snli)\nX_test_mnli_matched_tensor = torch.tensor(X_test_mnli_matched)\nX_test_mnli_mismatched_tensor = torch.tensor(X_test_mnli_mismatched)\nX_test_anli_r1_tensor = torch.tensor(X_test_anli_r1)\nX_test_anli_r2_tensor = torch.tensor(X_test_anli_r2)\nX_test_anli_r3_tensor = torch.tensor(X_test_anli_r3)\n\ny_test_snli_tensor = torch.tensor(y_test_snli)\ny_test_mnli_matched_tensor = torch.tensor(y_test_mnli_matched)\ny_test_mnli_mismatched_tensor = torch.tensor(y_test_mnli_mismatched)\ny_test_anli_r1_tensor = torch.tensor(y_test_anli_r1)\ny_test_anli_r2_tensor = torch.tensor(y_test_anli_r2)\ny_test_anli_r3_tensor = torch.tensor(y_test_anli_r3)\n\n# Concatenate X_test and y_test for all tasks\nX_test_all = torch.cat([X_test_snli_tensor, X_test_mnli_matched_tensor, X_test_mnli_mismatched_tensor, X_test_anli_r1_tensor, X_test_anli_r2_tensor, X_test_anli_r3_tensor], dim=0)\ny_test_all = torch.cat([y_test_snli_tensor, y_test_mnli_matched_tensor, y_test_mnli_mismatched_tensor, y_test_anli_r1_tensor, y_test_anli_r2_tensor, y_test_anli_r3_tensor], dim=0)\n\n# Create graph data for ALL combined data\ntrain_graph_all = create_graph(X_train_all, y_train_all)\ntest_graph_all = create_graph(X_test_all, y_test_all)\n\n# Train and evaluate the model on the combined dataset\nmodel_all, final_val_loss_all, final_val_accuracy_all = train_and_evaluate(\n    train_graph_all, test_graph_all, 'Combined Tasks'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:16:58.933554Z","iopub.execute_input":"2024-08-20T12:16:58.933940Z","iopub.status.idle":"2024-08-20T12:17:00.821494Z","shell.execute_reply.started":"2024-08-20T12:16:58.933909Z","shell.execute_reply":"2024-08-20T12:17:00.820202Z"},"trusted":true},"execution_count":133,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_34/3613549310.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  graph_data = Data(x=torch.tensor(data, dtype=torch.float), edge_index=edge_index, y=torch.tensor(labels, dtype=torch.long))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/100, Train Loss: 0.8796, Val Loss: 0.8055, Val Recall: 0.6957, Val F1-Score: 0.7034\nEpoch 2/100, Train Loss: 0.8194, Val Loss: 0.7507, Val Recall: 0.6753, Val F1-Score: 0.6774\nEpoch 3/100, Train Loss: 0.7670, Val Loss: 0.6996, Val Recall: 0.6603, Val F1-Score: 0.6467\nEpoch 4/100, Train Loss: 0.7137, Val Loss: 0.6520, Val Recall: 0.6585, Val F1-Score: 0.6409\nEpoch 5/100, Train Loss: 0.6682, Val Loss: 0.6078, Val Recall: 0.6583, Val F1-Score: 0.6406\nEpoch 6/100, Train Loss: 0.6260, Val Loss: 0.5669, Val Recall: 0.6584, Val F1-Score: 0.6407\nEpoch 7/100, Train Loss: 0.5838, Val Loss: 0.5295, Val Recall: 0.6586, Val F1-Score: 0.6409\nEpoch 8/100, Train Loss: 0.5476, Val Loss: 0.4962, Val Recall: 0.6588, Val F1-Score: 0.6411\nEpoch 9/100, Train Loss: 0.5152, Val Loss: 0.4658, Val Recall: 0.6590, Val F1-Score: 0.6413\nEpoch 10/100, Train Loss: 0.4860, Val Loss: 0.4374, Val Recall: 0.6592, Val F1-Score: 0.6415\nEpoch 11/100, Train Loss: 0.4580, Val Loss: 0.4111, Val Recall: 0.6595, Val F1-Score: 0.6418\nEpoch 12/100, Train Loss: 0.4324, Val Loss: 0.3869, Val Recall: 0.6594, Val F1-Score: 0.6416\nEpoch 13/100, Train Loss: 0.4075, Val Loss: 0.3646, Val Recall: 0.6594, Val F1-Score: 0.6416\nEpoch 14/100, Train Loss: 0.3897, Val Loss: 0.3441, Val Recall: 0.6594, Val F1-Score: 0.6416\nEpoch 15/100, Train Loss: 0.3645, Val Loss: 0.3256, Val Recall: 0.6595, Val F1-Score: 0.6418\nEpoch 16/100, Train Loss: 0.3467, Val Loss: 0.3089, Val Recall: 0.6595, Val F1-Score: 0.6418\nEpoch 17/100, Train Loss: 0.3332, Val Loss: 0.2941, Val Recall: 0.6595, Val F1-Score: 0.6418\nEpoch 18/100, Train Loss: 0.3140, Val Loss: 0.2808, Val Recall: 0.6597, Val F1-Score: 0.6420\nEpoch 19/100, Train Loss: 0.3018, Val Loss: 0.2692, Val Recall: 0.6601, Val F1-Score: 0.6423\nEpoch 20/100, Train Loss: 0.2895, Val Loss: 0.2589, Val Recall: 0.6597, Val F1-Score: 0.6420\nEpoch 21/100, Train Loss: 0.2778, Val Loss: 0.2500, Val Recall: 0.6599, Val F1-Score: 0.6422\nEpoch 22/100, Train Loss: 0.2695, Val Loss: 0.2421, Val Recall: 0.6599, Val F1-Score: 0.6422\nEpoch 23/100, Train Loss: 0.2599, Val Loss: 0.2353, Val Recall: 0.6599, Val F1-Score: 0.6422\nEpoch 24/100, Train Loss: 0.2531, Val Loss: 0.2294, Val Recall: 0.6597, Val F1-Score: 0.6420\nEpoch 25/100, Train Loss: 0.2454, Val Loss: 0.2243, Val Recall: 0.6599, Val F1-Score: 0.6421\nEpoch 26/100, Train Loss: 0.2421, Val Loss: 0.2199, Val Recall: 0.6599, Val F1-Score: 0.6421\nEpoch 27/100, Train Loss: 0.2374, Val Loss: 0.2160, Val Recall: 0.6601, Val F1-Score: 0.6423\nEpoch 28/100, Train Loss: 0.2328, Val Loss: 0.2127, Val Recall: 0.6617, Val F1-Score: 0.6457\nEpoch 29/100, Train Loss: 0.2273, Val Loss: 0.2098, Val Recall: 0.6650, Val F1-Score: 0.6524\nEpoch 30/100, Train Loss: 0.2236, Val Loss: 0.2072, Val Recall: 0.6698, Val F1-Score: 0.6620\nEpoch 31/100, Train Loss: 0.2214, Val Loss: 0.2050, Val Recall: 0.6877, Val F1-Score: 0.6948\nEpoch 32/100, Train Loss: 0.2183, Val Loss: 0.2029, Val Recall: 0.6976, Val F1-Score: 0.7116\nEpoch 33/100, Train Loss: 0.2180, Val Loss: 0.2011, Val Recall: 0.7006, Val F1-Score: 0.7162\nEpoch 34/100, Train Loss: 0.2125, Val Loss: 0.1995, Val Recall: 0.7023, Val F1-Score: 0.7184\nEpoch 35/100, Train Loss: 0.2122, Val Loss: 0.1980, Val Recall: 0.7052, Val F1-Score: 0.7230\nEpoch 36/100, Train Loss: 0.2089, Val Loss: 0.1967, Val Recall: 0.7115, Val F1-Score: 0.7316\nEpoch 37/100, Train Loss: 0.2065, Val Loss: 0.1955, Val Recall: 0.7144, Val F1-Score: 0.7353\nEpoch 38/100, Train Loss: 0.2079, Val Loss: 0.1944, Val Recall: 0.7188, Val F1-Score: 0.7409\nEpoch 39/100, Train Loss: 0.2081, Val Loss: 0.1935, Val Recall: 0.7236, Val F1-Score: 0.7473\nEpoch 40/100, Train Loss: 0.2047, Val Loss: 0.1927, Val Recall: 0.7275, Val F1-Score: 0.7510\nEpoch 41/100, Train Loss: 0.2055, Val Loss: 0.1921, Val Recall: 0.7314, Val F1-Score: 0.7545\nEpoch 42/100, Train Loss: 0.2029, Val Loss: 0.1915, Val Recall: 0.7330, Val F1-Score: 0.7559\nEpoch 43/100, Train Loss: 0.2019, Val Loss: 0.1911, Val Recall: 0.7330, Val F1-Score: 0.7553\nEpoch 44/100, Train Loss: 0.2053, Val Loss: 0.1907, Val Recall: 0.7325, Val F1-Score: 0.7542\nEpoch 45/100, Train Loss: 0.2011, Val Loss: 0.1904, Val Recall: 0.7354, Val F1-Score: 0.7569\nEpoch 46/100, Train Loss: 0.2004, Val Loss: 0.1901, Val Recall: 0.7353, Val F1-Score: 0.7562\nEpoch 47/100, Train Loss: 0.1990, Val Loss: 0.1899, Val Recall: 0.7353, Val F1-Score: 0.7562\nEpoch 48/100, Train Loss: 0.2013, Val Loss: 0.1897, Val Recall: 0.7366, Val F1-Score: 0.7575\nEpoch 49/100, Train Loss: 0.1994, Val Loss: 0.1895, Val Recall: 0.7366, Val F1-Score: 0.7572\nEpoch 50/100, Train Loss: 0.2004, Val Loss: 0.1894, Val Recall: 0.7366, Val F1-Score: 0.7572\nEpoch 51/100, Train Loss: 0.1972, Val Loss: 0.1893, Val Recall: 0.7399, Val F1-Score: 0.7611\nEpoch 52/100, Train Loss: 0.1989, Val Loss: 0.1892, Val Recall: 0.7401, Val F1-Score: 0.7616\nEpoch 53/100, Train Loss: 0.1994, Val Loss: 0.1891, Val Recall: 0.7401, Val F1-Score: 0.7616\nEpoch 54/100, Train Loss: 0.1981, Val Loss: 0.1891, Val Recall: 0.7419, Val F1-Score: 0.7640\nEpoch 55/100, Train Loss: 0.1969, Val Loss: 0.1891, Val Recall: 0.7386, Val F1-Score: 0.7601\nEpoch 56/100, Train Loss: 0.1980, Val Loss: 0.1891, Val Recall: 0.7421, Val F1-Score: 0.7645\nEpoch 57/100, Train Loss: 0.1979, Val Loss: 0.1890, Val Recall: 0.7406, Val F1-Score: 0.7631\nEpoch 58/100, Train Loss: 0.1983, Val Loss: 0.1890, Val Recall: 0.7421, Val F1-Score: 0.7648\nEpoch 59/100, Train Loss: 0.1971, Val Loss: 0.1890, Val Recall: 0.7406, Val F1-Score: 0.7634\nEpoch 60/100, Train Loss: 0.1940, Val Loss: 0.1889, Val Recall: 0.7373, Val F1-Score: 0.7594\nEpoch 61/100, Train Loss: 0.1953, Val Loss: 0.1888, Val Recall: 0.7373, Val F1-Score: 0.7597\nEarly stopping triggered\nFinal Validation Loss on Combined Tasks: 0.1888\nFinal Validation Recall on Combined Tasks: 0.7373\nFinal Validation F1-Score on Combined Tasks: 0.7597\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[133], line 46\u001b[0m\n\u001b[1;32m     43\u001b[0m test_graph_all \u001b[38;5;241m=\u001b[39m create_graph(X_test_all, y_test_all)\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Train and evaluate the model on the combined dataset\u001b[39;00m\n\u001b[0;32m---> 46\u001b[0m model_all, final_val_loss_all, final_val_accuracy_all \u001b[38;5;241m=\u001b[39m train_and_evaluate(\n\u001b[1;32m     47\u001b[0m     train_graph_all, test_graph_all, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCombined Tasks\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     48\u001b[0m )\n","\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 3)"],"ename":"ValueError","evalue":"too many values to unpack (expected 3)","output_type":"error"}]}]}