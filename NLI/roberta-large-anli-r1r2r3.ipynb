{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2933,"sourceType":"datasetVersion","datasetId":1670},{"sourceId":4548821,"sourceType":"datasetVersion","datasetId":2655798},{"sourceId":4550791,"sourceType":"datasetVersion","datasetId":2656775}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport torch\nfrom torch.utils.data import DataLoader, Dataset\nfrom transformers import AutoTokenizer\nfrom sklearn.metrics import accuracy_score\n\n\ntokenizer = AutoTokenizer.from_pretrained(\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-05T15:19:41.786546Z","iopub.execute_input":"2024-04-05T15:19:41.787292Z","iopub.status.idle":"2024-04-05T15:19:56.553282Z","shell.execute_reply.started":"2024-04-05T15:19:41.787258Z","shell.execute_reply":"2024-04-05T15:19:56.552206Z"},"trusted":true},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/49.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bbf6c6bf682445adbe3aefece62701a2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/703 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96238b1ec7404744a68bbd656f40a265"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce6cbc18f3a545aca8f75994bca6c9cb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"215ef128483e4d7da78a4fda3dbb859b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0b4d1a6f28b3463d81e3831c77ed2d4f"}},"metadata":{}}]},{"cell_type":"code","source":"# Function to tokenize the premise, hypothesis, and reason\ndef preprocess_data(df):\n    # Concatenate the reason with the hypothesis\n    concatenated_hypotheses = df['hypothesis'] + \" [SEP] \" + df['reason']\n    return tokenizer(df['premise'].tolist(), concatenated_hypotheses.tolist(), padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n\n# Custom Dataset class\nclass ANLIDataset(Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n\n    def __getitem__(self, idx):\n        item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}  # Updated line\n        item['labels'] = self.labels[idx]  # Assuming labels are simple lists or numpy arrays, no need for tensor conversion here\n        return item\n\n    def __len__(self):\n        return len(self.labels)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:19:56.555276Z","iopub.execute_input":"2024-04-05T15:19:56.555735Z","iopub.status.idle":"2024-04-05T15:19:56.563341Z","shell.execute_reply.started":"2024-04-05T15:19:56.555705Z","shell.execute_reply":"2024-04-05T15:19:56.562459Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# Load the test dataset\ndf = pd.read_csv('/kaggle/input/anli-a-large-scale-nli-benchmark-dataset/test_r1.csv')  # Make sure to update the path to your dataset location\ntokenized_data = preprocess_data(df)\ndataset = ANLIDataset(tokenized_data, df['label'].tolist())\n","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:19:56.564504Z","iopub.execute_input":"2024-04-05T15:19:56.564833Z","iopub.status.idle":"2024-04-05T15:19:56.992922Z","shell.execute_reply.started":"2024-04-05T15:19:56.564801Z","shell.execute_reply":"2024-04-05T15:19:56.992078Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"df.head()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:19:56.995029Z","iopub.execute_input":"2024-04-05T15:19:56.995397Z","iopub.status.idle":"2024-04-05T15:19:57.012019Z","shell.execute_reply.started":"2024-04-05T15:19:56.995368Z","shell.execute_reply":"2024-04-05T15:19:57.011055Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                                    uid  \\\n0  4aae63a8-fcf7-406c-a2f3-50c31c5934a9   \n1  c577b92c-78fb-4e1d-ae1d-34133609c142   \n2  26936cd9-1a5a-4a2b-9fca-899d61880ca0   \n3  cd977941-273b-4748-a5d2-6c7234a2a302   \n4  1a9eae8f-27d9-47ba-80b8-7d1402ee524a   \n\n                                             premise  \\\n0  Ernest Jones is a British jeweller and watchma...   \n1  Old Trafford is a football stadium in Old Traf...   \n2  Magnus is a Belgian joint dance project of Tom...   \n3  Shadowboxer is a 2005 crime thriller film dire...   \n4  Takaaki Kajita (梶田 隆章 , Kajita Takaaki ) is a ...   \n\n                                          hypothesis  label  \\\n0  The first Ernest Jones store was opened on the...      0   \n1  There are only 10 larger football stadiums in ...      0   \n2  \"The body gave you everything\" album was not r...      0   \n3  Shadowboxer was written and directed by Lee Da...      1   \n4  Arthur B. McDonald is a Japanese physicist, kn...      2   \n\n                                              reason  \n0  The first store was opened in London, which is...  \n1  The text says that it is the 11th largest foot...  \n2  it was released on March 29, 2004. \"not this b...  \n3  It is not know who wrote the Shadowboxer. The ...  \n4     Arthur B. McDonald is Canadian in the context.  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>uid</th>\n      <th>premise</th>\n      <th>hypothesis</th>\n      <th>label</th>\n      <th>reason</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>4aae63a8-fcf7-406c-a2f3-50c31c5934a9</td>\n      <td>Ernest Jones is a British jeweller and watchma...</td>\n      <td>The first Ernest Jones store was opened on the...</td>\n      <td>0</td>\n      <td>The first store was opened in London, which is...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>c577b92c-78fb-4e1d-ae1d-34133609c142</td>\n      <td>Old Trafford is a football stadium in Old Traf...</td>\n      <td>There are only 10 larger football stadiums in ...</td>\n      <td>0</td>\n      <td>The text says that it is the 11th largest foot...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>26936cd9-1a5a-4a2b-9fca-899d61880ca0</td>\n      <td>Magnus is a Belgian joint dance project of Tom...</td>\n      <td>\"The body gave you everything\" album was not r...</td>\n      <td>0</td>\n      <td>it was released on March 29, 2004. \"not this b...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>cd977941-273b-4748-a5d2-6c7234a2a302</td>\n      <td>Shadowboxer is a 2005 crime thriller film dire...</td>\n      <td>Shadowboxer was written and directed by Lee Da...</td>\n      <td>1</td>\n      <td>It is not know who wrote the Shadowboxer. The ...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1a9eae8f-27d9-47ba-80b8-7d1402ee524a</td>\n      <td>Takaaki Kajita (梶田 隆章 , Kajita Takaaki ) is a ...</td>\n      <td>Arthur B. McDonald is a Japanese physicist, kn...</td>\n      <td>2</td>\n      <td>Arthur B. McDonald is Canadian in the context.</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df.info()","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:19:57.013181Z","iopub.execute_input":"2024-04-05T15:19:57.013490Z","iopub.status.idle":"2024-04-05T15:19:57.036166Z","shell.execute_reply.started":"2024-04-05T15:19:57.013463Z","shell.execute_reply":"2024-04-05T15:19:57.035134Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 1000 entries, 0 to 999\nData columns (total 5 columns):\n #   Column      Non-Null Count  Dtype \n---  ------      --------------  ----- \n 0   uid         1000 non-null   object\n 1   premise     1000 non-null   object\n 2   hypothesis  1000 non-null   object\n 3   label       1000 non-null   int64 \n 4   reason      1000 non-null   object\ndtypes: int64(1), object(4)\nmemory usage: 39.2+ KB\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\n# DataLoader\nbatch_size = 32\ndata_loader = DataLoader(dataset, batch_size=batch_size, shuffle=False)\n\n# Model Initialization\nmodel = AutoModelForSequenceClassification.from_pretrained(\"ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli\")\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:19:57.037599Z","iopub.execute_input":"2024-04-05T15:19:57.037890Z","iopub.status.idle":"2024-04-05T15:20:55.540407Z","shell.execute_reply.started":"2024-04-05T15:19:57.037864Z","shell.execute_reply":"2024-04-05T15:20:55.539279Z"},"trusted":true},"execution_count":6,"outputs":[{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/1.43G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0593db7abb6f4dde83bfc3d7f6e8a997"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/_utils.py:831: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n  return self.fget.__get__(instance, owner)()\nSome weights of the model checkpoint at ynie/roberta-large-snli_mnli_fever_anli_R1_R2_R3-nli were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm.auto import tqdm\n\n# Evaluation function with progress bar\ndef evaluate(model, data_loader):\n    model.eval()\n    predictions, true_labels = [], []\n    progress_bar = tqdm(data_loader, desc='Evaluating', unit='batch', leave=False)\n    \n    with torch.no_grad():\n        for batch in progress_bar:\n            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n            labels = batch['labels'].to(device)\n            outputs = model(**inputs)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n            labels = labels.detach().cpu().numpy()\n            predictions.extend(preds)\n            true_labels.extend(labels)\n    \n    accuracy = accuracy_score(true_labels, predictions)\n    return accuracy\n\n# Run evaluation\naccuracy = evaluate(model, data_loader)\nprint(f'Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:20:55.541582Z","iopub.execute_input":"2024-04-05T15:20:55.542104Z","iopub.status.idle":"2024-04-05T15:21:21.620750Z","shell.execute_reply.started":"2024-04-05T15:20:55.542063Z","shell.execute_reply":"2024-04-05T15:21:21.619773Z"},"trusted":true},"execution_count":7,"outputs":[{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/32 [00:00<?, ?batch/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Accuracy: 0.702\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load Round 2 dataset\ndf_r2 = pd.read_csv('/kaggle/input/anli-a-large-scale-nli-benchmark-dataset/test_r2.csv')  # Make sure to adjust the path accordingly\ntokenized_data_r2 = preprocess_data(df_r2)\ndataset_r2 = ANLIDataset(tokenized_data_r2, df_r2['label'].tolist())\n\n# DataLoader for Round 2\ndata_loader_r2 = DataLoader(dataset_r2, batch_size=16, shuffle=False)\n\nmodel.eval()  # Set the model to evaluation mode\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:22:56.068691Z","iopub.execute_input":"2024-04-05T15:22:56.069063Z","iopub.status.idle":"2024-04-05T15:22:56.459401Z","shell.execute_reply.started":"2024-04-05T15:22:56.069033Z","shell.execute_reply":"2024-04-05T15:22:56.458273Z"},"trusted":true},"execution_count":10,"outputs":[{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluation function\ndef evaluate(model, data_loader):\n    model.eval()\n    predictions, true_labels = [], []\n    progress_bar = tqdm(data_loader, desc='Evaluating')\n    \n    with torch.no_grad():\n        for batch in progress_bar:\n            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n            labels = batch['labels'].to(device)\n            outputs = model(**inputs)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n            labels = labels.detach().cpu().numpy()\n            predictions.extend(preds)\n            true_labels.extend(labels)\n    \n    accuracy = accuracy_score(true_labels, predictions)\n    return accuracy\n\n# Run evaluation for Round 2\naccuracy_r2 = evaluate(model, data_loader_r2)\nprint(f'Round 2 Accuracy: {accuracy_r2}')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:22:59.984028Z","iopub.execute_input":"2024-04-05T15:22:59.985143Z","iopub.status.idle":"2024-04-05T15:23:24.582822Z","shell.execute_reply.started":"2024-04-05T15:22:59.985077Z","shell.execute_reply":"2024-04-05T15:23:24.581779Z"},"trusted":true},"execution_count":11,"outputs":[{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/63 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d50d7bfd83d4613b3487168ac60e054"}},"metadata":{}},{"name":"stdout","text":"Round 2 Accuracy: 0.59\n","output_type":"stream"}]},{"cell_type":"code","source":"# Load Round 3 dataset\ndf_r3 = pd.read_csv('/kaggle/input/anli-a-large-scale-nli-benchmark-dataset/test_r3.csv')  # Ensure this path points to your Round 3 dataset\ntokenized_data_r3 = preprocess_data(df_r3)\ndataset_r3 = ANLIDataset(tokenized_data_r3, df_r3['label'].tolist())\n\n# DataLoader for Round 3\ndata_loader_r3 = DataLoader(dataset_r3, batch_size=16, shuffle=False)\n\n# Model and device setup\nmodel.eval()  # Ensure the model is in evaluation mode\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:24:17.380950Z","iopub.execute_input":"2024-04-05T15:24:17.381652Z","iopub.status.idle":"2024-04-05T15:24:18.035985Z","shell.execute_reply.started":"2024-04-05T15:24:17.381619Z","shell.execute_reply":"2024-04-05T15:24:18.035066Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"RobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 1024)\n      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0-23): 24 x RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=1024, out_features=1024, bias=True)\n              (key): Linear(in_features=1024, out_features=1024, bias=True)\n              (value): Linear(in_features=1024, out_features=1024, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=1024, out_features=3, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"code","source":"# Evaluation function\ndef evaluate(model, data_loader):\n    model.eval()\n    predictions, true_labels = [], []\n    progress_bar = tqdm(data_loader, desc='Evaluating')\n    \n    with torch.no_grad():\n        for batch in progress_bar:\n            inputs = {k: v.to(device) for k, v in batch.items() if k != 'labels'}\n            labels = batch['labels'].to(device)\n            outputs = model(**inputs)\n            logits = outputs.logits\n            preds = torch.argmax(logits, dim=-1).detach().cpu().numpy()\n            labels = labels.detach().cpu().numpy()\n            predictions.extend(preds)\n            true_labels.extend(labels)\n    \n    accuracy = accuracy_score(true_labels, predictions)\n    return accuracy\n\n# Run evaluation for Round 3\naccuracy_r3 = evaluate(model, data_loader_r3)\nprint(f'Round 3 Accuracy: {accuracy_r3}')","metadata":{"execution":{"iopub.status.busy":"2024-04-05T15:24:20.752747Z","iopub.execute_input":"2024-04-05T15:24:20.753648Z","iopub.status.idle":"2024-04-05T15:25:14.176381Z","shell.execute_reply.started":"2024-04-05T15:24:20.753606Z","shell.execute_reply":"2024-04-05T15:25:14.175408Z"},"trusted":true},"execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"Evaluating:   0%|          | 0/75 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dd4deb80f05d42d8bc486e895d77679b"}},"metadata":{}},{"name":"stdout","text":"Round 3 Accuracy: 0.5775\n","output_type":"stream"}]}]}