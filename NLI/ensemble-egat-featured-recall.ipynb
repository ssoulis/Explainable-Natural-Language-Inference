{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2933,"sourceType":"datasetVersion","datasetId":1670},{"sourceId":4548821,"sourceType":"datasetVersion","datasetId":2655798},{"sourceId":4550791,"sourceType":"datasetVersion","datasetId":2656775},{"sourceId":8083662,"sourceType":"datasetVersion","datasetId":4771616},{"sourceId":8083668,"sourceType":"datasetVersion","datasetId":4771621},{"sourceId":8083678,"sourceType":"datasetVersion","datasetId":4771629},{"sourceId":8084913,"sourceType":"datasetVersion","datasetId":4772442}],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\n\n# Load the SNLI test data (including true labels)\nsnli_test_path = \"/kaggle/input/stanford-natural-language-inference-corpus/snli_1.0_test.csv\"\nsnli_test_df = pd.read_csv(snli_test_path)\n\n# Define file paths for SNLI prediction files\nsnli_predictions_paths = {\n    \"deberta\": \"/kaggle/input/deberta-nli/deberta_snli_predictions.csv\",\n    \"roberta\": \"/kaggle/input/roberta/roberta_snli_predictions.csv\",\n    \"albert\": \"/kaggle/input/albert/albert_snli_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_snli = \"/kaggle/working/combined_snli_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_snli_df = pd.DataFrame(columns=columns)\n\nlabel_mapping = {'entailment': 0, 'neutral': 1, 'contradiction': 2}\n\n# Load and merge the predictions\nfor model, path in snli_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_snli_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_snli_df['True_Label'] = snli_test_df['gold_label'].map(label_mapping)\n\n# Convert True_Label to integer type\ncombined_snli_df['True_Label'] = combined_snli_df['True_Label'].astype('Int64')\n\n# Save the combined DataFrame to CSV\ncombined_snli_df.to_csv(output_csv_path_snli, index=False)\n\nprint(f\"Combined SNLI predictions with true labels saved to {output_csv_path_snli}\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-20T12:51:38.162534Z","iopub.execute_input":"2024-08-20T12:51:38.163370Z","iopub.status.idle":"2024-08-20T12:51:38.455774Z","shell.execute_reply.started":"2024-08-20T12:51:38.163335Z","shell.execute_reply":"2024-08-20T12:51:38.454492Z"},"trusted":true},"execution_count":312,"outputs":[{"name":"stdout","text":"Combined SNLI predictions with true labels saved to /kaggle/working/combined_snli_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_snli_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:38.458083Z","iopub.execute_input":"2024-08-20T12:51:38.458473Z","iopub.status.idle":"2024-08-20T12:51:38.477321Z","shell.execute_reply.started":"2024-08-20T12:51:38.458443Z","shell.execute_reply":"2024-08-20T12:51:38.476352Z"},"trusted":true},"execution_count":313,"outputs":[{"execution_count":313,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.034767         0.962592               0.002641   \n1            0.001921         0.319032               0.679047   \n2            0.998783         0.000764               0.000453   \n3            0.001001         0.997708               0.001291   \n4            0.001080         0.301363               0.697557   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.012451         0.927093               0.060457   \n1            0.752766         0.242251               0.004983   \n2            0.000254         0.004494               0.995253   \n3            0.005844         0.990736               0.003419   \n4            0.278348         0.718575               0.003076   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.008653        0.947434              0.043913           1  \n1           0.740332        0.256434              0.003235           0  \n2           0.004677        0.060481              0.934843           2  \n3           0.034056        0.956687              0.009257           1  \n4           0.498761        0.499270              0.001969           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.034767</td>\n      <td>0.962592</td>\n      <td>0.002641</td>\n      <td>0.012451</td>\n      <td>0.927093</td>\n      <td>0.060457</td>\n      <td>0.008653</td>\n      <td>0.947434</td>\n      <td>0.043913</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001921</td>\n      <td>0.319032</td>\n      <td>0.679047</td>\n      <td>0.752766</td>\n      <td>0.242251</td>\n      <td>0.004983</td>\n      <td>0.740332</td>\n      <td>0.256434</td>\n      <td>0.003235</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.998783</td>\n      <td>0.000764</td>\n      <td>0.000453</td>\n      <td>0.000254</td>\n      <td>0.004494</td>\n      <td>0.995253</td>\n      <td>0.004677</td>\n      <td>0.060481</td>\n      <td>0.934843</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001001</td>\n      <td>0.997708</td>\n      <td>0.001291</td>\n      <td>0.005844</td>\n      <td>0.990736</td>\n      <td>0.003419</td>\n      <td>0.034056</td>\n      <td>0.956687</td>\n      <td>0.009257</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.001080</td>\n      <td>0.301363</td>\n      <td>0.697557</td>\n      <td>0.278348</td>\n      <td>0.718575</td>\n      <td>0.003076</td>\n      <td>0.498761</td>\n      <td>0.499270</td>\n      <td>0.001969</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the ANLI Round 1 test data (including true labels)\nmnli_matched_test_path = \"/kaggle/input/nli-dataset-for-sentence-understanding/mnli_validation_matched.csv\"\nmnli_matched_test_df = pd.read_csv(mnli_matched_test_path)\n\n# Define file paths for ANLI Round 1 prediction files\nmnli_matched_predictions_paths = {\n    \"deberta\": \"/kaggle/input/validation/deberta_mnli_matched_val_predictions.csv\",\n    \"roberta\": \"/kaggle/input/validation/roberta_mnli_matched_val_predictions.csv\",\n    \"albert\": \"/kaggle/input/validation/albert_mnli_matched_val_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_mnli_matched = \"/kaggle/working/combined_mnli_matched_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_mnli_matched_df = pd.DataFrame(columns=columns)\n\n# Load and merge the predictions\nfor model, path in mnli_matched_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_mnli_matched_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_mnli_matched_df['True_Label'] = mnli_matched_test_df['label']\n\n# Save the combined DataFrame to CSV\ncombined_mnli_matched_df.to_csv(output_csv_path_mnli_matched, index=False)\n\nprint(f\"Combined MNLI-matched predictions with true labels saved to {output_csv_path_mnli_matched}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:38.478747Z","iopub.execute_input":"2024-08-20T12:51:38.479090Z","iopub.status.idle":"2024-08-20T12:51:38.693896Z","shell.execute_reply.started":"2024-08-20T12:51:38.479021Z","shell.execute_reply":"2024-08-20T12:51:38.692941Z"},"trusted":true},"execution_count":314,"outputs":[{"name":"stdout","text":"Combined MNLI-matched predictions with true labels saved to /kaggle/working/combined_mnli_matched_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_mnli_matched_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:38.695205Z","iopub.execute_input":"2024-08-20T12:51:38.695551Z","iopub.status.idle":"2024-08-20T12:51:38.710381Z","shell.execute_reply.started":"2024-08-20T12:51:38.695523Z","shell.execute_reply":"2024-08-20T12:51:38.709445Z"},"trusted":true},"execution_count":315,"outputs":[{"execution_count":315,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.005164         0.993364               0.001472   \n1            0.999153         0.000526               0.000321   \n2            0.000989         0.044792               0.954219   \n3            0.994965         0.004808               0.000228   \n4            0.999657         0.000220               0.000123   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.017844         0.950246               0.031909   \n1            0.001413         0.002030               0.996557   \n2            0.954781         0.042249               0.002970   \n3            0.000343         0.003511               0.996146   \n4            0.000079         0.000496               0.999425   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.010844        0.983012              0.006144           1  \n1           0.005388        0.007536              0.987076           2  \n2           0.853862        0.143483              0.002655           0  \n3           0.004128        0.070757              0.925115           2  \n4           0.003864        0.029262              0.966875           2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.005164</td>\n      <td>0.993364</td>\n      <td>0.001472</td>\n      <td>0.017844</td>\n      <td>0.950246</td>\n      <td>0.031909</td>\n      <td>0.010844</td>\n      <td>0.983012</td>\n      <td>0.006144</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.999153</td>\n      <td>0.000526</td>\n      <td>0.000321</td>\n      <td>0.001413</td>\n      <td>0.002030</td>\n      <td>0.996557</td>\n      <td>0.005388</td>\n      <td>0.007536</td>\n      <td>0.987076</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000989</td>\n      <td>0.044792</td>\n      <td>0.954219</td>\n      <td>0.954781</td>\n      <td>0.042249</td>\n      <td>0.002970</td>\n      <td>0.853862</td>\n      <td>0.143483</td>\n      <td>0.002655</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.994965</td>\n      <td>0.004808</td>\n      <td>0.000228</td>\n      <td>0.000343</td>\n      <td>0.003511</td>\n      <td>0.996146</td>\n      <td>0.004128</td>\n      <td>0.070757</td>\n      <td>0.925115</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.999657</td>\n      <td>0.000220</td>\n      <td>0.000123</td>\n      <td>0.000079</td>\n      <td>0.000496</td>\n      <td>0.999425</td>\n      <td>0.003864</td>\n      <td>0.029262</td>\n      <td>0.966875</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the ANLI Round 1 test data (including true labels)\nmnli_mismatched_test_path = \"/kaggle/input/nli-dataset-for-sentence-understanding/mnli_validation_mismatched.csv\"\nmnli_mismatched_test_df = pd.read_csv(mnli_mismatched_test_path)\n\n# Define file paths for ANLI Round 1 prediction files\nmnli_mismatched_predictions_paths = {\n    \"deberta\": \"/kaggle/input/validation/deberta_mnli_mismatched_val_predictions.csv\",\n    \"roberta\": \"/kaggle/input/validation/roberta_mnli_mismatched_val_predictions.csv\",\n    \"albert\": \"/kaggle/input/validation/albert_mnli_mismatched_val_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_mnli_mismatched = \"/kaggle/working/combined_mnli_mismatched_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_mnli_mismatched_df = pd.DataFrame(columns=columns)\n\n# Load and merge the predictions\nfor model, path in mnli_mismatched_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_mnli_mismatched_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_mnli_mismatched_df['True_Label'] = mnli_mismatched_test_df['label']\n\n# Save the combined DataFrame to CSV\ncombined_mnli_mismatched_df.to_csv(output_csv_path_mnli_mismatched, index=False)\n\nprint(f\"Combined MNLI-mismatched predictions with true labels saved to {output_csv_path_mnli_mismatched}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:38.713976Z","iopub.execute_input":"2024-08-20T12:51:38.714462Z","iopub.status.idle":"2024-08-20T12:51:38.901626Z","shell.execute_reply.started":"2024-08-20T12:51:38.714432Z","shell.execute_reply":"2024-08-20T12:51:38.900670Z"},"trusted":true},"execution_count":316,"outputs":[{"name":"stdout","text":"Combined MNLI-mismatched predictions with true labels saved to /kaggle/working/combined_mnli_mismatched_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_mnli_mismatched_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:38.902823Z","iopub.execute_input":"2024-08-20T12:51:38.903199Z","iopub.status.idle":"2024-08-20T12:51:38.919104Z","shell.execute_reply.started":"2024-08-20T12:51:38.903170Z","shell.execute_reply":"2024-08-20T12:51:38.918093Z"},"trusted":true},"execution_count":317,"outputs":[{"execution_count":317,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.999667         0.000160               0.000173   \n1            0.998119         0.000962               0.000919   \n2            0.000552         0.004809               0.994639   \n3            0.827653         0.171961               0.000386   \n4            0.000292         0.002875               0.996833   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.000068         0.000402               0.999529   \n1            0.000183         0.001511               0.998306   \n2            0.986062         0.012020               0.001918   \n3            0.000478         0.270953               0.728569   \n4            0.975167         0.021904               0.002929   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.000894        0.003787              0.995318           2  \n1           0.006421        0.010224              0.983355           2  \n2           0.975041        0.023354              0.001605           0  \n3           0.001722        0.796122              0.202156           2  \n4           0.965952        0.032748              0.001300           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.999667</td>\n      <td>0.000160</td>\n      <td>0.000173</td>\n      <td>0.000068</td>\n      <td>0.000402</td>\n      <td>0.999529</td>\n      <td>0.000894</td>\n      <td>0.003787</td>\n      <td>0.995318</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.998119</td>\n      <td>0.000962</td>\n      <td>0.000919</td>\n      <td>0.000183</td>\n      <td>0.001511</td>\n      <td>0.998306</td>\n      <td>0.006421</td>\n      <td>0.010224</td>\n      <td>0.983355</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000552</td>\n      <td>0.004809</td>\n      <td>0.994639</td>\n      <td>0.986062</td>\n      <td>0.012020</td>\n      <td>0.001918</td>\n      <td>0.975041</td>\n      <td>0.023354</td>\n      <td>0.001605</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.827653</td>\n      <td>0.171961</td>\n      <td>0.000386</td>\n      <td>0.000478</td>\n      <td>0.270953</td>\n      <td>0.728569</td>\n      <td>0.001722</td>\n      <td>0.796122</td>\n      <td>0.202156</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000292</td>\n      <td>0.002875</td>\n      <td>0.996833</td>\n      <td>0.975167</td>\n      <td>0.021904</td>\n      <td>0.002929</td>\n      <td>0.965952</td>\n      <td>0.032748</td>\n      <td>0.001300</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the ANLI Round 1 test data (including true labels)\nanli_r1_test_path = \"/kaggle/input/anli-a-large-scale-nli-benchmark-dataset/test_r1.csv\"\nanli_r1_test_df = pd.read_csv(anli_r1_test_path)\n\n# Define file paths for ANLI Round 1 prediction files\nanli_r1_predictions_paths = {\n    \"deberta\": \"/kaggle/input/deberta-nli/deberta_anli_r1_predictions.csv\",\n    \"roberta\": \"/kaggle/input/roberta/roberta_anli_r1_predictions.csv\",\n    \"albert\": \"/kaggle/input/albert/albert_anli_r1_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_anli_r1 = \"/kaggle/working/combined_anli_r1_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_anli_r1_df = pd.DataFrame(columns=columns)\n\n# Load and merge the predictions\nfor model, path in anli_r1_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_anli_r1_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_anli_r1_df['True_Label'] = anli_r1_test_df['label']\n\n# Save the combined DataFrame to CSV\ncombined_anli_r1_df.to_csv(output_csv_path_anli_r1, index=False)\n\nprint(f\"Combined ANLI Round 1 predictions with true labels saved to {output_csv_path_anli_r1}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:38.920386Z","iopub.execute_input":"2024-08-20T12:51:38.921102Z","iopub.status.idle":"2024-08-20T12:51:38.970989Z","shell.execute_reply.started":"2024-08-20T12:51:38.921062Z","shell.execute_reply":"2024-08-20T12:51:38.970005Z"},"trusted":true},"execution_count":318,"outputs":[{"name":"stdout","text":"Combined ANLI Round 1 predictions with true labels saved to /kaggle/working/combined_anli_r1_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_anli_r1_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:38.972242Z","iopub.execute_input":"2024-08-20T12:51:38.972548Z","iopub.status.idle":"2024-08-20T12:51:38.987538Z","shell.execute_reply.started":"2024-08-20T12:51:38.972522Z","shell.execute_reply":"2024-08-20T12:51:38.986379Z"},"trusted":true},"execution_count":319,"outputs":[{"execution_count":319,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.015388         0.976305               0.008307   \n1            0.224603         0.501549               0.273848   \n2            0.006642         0.976690               0.016669   \n3            0.966494         0.032235               0.001272   \n4            0.880736         0.028293               0.090971   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.996714         0.000376               0.002910   \n1            0.875720         0.000724               0.123556   \n2            0.999484         0.000330               0.000186   \n3            0.000686         0.998181               0.001133   \n4            0.000378         0.000197               0.999425   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.322974        0.667628              0.009398           0  \n1           0.998526        0.000604              0.000869           0  \n2           0.783352        0.212241              0.004407           0  \n3           0.002134        0.989523              0.008343           1  \n4           0.023283        0.013253              0.963464           2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.015388</td>\n      <td>0.976305</td>\n      <td>0.008307</td>\n      <td>0.996714</td>\n      <td>0.000376</td>\n      <td>0.002910</td>\n      <td>0.322974</td>\n      <td>0.667628</td>\n      <td>0.009398</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.224603</td>\n      <td>0.501549</td>\n      <td>0.273848</td>\n      <td>0.875720</td>\n      <td>0.000724</td>\n      <td>0.123556</td>\n      <td>0.998526</td>\n      <td>0.000604</td>\n      <td>0.000869</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.006642</td>\n      <td>0.976690</td>\n      <td>0.016669</td>\n      <td>0.999484</td>\n      <td>0.000330</td>\n      <td>0.000186</td>\n      <td>0.783352</td>\n      <td>0.212241</td>\n      <td>0.004407</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.966494</td>\n      <td>0.032235</td>\n      <td>0.001272</td>\n      <td>0.000686</td>\n      <td>0.998181</td>\n      <td>0.001133</td>\n      <td>0.002134</td>\n      <td>0.989523</td>\n      <td>0.008343</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.880736</td>\n      <td>0.028293</td>\n      <td>0.090971</td>\n      <td>0.000378</td>\n      <td>0.000197</td>\n      <td>0.999425</td>\n      <td>0.023283</td>\n      <td>0.013253</td>\n      <td>0.963464</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the ANLI Round 2 test data (including true labels)\nanli_r2_test_path = \"/kaggle/input/anli-a-large-scale-nli-benchmark-dataset/test_r2.csv\"\nanli_r2_test_df = pd.read_csv(anli_r2_test_path)\n\n# Define file paths for ANLI Round 2 prediction files\nanli_r2_predictions_paths = {\n    \"deberta\": \"/kaggle/input/deberta-nli/deberta_anli_r2_predictions.csv\",\n    \"roberta\": \"/kaggle/input/roberta/roberta_anli_r2_predictions.csv\",\n    \"albert\": \"/kaggle/input/albert/albert_anli_r2_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_anli_r2 = \"/kaggle/working/combined_anli_r2_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_anli_r2_df = pd.DataFrame(columns=columns)\n\n# Load and merge the predictions\nfor model, path in anli_r2_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_anli_r2_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_anli_r2_df['True_Label'] = anli_r2_test_df['label']\n\n# Save the combined DataFrame to CSV\ncombined_anli_r2_df.to_csv(output_csv_path_anli_r2, index=False)\n\nprint(f\"Combined ANLI Round 2 predictions with true labels saved to {output_csv_path_anli_r2}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:38.989171Z","iopub.execute_input":"2024-08-20T12:51:38.989447Z","iopub.status.idle":"2024-08-20T12:51:39.036889Z","shell.execute_reply.started":"2024-08-20T12:51:38.989421Z","shell.execute_reply":"2024-08-20T12:51:39.035928Z"},"trusted":true},"execution_count":320,"outputs":[{"name":"stdout","text":"Combined ANLI Round 2 predictions with true labels saved to /kaggle/working/combined_anli_r2_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_anli_r2_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.038182Z","iopub.execute_input":"2024-08-20T12:51:39.038549Z","iopub.status.idle":"2024-08-20T12:51:39.053413Z","shell.execute_reply.started":"2024-08-20T12:51:39.038519Z","shell.execute_reply":"2024-08-20T12:51:39.052473Z"},"trusted":true},"execution_count":321,"outputs":[{"execution_count":321,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.001309         0.029617               0.969075   \n1            0.724144         0.273676               0.002180   \n2            0.071604         0.917894               0.010503   \n3            0.066162         0.929179               0.004659   \n4            0.906199         0.089873               0.003928   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.999506         0.000264               0.000230   \n1            0.026951         0.054230               0.918819   \n2            0.001282         0.998108               0.000610   \n3            0.007091         0.992694               0.000215   \n4            0.006259         0.989432               0.004309   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.863365        0.133388              0.003246           0  \n1           0.072900        0.904344              0.022756           1  \n2           0.027402        0.972218              0.000380           0  \n3           0.632171        0.365194              0.002635           1  \n4           0.064109        0.234642              0.701249           2  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.001309</td>\n      <td>0.029617</td>\n      <td>0.969075</td>\n      <td>0.999506</td>\n      <td>0.000264</td>\n      <td>0.000230</td>\n      <td>0.863365</td>\n      <td>0.133388</td>\n      <td>0.003246</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.724144</td>\n      <td>0.273676</td>\n      <td>0.002180</td>\n      <td>0.026951</td>\n      <td>0.054230</td>\n      <td>0.918819</td>\n      <td>0.072900</td>\n      <td>0.904344</td>\n      <td>0.022756</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.071604</td>\n      <td>0.917894</td>\n      <td>0.010503</td>\n      <td>0.001282</td>\n      <td>0.998108</td>\n      <td>0.000610</td>\n      <td>0.027402</td>\n      <td>0.972218</td>\n      <td>0.000380</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.066162</td>\n      <td>0.929179</td>\n      <td>0.004659</td>\n      <td>0.007091</td>\n      <td>0.992694</td>\n      <td>0.000215</td>\n      <td>0.632171</td>\n      <td>0.365194</td>\n      <td>0.002635</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.906199</td>\n      <td>0.089873</td>\n      <td>0.003928</td>\n      <td>0.006259</td>\n      <td>0.989432</td>\n      <td>0.004309</td>\n      <td>0.064109</td>\n      <td>0.234642</td>\n      <td>0.701249</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Load the ANLI Round 3 test data (including true labels)\nanli_r3_test_path = \"/kaggle/input/anli-a-large-scale-nli-benchmark-dataset/test_r3.csv\"\nanli_r3_test_df = pd.read_csv(anli_r3_test_path)\n\n# Define file paths for ANLI Round 2 prediction files\nanli_r3_predictions_paths = {\n    \"deberta\": \"/kaggle/input/deberta-nli/deberta_anli_r3_predictions.csv\",\n    \"roberta\": \"/kaggle/input/roberta/roberta_anli_r3_predictions.csv\",\n    \"albert\": \"/kaggle/input/albert/albert_anli_r3_predictions.csv\",\n}\n\n# Specify where you want to save the combined predictions CSV file\noutput_csv_path_anli_r3 = \"/kaggle/working/combined_anli_r3_df\"\n\n# Define the column names, placing True_Label at the end\ncolumns = [\n    'Deberta_Entailment', 'Deberta_Neutral', 'Deberta_Contradiction',\n    'Roberta_Entailment', 'Roberta_Neutral', 'Roberta_Contradiction',\n    'Albert_Entailment', 'Albert_Neutral', 'Albert_Contradiction',\n    'True_Label'  # Ensuring True_Label is the last column\n]\n\n# Initialize the DataFrame with specified columns\ncombined_anli_r3_df = pd.DataFrame(columns=columns)\n\n# Load and merge the predictions\nfor model, path in anli_r3_predictions_paths.items():\n    predictions_df = pd.read_csv(path)\n    for label in ['Entailment', 'Neutral', 'Contradiction']:\n        combined_anli_r3_df[f\"{model.capitalize()}_{label}\"] = predictions_df[label]\n\n# Assign the true labels to the True_Label column, now positioned at the end\ncombined_anli_r3_df['True_Label'] = anli_r3_test_df['label']\n\n# Save the combined DataFrame to CSV\ncombined_anli_r3_df.to_csv(output_csv_path_anli_r3, index=False)\n\nprint(f\"Combined ANLI Round 3 predictions with true labels saved to {output_csv_path_anli_r3}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.054762Z","iopub.execute_input":"2024-08-20T12:51:39.055088Z","iopub.status.idle":"2024-08-20T12:51:39.107437Z","shell.execute_reply.started":"2024-08-20T12:51:39.055055Z","shell.execute_reply":"2024-08-20T12:51:39.106463Z"},"trusted":true},"execution_count":322,"outputs":[{"name":"stdout","text":"Combined ANLI Round 3 predictions with true labels saved to /kaggle/working/combined_anli_r3_df\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_anli_r3_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.108618Z","iopub.execute_input":"2024-08-20T12:51:39.108905Z","iopub.status.idle":"2024-08-20T12:51:39.123213Z","shell.execute_reply.started":"2024-08-20T12:51:39.108879Z","shell.execute_reply":"2024-08-20T12:51:39.122281Z"},"trusted":true},"execution_count":323,"outputs":[{"execution_count":323,"output_type":"execute_result","data":{"text/plain":"   Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0            0.005921         0.960529               0.033551   \n1            0.009586         0.934714               0.055700   \n2            0.003428         0.976393               0.020179   \n3            0.004633         0.023985               0.971382   \n4            0.017428         0.633695               0.348877   \n\n   Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0            0.022959         0.976533               0.000509   \n1            0.999611         0.000205               0.000185   \n2            0.002020         0.997897               0.000083   \n3            0.974441         0.024459               0.001100   \n4            0.984416         0.011166               0.004419   \n\n   Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0           0.001848        0.998084              0.000067           0  \n1           0.951772        0.048075              0.000153           0  \n2           0.001014        0.998984              0.000002           0  \n3           0.996749        0.000989              0.002262           0  \n4           0.000518        0.128416              0.871066           0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.005921</td>\n      <td>0.960529</td>\n      <td>0.033551</td>\n      <td>0.022959</td>\n      <td>0.976533</td>\n      <td>0.000509</td>\n      <td>0.001848</td>\n      <td>0.998084</td>\n      <td>0.000067</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.009586</td>\n      <td>0.934714</td>\n      <td>0.055700</td>\n      <td>0.999611</td>\n      <td>0.000205</td>\n      <td>0.000185</td>\n      <td>0.951772</td>\n      <td>0.048075</td>\n      <td>0.000153</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.003428</td>\n      <td>0.976393</td>\n      <td>0.020179</td>\n      <td>0.002020</td>\n      <td>0.997897</td>\n      <td>0.000083</td>\n      <td>0.001014</td>\n      <td>0.998984</td>\n      <td>0.000002</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004633</td>\n      <td>0.023985</td>\n      <td>0.971382</td>\n      <td>0.974441</td>\n      <td>0.024459</td>\n      <td>0.001100</td>\n      <td>0.996749</td>\n      <td>0.000989</td>\n      <td>0.002262</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.017428</td>\n      <td>0.633695</td>\n      <td>0.348877</td>\n      <td>0.984416</td>\n      <td>0.011166</td>\n      <td>0.004419</td>\n      <td>0.000518</td>\n      <td>0.128416</td>\n      <td>0.871066</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"# Check for missing values\nmissing_values_anli1 = combined_anli_r1_df.isnull().sum()\n\nmissing_values_anli2 = combined_anli_r2_df.isnull().sum()\n\nmissing_values_anli3 = combined_anli_r3_df.isnull().sum()\n\nmissing_values_snli = combined_snli_df.isnull().sum()\n\nmissing_values_mnli_matched = combined_mnli_matched_df.isnull().sum()\n\nmissing_values_mnli_mismatched = combined_mnli_mismatched_df.isnull().sum()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.125019Z","iopub.execute_input":"2024-08-20T12:51:39.125466Z","iopub.status.idle":"2024-08-20T12:51:39.137404Z","shell.execute_reply.started":"2024-08-20T12:51:39.125428Z","shell.execute_reply":"2024-08-20T12:51:39.136522Z"},"trusted":true},"execution_count":324,"outputs":[]},{"cell_type":"code","source":"missing_values_anli1","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.142469Z","iopub.execute_input":"2024-08-20T12:51:39.142764Z","iopub.status.idle":"2024-08-20T12:51:39.149946Z","shell.execute_reply.started":"2024-08-20T12:51:39.142738Z","shell.execute_reply":"2024-08-20T12:51:39.149061Z"},"trusted":true},"execution_count":325,"outputs":[{"execution_count":325,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"missing_values_anli2","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.151162Z","iopub.execute_input":"2024-08-20T12:51:39.151448Z","iopub.status.idle":"2024-08-20T12:51:39.160641Z","shell.execute_reply.started":"2024-08-20T12:51:39.151422Z","shell.execute_reply":"2024-08-20T12:51:39.159818Z"},"trusted":true},"execution_count":326,"outputs":[{"execution_count":326,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"missing_values_anli3","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.161746Z","iopub.execute_input":"2024-08-20T12:51:39.162046Z","iopub.status.idle":"2024-08-20T12:51:39.172360Z","shell.execute_reply.started":"2024-08-20T12:51:39.161999Z","shell.execute_reply":"2024-08-20T12:51:39.171354Z"},"trusted":true},"execution_count":327,"outputs":[{"execution_count":327,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"missing_values_snli","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.173563Z","iopub.execute_input":"2024-08-20T12:51:39.173850Z","iopub.status.idle":"2024-08-20T12:51:39.182998Z","shell.execute_reply.started":"2024-08-20T12:51:39.173825Z","shell.execute_reply":"2024-08-20T12:51:39.182102Z"},"trusted":true},"execution_count":328,"outputs":[{"execution_count":328,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment         0\nDeberta_Neutral            0\nDeberta_Contradiction      0\nRoberta_Entailment         0\nRoberta_Neutral            0\nRoberta_Contradiction      0\nAlbert_Entailment          0\nAlbert_Neutral             0\nAlbert_Contradiction       0\nTrue_Label               176\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"missing_values_mnli_matched","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.184113Z","iopub.execute_input":"2024-08-20T12:51:39.184410Z","iopub.status.idle":"2024-08-20T12:51:39.195989Z","shell.execute_reply.started":"2024-08-20T12:51:39.184385Z","shell.execute_reply":"2024-08-20T12:51:39.195058Z"},"trusted":true},"execution_count":329,"outputs":[{"execution_count":329,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"missing_values_mnli_mismatched","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.197388Z","iopub.execute_input":"2024-08-20T12:51:39.197709Z","iopub.status.idle":"2024-08-20T12:51:39.207511Z","shell.execute_reply.started":"2024-08-20T12:51:39.197670Z","shell.execute_reply":"2024-08-20T12:51:39.206531Z"},"trusted":true},"execution_count":330,"outputs":[{"execution_count":330,"output_type":"execute_result","data":{"text/plain":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"combined_snli_df.dropna(subset=['True_Label'], inplace=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.208613Z","iopub.execute_input":"2024-08-20T12:51:39.208969Z","iopub.status.idle":"2024-08-20T12:51:39.217757Z","shell.execute_reply.started":"2024-08-20T12:51:39.208944Z","shell.execute_reply":"2024-08-20T12:51:39.216916Z"},"trusted":true},"execution_count":331,"outputs":[]},{"cell_type":"code","source":"# Verify missing values again after removal\nmissing_values_snli_after_removal = combined_snli_df.isnull().sum()\nprint(missing_values_snli_after_removal)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.218793Z","iopub.execute_input":"2024-08-20T12:51:39.219080Z","iopub.status.idle":"2024-08-20T12:51:39.227799Z","shell.execute_reply.started":"2024-08-20T12:51:39.219048Z","shell.execute_reply":"2024-08-20T12:51:39.226850Z"},"trusted":true},"execution_count":332,"outputs":[{"name":"stdout","text":"Deberta_Entailment       0\nDeberta_Neutral          0\nDeberta_Contradiction    0\nRoberta_Entailment       0\nRoberta_Neutral          0\nRoberta_Contradiction    0\nAlbert_Entailment        0\nAlbert_Neutral           0\nAlbert_Contradiction     0\nTrue_Label               0\ndtype: int64\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_snli_df.info()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.229117Z","iopub.execute_input":"2024-08-20T12:51:39.229512Z","iopub.status.idle":"2024-08-20T12:51:39.242615Z","shell.execute_reply.started":"2024-08-20T12:51:39.229458Z","shell.execute_reply":"2024-08-20T12:51:39.241621Z"},"trusted":true},"execution_count":333,"outputs":[{"name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nIndex: 9824 entries, 0 to 9999\nData columns (total 10 columns):\n #   Column                 Non-Null Count  Dtype  \n---  ------                 --------------  -----  \n 0   Deberta_Entailment     9824 non-null   float64\n 1   Deberta_Neutral        9824 non-null   float64\n 2   Deberta_Contradiction  9824 non-null   float64\n 3   Roberta_Entailment     9824 non-null   float64\n 4   Roberta_Neutral        9824 non-null   float64\n 5   Roberta_Contradiction  9824 non-null   float64\n 6   Albert_Entailment      9824 non-null   float64\n 7   Albert_Neutral         9824 non-null   float64\n 8   Albert_Contradiction   9824 non-null   float64\n 9   True_Label             9824 non-null   Int64  \ndtypes: Int64(1), float64(9)\nmemory usage: 853.8 KB\n","output_type":"stream"}]},{"cell_type":"code","source":"combined_snli_df","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.243892Z","iopub.execute_input":"2024-08-20T12:51:39.244546Z","iopub.status.idle":"2024-08-20T12:51:39.263445Z","shell.execute_reply.started":"2024-08-20T12:51:39.244509Z","shell.execute_reply":"2024-08-20T12:51:39.262478Z"},"trusted":true},"execution_count":334,"outputs":[{"execution_count":334,"output_type":"execute_result","data":{"text/plain":"      Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0               0.034767         0.962592               0.002641   \n1               0.001921         0.319032               0.679047   \n2               0.998783         0.000764               0.000453   \n3               0.001001         0.997708               0.001291   \n4               0.001080         0.301363               0.697557   \n...                  ...              ...                    ...   \n9995            0.998825         0.001033               0.000142   \n9996            0.000704         0.009793               0.989503   \n9997            0.999171         0.000493               0.000336   \n9998            0.000267         0.002178               0.997556   \n9999            0.003641         0.995224               0.001135   \n\n      Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0               0.012451         0.927093               0.060457   \n1               0.752766         0.242251               0.004983   \n2               0.000254         0.004494               0.995253   \n3               0.005844         0.990736               0.003419   \n4               0.278348         0.718575               0.003076   \n...                  ...              ...                    ...   \n9995            0.001264         0.028942               0.969794   \n9996            0.780946         0.217053               0.002001   \n9997            0.000054         0.000765               0.999181   \n9998            0.983402         0.015884               0.000714   \n9999            0.000904         0.995495               0.003601   \n\n      Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \n0              0.008653        0.947434              0.043913           1  \n1              0.740332        0.256434              0.003235           0  \n2              0.004677        0.060481              0.934843           2  \n3              0.034056        0.956687              0.009257           1  \n4              0.498761        0.499270              0.001969           0  \n...                 ...             ...                   ...         ...  \n9995           0.006420        0.057240              0.936340           2  \n9996           0.894637        0.104095              0.001267           0  \n9997           0.000838        0.002670              0.996493           2  \n9998           0.984347        0.015223              0.000430           0  \n9999           0.005047        0.990218              0.004735           1  \n\n[9824 rows x 10 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.034767</td>\n      <td>0.962592</td>\n      <td>0.002641</td>\n      <td>0.012451</td>\n      <td>0.927093</td>\n      <td>0.060457</td>\n      <td>0.008653</td>\n      <td>0.947434</td>\n      <td>0.043913</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.001921</td>\n      <td>0.319032</td>\n      <td>0.679047</td>\n      <td>0.752766</td>\n      <td>0.242251</td>\n      <td>0.004983</td>\n      <td>0.740332</td>\n      <td>0.256434</td>\n      <td>0.003235</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.998783</td>\n      <td>0.000764</td>\n      <td>0.000453</td>\n      <td>0.000254</td>\n      <td>0.004494</td>\n      <td>0.995253</td>\n      <td>0.004677</td>\n      <td>0.060481</td>\n      <td>0.934843</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.001001</td>\n      <td>0.997708</td>\n      <td>0.001291</td>\n      <td>0.005844</td>\n      <td>0.990736</td>\n      <td>0.003419</td>\n      <td>0.034056</td>\n      <td>0.956687</td>\n      <td>0.009257</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.001080</td>\n      <td>0.301363</td>\n      <td>0.697557</td>\n      <td>0.278348</td>\n      <td>0.718575</td>\n      <td>0.003076</td>\n      <td>0.498761</td>\n      <td>0.499270</td>\n      <td>0.001969</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9995</th>\n      <td>0.998825</td>\n      <td>0.001033</td>\n      <td>0.000142</td>\n      <td>0.001264</td>\n      <td>0.028942</td>\n      <td>0.969794</td>\n      <td>0.006420</td>\n      <td>0.057240</td>\n      <td>0.936340</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9996</th>\n      <td>0.000704</td>\n      <td>0.009793</td>\n      <td>0.989503</td>\n      <td>0.780946</td>\n      <td>0.217053</td>\n      <td>0.002001</td>\n      <td>0.894637</td>\n      <td>0.104095</td>\n      <td>0.001267</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9997</th>\n      <td>0.999171</td>\n      <td>0.000493</td>\n      <td>0.000336</td>\n      <td>0.000054</td>\n      <td>0.000765</td>\n      <td>0.999181</td>\n      <td>0.000838</td>\n      <td>0.002670</td>\n      <td>0.996493</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9998</th>\n      <td>0.000267</td>\n      <td>0.002178</td>\n      <td>0.997556</td>\n      <td>0.983402</td>\n      <td>0.015884</td>\n      <td>0.000714</td>\n      <td>0.984347</td>\n      <td>0.015223</td>\n      <td>0.000430</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9999</th>\n      <td>0.003641</td>\n      <td>0.995224</td>\n      <td>0.001135</td>\n      <td>0.000904</td>\n      <td>0.995495</td>\n      <td>0.003601</td>\n      <td>0.005047</td>\n      <td>0.990218</td>\n      <td>0.004735</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>9824 rows × 10 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import numpy as np\n\ndef calculate_margin(row):\n    # Assuming the row only contains the probabilities\n    sorted_probs = np.sort(row)  # Sort probabilities in ascending order\n    if len(sorted_probs) > 1:\n        return sorted_probs[-1] - sorted_probs[-2]  # Difference between the highest and second highest\n    else:\n        return 0  # This handles the edge case where there is only one probability value\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.264814Z","iopub.execute_input":"2024-08-20T12:51:39.265230Z","iopub.status.idle":"2024-08-20T12:51:39.271881Z","shell.execute_reply.started":"2024-08-20T12:51:39.265191Z","shell.execute_reply":"2024-08-20T12:51:39.271084Z"},"trusted":true},"execution_count":335,"outputs":[]},{"cell_type":"code","source":"# Applying to a sample DataFrame with made-up column names\ncombined_snli_df['confidence_margin_entailment'] = combined_snli_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_snli_df['confidence_margin_neutral'] = combined_snli_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_snli_df['confidence_margin_contradiction'] = combined_snli_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n\n\n# Applying to a sample DataFrame with made-up column names\ncombined_mnli_matched_df['confidence_margin_entailment'] = combined_mnli_matched_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_mnli_matched_df['confidence_margin_neutral'] = combined_mnli_matched_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_mnli_matched_df['confidence_margin_contradiction'] = combined_mnli_matched_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n\n# Applying to a sample DataFrame with made-up column names\ncombined_mnli_mismatched_df['confidence_margin_entailment'] = combined_mnli_mismatched_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_mnli_mismatched_df['confidence_margin_neutral'] = combined_mnli_mismatched_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_mnli_mismatched_df['confidence_margin_contradiction'] = combined_mnli_mismatched_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n\n# Applying to a sample DataFrame with made-up column names\ncombined_anli_r1_df['confidence_margin_entailment'] = combined_anli_r1_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_anli_r1_df['confidence_margin_neutral'] = combined_anli_r1_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_anli_r1_df['confidence_margin_contradiction'] = combined_anli_r1_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n\n\n# Applying to a sample DataFrame with made-up column names\ncombined_anli_r2_df['confidence_margin_entailment'] = combined_anli_r2_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_anli_r2_df['confidence_margin_neutral'] = combined_anli_r2_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_anli_r2_df['confidence_margin_contradiction'] = combined_anli_r2_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n\n\n# Applying to a sample DataFrame with made-up column names\ncombined_anli_r3_df['confidence_margin_entailment'] = combined_anli_r3_df[['Deberta_Entailment', 'Roberta_Entailment', 'Albert_Entailment']].apply(calculate_margin, axis=1)\ncombined_anli_r3_df['confidence_margin_neutral'] = combined_anli_r3_df[['Deberta_Neutral', 'Roberta_Neutral', 'Albert_Neutral']].apply(calculate_margin, axis=1)\ncombined_anli_r3_df['confidence_margin_contradiction'] = combined_anli_r3_df[['Deberta_Contradiction', 'Roberta_Contradiction', 'Albert_Contradiction']].apply(calculate_margin, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:39.273235Z","iopub.execute_input":"2024-08-20T12:51:39.273570Z","iopub.status.idle":"2024-08-20T12:51:42.080526Z","shell.execute_reply.started":"2024-08-20T12:51:39.273537Z","shell.execute_reply":"2024-08-20T12:51:42.079622Z"},"trusted":true},"execution_count":336,"outputs":[]},{"cell_type":"code","source":"combined_anli_r3_df","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:42.081918Z","iopub.execute_input":"2024-08-20T12:51:42.082646Z","iopub.status.idle":"2024-08-20T12:51:42.103744Z","shell.execute_reply.started":"2024-08-20T12:51:42.082610Z","shell.execute_reply":"2024-08-20T12:51:42.102735Z"},"trusted":true},"execution_count":337,"outputs":[{"execution_count":337,"output_type":"execute_result","data":{"text/plain":"      Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0               0.005921         0.960529               0.033551   \n1               0.009586         0.934714               0.055700   \n2               0.003428         0.976393               0.020179   \n3               0.004633         0.023985               0.971382   \n4               0.017428         0.633695               0.348877   \n...                  ...              ...                    ...   \n1195            0.150312         0.806051               0.043637   \n1196            0.971834         0.026294               0.001872   \n1197            0.973818         0.025074               0.001109   \n1198            0.341781         0.226539               0.431681   \n1199            0.472737         0.477215               0.050049   \n\n      Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0               0.022959         0.976533               0.000509   \n1               0.999611         0.000205               0.000185   \n2               0.002020         0.997897               0.000083   \n3               0.974441         0.024459               0.001100   \n4               0.984416         0.011166               0.004419   \n...                  ...              ...                    ...   \n1195            0.032452         0.068553               0.898994   \n1196            0.009070         0.824654               0.166276   \n1197            0.000352         0.000972               0.998677   \n1198            0.006147         0.073669               0.920184   \n1199            0.000420         0.991832               0.007748   \n\n      Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \\\n0              0.001848        0.998084              0.000067           0   \n1              0.951772        0.048075              0.000153           0   \n2              0.001014        0.998984              0.000002           0   \n3              0.996749        0.000989              0.002262           0   \n4              0.000518        0.128416              0.871066           0   \n...                 ...             ...                   ...         ...   \n1195           0.122045        0.254093              0.623862           2   \n1196           0.001115        0.003229              0.995656           2   \n1197           0.310862        0.618914              0.070225           2   \n1198           0.054172        0.317935              0.627893           2   \n1199           0.015091        0.121354              0.863554           2   \n\n      confidence_margin_entailment  confidence_margin_neutral  \\\n0                         0.017038                   0.021552   \n1                         0.047839                   0.886640   \n2                         0.001408                   0.001087   \n3                         0.022308                   0.000474   \n4                         0.966988                   0.505279   \n...                            ...                        ...   \n1195                      0.028268                   0.551958   \n1196                      0.962764                   0.798360   \n1197                      0.662956                   0.593840   \n1198                      0.287609                   0.091397   \n1199                      0.457646                   0.514617   \n\n      confidence_margin_contradiction  \n0                            0.033042  \n1                            0.055515  \n2                            0.020096  \n3                            0.969120  \n4                            0.522189  \n...                               ...  \n1195                         0.275132  \n1196                         0.829379  \n1197                         0.928452  \n1198                         0.292291  \n1199                         0.813506  \n\n[1200 rows x 13 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n      <th>confidence_margin_entailment</th>\n      <th>confidence_margin_neutral</th>\n      <th>confidence_margin_contradiction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.005921</td>\n      <td>0.960529</td>\n      <td>0.033551</td>\n      <td>0.022959</td>\n      <td>0.976533</td>\n      <td>0.000509</td>\n      <td>0.001848</td>\n      <td>0.998084</td>\n      <td>0.000067</td>\n      <td>0</td>\n      <td>0.017038</td>\n      <td>0.021552</td>\n      <td>0.033042</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.009586</td>\n      <td>0.934714</td>\n      <td>0.055700</td>\n      <td>0.999611</td>\n      <td>0.000205</td>\n      <td>0.000185</td>\n      <td>0.951772</td>\n      <td>0.048075</td>\n      <td>0.000153</td>\n      <td>0</td>\n      <td>0.047839</td>\n      <td>0.886640</td>\n      <td>0.055515</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.003428</td>\n      <td>0.976393</td>\n      <td>0.020179</td>\n      <td>0.002020</td>\n      <td>0.997897</td>\n      <td>0.000083</td>\n      <td>0.001014</td>\n      <td>0.998984</td>\n      <td>0.000002</td>\n      <td>0</td>\n      <td>0.001408</td>\n      <td>0.001087</td>\n      <td>0.020096</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004633</td>\n      <td>0.023985</td>\n      <td>0.971382</td>\n      <td>0.974441</td>\n      <td>0.024459</td>\n      <td>0.001100</td>\n      <td>0.996749</td>\n      <td>0.000989</td>\n      <td>0.002262</td>\n      <td>0</td>\n      <td>0.022308</td>\n      <td>0.000474</td>\n      <td>0.969120</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.017428</td>\n      <td>0.633695</td>\n      <td>0.348877</td>\n      <td>0.984416</td>\n      <td>0.011166</td>\n      <td>0.004419</td>\n      <td>0.000518</td>\n      <td>0.128416</td>\n      <td>0.871066</td>\n      <td>0</td>\n      <td>0.966988</td>\n      <td>0.505279</td>\n      <td>0.522189</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1195</th>\n      <td>0.150312</td>\n      <td>0.806051</td>\n      <td>0.043637</td>\n      <td>0.032452</td>\n      <td>0.068553</td>\n      <td>0.898994</td>\n      <td>0.122045</td>\n      <td>0.254093</td>\n      <td>0.623862</td>\n      <td>2</td>\n      <td>0.028268</td>\n      <td>0.551958</td>\n      <td>0.275132</td>\n    </tr>\n    <tr>\n      <th>1196</th>\n      <td>0.971834</td>\n      <td>0.026294</td>\n      <td>0.001872</td>\n      <td>0.009070</td>\n      <td>0.824654</td>\n      <td>0.166276</td>\n      <td>0.001115</td>\n      <td>0.003229</td>\n      <td>0.995656</td>\n      <td>2</td>\n      <td>0.962764</td>\n      <td>0.798360</td>\n      <td>0.829379</td>\n    </tr>\n    <tr>\n      <th>1197</th>\n      <td>0.973818</td>\n      <td>0.025074</td>\n      <td>0.001109</td>\n      <td>0.000352</td>\n      <td>0.000972</td>\n      <td>0.998677</td>\n      <td>0.310862</td>\n      <td>0.618914</td>\n      <td>0.070225</td>\n      <td>2</td>\n      <td>0.662956</td>\n      <td>0.593840</td>\n      <td>0.928452</td>\n    </tr>\n    <tr>\n      <th>1198</th>\n      <td>0.341781</td>\n      <td>0.226539</td>\n      <td>0.431681</td>\n      <td>0.006147</td>\n      <td>0.073669</td>\n      <td>0.920184</td>\n      <td>0.054172</td>\n      <td>0.317935</td>\n      <td>0.627893</td>\n      <td>2</td>\n      <td>0.287609</td>\n      <td>0.091397</td>\n      <td>0.292291</td>\n    </tr>\n    <tr>\n      <th>1199</th>\n      <td>0.472737</td>\n      <td>0.477215</td>\n      <td>0.050049</td>\n      <td>0.000420</td>\n      <td>0.991832</td>\n      <td>0.007748</td>\n      <td>0.015091</td>\n      <td>0.121354</td>\n      <td>0.863554</td>\n      <td>2</td>\n      <td>0.457646</td>\n      <td>0.514617</td>\n      <td>0.813506</td>\n    </tr>\n  </tbody>\n</table>\n<p>1200 rows × 13 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot histograms for the confidence margins\ndef plot_confidence_margins(df):\n    plt.figure(figsize=(15, 5))\n    plt.subplot(1, 3, 1)\n    plt.hist(df['confidence_margin_entailment'], bins=50, alpha=0.75, label='Entailment')\n    plt.title('Confidence Margin Histogram - Entailment')\n    plt.xlabel('Margin')\n    plt.ylabel('Frequency')\n    \n    plt.subplot(1, 3, 2)\n    plt.hist(df['confidence_margin_neutral'], bins=50, alpha=0.75, label='Neutral', color='green')\n    plt.title('Confidence Margin Histogram - Neutral')\n    plt.xlabel('Margin')\n    plt.ylabel('Frequency')\n    \n    plt.subplot(1, 3, 3)\n    plt.hist(df['confidence_margin_contradiction'], bins=50, alpha=0.75, label='Contradiction', color='red')\n    plt.title('Confidence Margin Histogram - Contradiction')\n    plt.xlabel('Margin')\n    plt.ylabel('Frequency')\n    \n    plt.tight_layout()\n    plt.show()\n\nplot_confidence_margins(combined_snli_df)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:42.105431Z","iopub.execute_input":"2024-08-20T12:51:42.105765Z","iopub.status.idle":"2024-08-20T12:51:42.934066Z","shell.execute_reply.started":"2024-08-20T12:51:42.105732Z","shell.execute_reply":"2024-08-20T12:51:42.933098Z"},"trusted":true},"execution_count":338,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x500 with 3 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"code","source":"# Define thresholds based on the distribution\nconfidence_threshold = 0.8\n\n# Apply threshold and create a new copy of the DataFrame\nhigh_confidence_snli = combined_snli_df[\n    (combined_snli_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_snli_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_snli_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()\n\n\n# Apply threshold and create a new copy of the DataFrame\nhigh_confidence_mnli_matched = combined_mnli_matched_df[\n    (combined_mnli_matched_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_mnli_matched_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_mnli_matched_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()\n\n# Apply threshold and create a new copy of the DataFrame\nhigh_confidence_mnli_mismatched = combined_mnli_mismatched_df[\n    (combined_mnli_mismatched_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_mnli_mismatched_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_mnli_mismatched_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()\n\n# Filter high confidence samples for ANLI round 1\nhigh_confidence_anli_r1 = combined_anli_r1_df[\n    (combined_anli_r1_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_anli_r1_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_anli_r1_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()\n\n# Filter high confidence samples for ANLI round 2\nhigh_confidence_anli_r2 = combined_anli_r2_df[\n    (combined_anli_r2_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_anli_r2_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_anli_r2_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()\n\n# Filter high confidence samples for ANLI round 3\nhigh_confidence_anli_r3 = combined_anli_r3_df[\n    (combined_anli_r3_df['confidence_margin_entailment'] >= confidence_threshold) | \n    (combined_anli_r3_df['confidence_margin_neutral'] >= confidence_threshold) |\n    (combined_anli_r3_df['confidence_margin_contradiction'] >= confidence_threshold)\n].copy()","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:42.935525Z","iopub.execute_input":"2024-08-20T12:51:42.935893Z","iopub.status.idle":"2024-08-20T12:51:42.960600Z","shell.execute_reply.started":"2024-08-20T12:51:42.935859Z","shell.execute_reply":"2024-08-20T12:51:42.959818Z"},"trusted":true},"execution_count":339,"outputs":[]},{"cell_type":"code","source":"# Majority vote logic with weights for numeric labels\ndef majority_vote_with_threshold(row):\n    votes = [0, 0, 0]  # Index 0 for entailment, 1 for neutral, 2 for contradiction\n    if row['confidence_margin_entailment'] >= confidence_threshold:\n        votes[0] += row['confidence_margin_entailment']\n    if row['confidence_margin_neutral'] >= confidence_threshold:\n        votes[1] += row['confidence_margin_neutral']\n    if row['confidence_margin_contradiction'] >= confidence_threshold:\n        votes[2] += row['confidence_margin_contradiction']\n    \n    # Return the index of the highest vote\n    return np.argmax(votes)\n\n# Apply the majority vote logic using .loc\nhigh_confidence_snli.loc[:, 'majority_vote'] = high_confidence_snli.apply(majority_vote_with_threshold, axis=1)\n# Apply the majority vote logic using .loc\nhigh_confidence_mnli_matched.loc[:, 'majority_vote'] = high_confidence_mnli_matched.apply(majority_vote_with_threshold, axis=1)\n# Apply majority vote logic\nhigh_confidence_mnli_mismatched.loc[:, 'majority_vote'] = high_confidence_mnli_mismatched.apply(majority_vote_with_threshold, axis=1)\n# Apply majority vote logic\nhigh_confidence_anli_r1.loc[:, 'majority_vote'] = high_confidence_anli_r1.apply(majority_vote_with_threshold, axis=1)\n# Apply majority vote logic\nhigh_confidence_anli_r2.loc[:, 'majority_vote'] = high_confidence_anli_r2.apply(majority_vote_with_threshold, axis=1)\n# Apply majority vote logic\nhigh_confidence_anli_r3.loc[:, 'majority_vote'] = high_confidence_anli_r3.apply(majority_vote_with_threshold, axis=1)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:42.961930Z","iopub.execute_input":"2024-08-20T12:51:42.962546Z","iopub.status.idle":"2024-08-20T12:51:44.811439Z","shell.execute_reply.started":"2024-08-20T12:51:42.962518Z","shell.execute_reply":"2024-08-20T12:51:44.810361Z"},"trusted":true},"execution_count":340,"outputs":[]},{"cell_type":"code","source":"high_confidence_anli_r3","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:44.812814Z","iopub.execute_input":"2024-08-20T12:51:44.813159Z","iopub.status.idle":"2024-08-20T12:51:44.835941Z","shell.execute_reply.started":"2024-08-20T12:51:44.813127Z","shell.execute_reply":"2024-08-20T12:51:44.834859Z"},"trusted":true},"execution_count":341,"outputs":[{"execution_count":341,"output_type":"execute_result","data":{"text/plain":"      Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n1               0.009586         0.934714               0.055700   \n3               0.004633         0.023985               0.971382   \n4               0.017428         0.633695               0.348877   \n5               0.002461         0.150678               0.846861   \n8               0.131793         0.776056               0.092151   \n...                  ...              ...                    ...   \n1193            0.957103         0.040533               0.002363   \n1194            0.945850         0.052813               0.001337   \n1196            0.971834         0.026294               0.001872   \n1197            0.973818         0.025074               0.001109   \n1199            0.472737         0.477215               0.050049   \n\n      Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n1               0.999611         0.000205               0.000185   \n3               0.974441         0.024459               0.001100   \n4               0.984416         0.011166               0.004419   \n5               0.999220         0.000554               0.000227   \n8               0.000384         0.002570               0.997046   \n...                  ...              ...                    ...   \n1193            0.000066         0.000316               0.999618   \n1194            0.000607         0.000853               0.998540   \n1196            0.009070         0.824654               0.166276   \n1197            0.000352         0.000972               0.998677   \n1199            0.000420         0.991832               0.007748   \n\n      Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \\\n1              0.951772        0.048075              0.000153           0   \n3              0.996749        0.000989              0.002262           0   \n4              0.000518        0.128416              0.871066           0   \n5              0.985221        0.014745              0.000034           0   \n8              0.049409        0.904356              0.046236           0   \n...                 ...             ...                   ...         ...   \n1193           0.009265        0.016116              0.974619           2   \n1194           0.000205        0.000291              0.999504           2   \n1196           0.001115        0.003229              0.995656           2   \n1197           0.310862        0.618914              0.070225           2   \n1199           0.015091        0.121354              0.863554           2   \n\n      confidence_margin_entailment  confidence_margin_neutral  \\\n1                         0.047839                   0.886640   \n3                         0.022308                   0.000474   \n4                         0.966988                   0.505279   \n5                         0.013998                   0.135933   \n8                         0.082384                   0.128299   \n...                            ...                        ...   \n1193                      0.947838                   0.024417   \n1194                      0.945243                   0.051960   \n1196                      0.962764                   0.798360   \n1197                      0.662956                   0.593840   \n1199                      0.457646                   0.514617   \n\n      confidence_margin_contradiction  majority_vote  \n1                            0.055515              1  \n3                            0.969120              2  \n4                            0.522189              0  \n5                            0.846635              2  \n8                            0.904895              2  \n...                               ...            ...  \n1193                         0.024999              0  \n1194                         0.000965              0  \n1196                         0.829379              0  \n1197                         0.928452              2  \n1199                         0.813506              2  \n\n[530 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n      <th>confidence_margin_entailment</th>\n      <th>confidence_margin_neutral</th>\n      <th>confidence_margin_contradiction</th>\n      <th>majority_vote</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>0.009586</td>\n      <td>0.934714</td>\n      <td>0.055700</td>\n      <td>0.999611</td>\n      <td>0.000205</td>\n      <td>0.000185</td>\n      <td>0.951772</td>\n      <td>0.048075</td>\n      <td>0.000153</td>\n      <td>0</td>\n      <td>0.047839</td>\n      <td>0.886640</td>\n      <td>0.055515</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.004633</td>\n      <td>0.023985</td>\n      <td>0.971382</td>\n      <td>0.974441</td>\n      <td>0.024459</td>\n      <td>0.001100</td>\n      <td>0.996749</td>\n      <td>0.000989</td>\n      <td>0.002262</td>\n      <td>0</td>\n      <td>0.022308</td>\n      <td>0.000474</td>\n      <td>0.969120</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.017428</td>\n      <td>0.633695</td>\n      <td>0.348877</td>\n      <td>0.984416</td>\n      <td>0.011166</td>\n      <td>0.004419</td>\n      <td>0.000518</td>\n      <td>0.128416</td>\n      <td>0.871066</td>\n      <td>0</td>\n      <td>0.966988</td>\n      <td>0.505279</td>\n      <td>0.522189</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.002461</td>\n      <td>0.150678</td>\n      <td>0.846861</td>\n      <td>0.999220</td>\n      <td>0.000554</td>\n      <td>0.000227</td>\n      <td>0.985221</td>\n      <td>0.014745</td>\n      <td>0.000034</td>\n      <td>0</td>\n      <td>0.013998</td>\n      <td>0.135933</td>\n      <td>0.846635</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.131793</td>\n      <td>0.776056</td>\n      <td>0.092151</td>\n      <td>0.000384</td>\n      <td>0.002570</td>\n      <td>0.997046</td>\n      <td>0.049409</td>\n      <td>0.904356</td>\n      <td>0.046236</td>\n      <td>0</td>\n      <td>0.082384</td>\n      <td>0.128299</td>\n      <td>0.904895</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1193</th>\n      <td>0.957103</td>\n      <td>0.040533</td>\n      <td>0.002363</td>\n      <td>0.000066</td>\n      <td>0.000316</td>\n      <td>0.999618</td>\n      <td>0.009265</td>\n      <td>0.016116</td>\n      <td>0.974619</td>\n      <td>2</td>\n      <td>0.947838</td>\n      <td>0.024417</td>\n      <td>0.024999</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1194</th>\n      <td>0.945850</td>\n      <td>0.052813</td>\n      <td>0.001337</td>\n      <td>0.000607</td>\n      <td>0.000853</td>\n      <td>0.998540</td>\n      <td>0.000205</td>\n      <td>0.000291</td>\n      <td>0.999504</td>\n      <td>2</td>\n      <td>0.945243</td>\n      <td>0.051960</td>\n      <td>0.000965</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1196</th>\n      <td>0.971834</td>\n      <td>0.026294</td>\n      <td>0.001872</td>\n      <td>0.009070</td>\n      <td>0.824654</td>\n      <td>0.166276</td>\n      <td>0.001115</td>\n      <td>0.003229</td>\n      <td>0.995656</td>\n      <td>2</td>\n      <td>0.962764</td>\n      <td>0.798360</td>\n      <td>0.829379</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1197</th>\n      <td>0.973818</td>\n      <td>0.025074</td>\n      <td>0.001109</td>\n      <td>0.000352</td>\n      <td>0.000972</td>\n      <td>0.998677</td>\n      <td>0.310862</td>\n      <td>0.618914</td>\n      <td>0.070225</td>\n      <td>2</td>\n      <td>0.662956</td>\n      <td>0.593840</td>\n      <td>0.928452</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1199</th>\n      <td>0.472737</td>\n      <td>0.477215</td>\n      <td>0.050049</td>\n      <td>0.000420</td>\n      <td>0.991832</td>\n      <td>0.007748</td>\n      <td>0.015091</td>\n      <td>0.121354</td>\n      <td>0.863554</td>\n      <td>2</td>\n      <td>0.457646</td>\n      <td>0.514617</td>\n      <td>0.813506</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>530 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"high_confidence_mnli_mismatched","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:44.837409Z","iopub.execute_input":"2024-08-20T12:51:44.838234Z","iopub.status.idle":"2024-08-20T12:51:44.862101Z","shell.execute_reply.started":"2024-08-20T12:51:44.838199Z","shell.execute_reply":"2024-08-20T12:51:44.861080Z"},"trusted":true},"execution_count":342,"outputs":[{"execution_count":342,"output_type":"execute_result","data":{"text/plain":"      Deberta_Entailment  Deberta_Neutral  Deberta_Contradiction  \\\n0               0.999667         0.000160               0.000173   \n1               0.998119         0.000962               0.000919   \n2               0.000552         0.004809               0.994639   \n3               0.827653         0.171961               0.000386   \n4               0.000292         0.002875               0.996833   \n...                  ...              ...                    ...   \n9824            0.000760         0.013837               0.985402   \n9825            0.000145         0.003906               0.995949   \n9826            0.996226         0.003656               0.000118   \n9828            0.997044         0.001226               0.001730   \n9829            0.999685         0.000112               0.000202   \n\n      Roberta_Entailment  Roberta_Neutral  Roberta_Contradiction  \\\n0               0.000068         0.000402               0.999529   \n1               0.000183         0.001511               0.998306   \n2               0.986062         0.012020               0.001918   \n3               0.000478         0.270953               0.728569   \n4               0.975167         0.021904               0.002929   \n...                  ...              ...                    ...   \n9824            0.972984         0.025382               0.001634   \n9825            0.952827         0.045891               0.001281   \n9826            0.000353         0.104717               0.894930   \n9828            0.000364         0.000717               0.998919   \n9829            0.000085         0.000289               0.999626   \n\n      Albert_Entailment  Albert_Neutral  Albert_Contradiction  True_Label  \\\n0              0.000894        0.003787              0.995318           2   \n1              0.006421        0.010224              0.983355           2   \n2              0.975041        0.023354              0.001605           0   \n3              0.001722        0.796122              0.202156           2   \n4              0.965952        0.032748              0.001300           0   \n...                 ...             ...                   ...         ...   \n9824           0.925212        0.069991              0.004798           0   \n9825           0.965645        0.032757              0.001598           0   \n9826           0.004991        0.039946              0.955062           2   \n9828           0.019078        0.039258              0.941663           2   \n9829           0.000420        0.001810              0.997770           2   \n\n      confidence_margin_entailment  confidence_margin_neutral  \\\n0                         0.998773                   0.003385   \n1                         0.991698                   0.008714   \n2                         0.011021                   0.011334   \n3                         0.825931                   0.525168   \n4                         0.009215                   0.010844   \n...                            ...                        ...   \n9824                      0.047772                   0.044608   \n9825                      0.012818                   0.013134   \n9826                      0.991234                   0.064771   \n9828                      0.977966                   0.038033   \n9829                      0.999265                   0.001521   \n\n      confidence_margin_contradiction  majority_vote  \n0                            0.004211              0  \n1                            0.014951              0  \n2                            0.992721              2  \n3                            0.526413              0  \n4                            0.993904              2  \n...                               ...            ...  \n9824                         0.980605              2  \n9825                         0.994351              2  \n9826                         0.060132              0  \n9828                         0.057256              0  \n9829                         0.001856              0  \n\n[5910 rows x 14 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Deberta_Entailment</th>\n      <th>Deberta_Neutral</th>\n      <th>Deberta_Contradiction</th>\n      <th>Roberta_Entailment</th>\n      <th>Roberta_Neutral</th>\n      <th>Roberta_Contradiction</th>\n      <th>Albert_Entailment</th>\n      <th>Albert_Neutral</th>\n      <th>Albert_Contradiction</th>\n      <th>True_Label</th>\n      <th>confidence_margin_entailment</th>\n      <th>confidence_margin_neutral</th>\n      <th>confidence_margin_contradiction</th>\n      <th>majority_vote</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.999667</td>\n      <td>0.000160</td>\n      <td>0.000173</td>\n      <td>0.000068</td>\n      <td>0.000402</td>\n      <td>0.999529</td>\n      <td>0.000894</td>\n      <td>0.003787</td>\n      <td>0.995318</td>\n      <td>2</td>\n      <td>0.998773</td>\n      <td>0.003385</td>\n      <td>0.004211</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.998119</td>\n      <td>0.000962</td>\n      <td>0.000919</td>\n      <td>0.000183</td>\n      <td>0.001511</td>\n      <td>0.998306</td>\n      <td>0.006421</td>\n      <td>0.010224</td>\n      <td>0.983355</td>\n      <td>2</td>\n      <td>0.991698</td>\n      <td>0.008714</td>\n      <td>0.014951</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.000552</td>\n      <td>0.004809</td>\n      <td>0.994639</td>\n      <td>0.986062</td>\n      <td>0.012020</td>\n      <td>0.001918</td>\n      <td>0.975041</td>\n      <td>0.023354</td>\n      <td>0.001605</td>\n      <td>0</td>\n      <td>0.011021</td>\n      <td>0.011334</td>\n      <td>0.992721</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.827653</td>\n      <td>0.171961</td>\n      <td>0.000386</td>\n      <td>0.000478</td>\n      <td>0.270953</td>\n      <td>0.728569</td>\n      <td>0.001722</td>\n      <td>0.796122</td>\n      <td>0.202156</td>\n      <td>2</td>\n      <td>0.825931</td>\n      <td>0.525168</td>\n      <td>0.526413</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.000292</td>\n      <td>0.002875</td>\n      <td>0.996833</td>\n      <td>0.975167</td>\n      <td>0.021904</td>\n      <td>0.002929</td>\n      <td>0.965952</td>\n      <td>0.032748</td>\n      <td>0.001300</td>\n      <td>0</td>\n      <td>0.009215</td>\n      <td>0.010844</td>\n      <td>0.993904</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>9824</th>\n      <td>0.000760</td>\n      <td>0.013837</td>\n      <td>0.985402</td>\n      <td>0.972984</td>\n      <td>0.025382</td>\n      <td>0.001634</td>\n      <td>0.925212</td>\n      <td>0.069991</td>\n      <td>0.004798</td>\n      <td>0</td>\n      <td>0.047772</td>\n      <td>0.044608</td>\n      <td>0.980605</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9825</th>\n      <td>0.000145</td>\n      <td>0.003906</td>\n      <td>0.995949</td>\n      <td>0.952827</td>\n      <td>0.045891</td>\n      <td>0.001281</td>\n      <td>0.965645</td>\n      <td>0.032757</td>\n      <td>0.001598</td>\n      <td>0</td>\n      <td>0.012818</td>\n      <td>0.013134</td>\n      <td>0.994351</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>9826</th>\n      <td>0.996226</td>\n      <td>0.003656</td>\n      <td>0.000118</td>\n      <td>0.000353</td>\n      <td>0.104717</td>\n      <td>0.894930</td>\n      <td>0.004991</td>\n      <td>0.039946</td>\n      <td>0.955062</td>\n      <td>2</td>\n      <td>0.991234</td>\n      <td>0.064771</td>\n      <td>0.060132</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9828</th>\n      <td>0.997044</td>\n      <td>0.001226</td>\n      <td>0.001730</td>\n      <td>0.000364</td>\n      <td>0.000717</td>\n      <td>0.998919</td>\n      <td>0.019078</td>\n      <td>0.039258</td>\n      <td>0.941663</td>\n      <td>2</td>\n      <td>0.977966</td>\n      <td>0.038033</td>\n      <td>0.057256</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>9829</th>\n      <td>0.999685</td>\n      <td>0.000112</td>\n      <td>0.000202</td>\n      <td>0.000085</td>\n      <td>0.000289</td>\n      <td>0.999626</td>\n      <td>0.000420</td>\n      <td>0.001810</td>\n      <td>0.997770</td>\n      <td>2</td>\n      <td>0.999265</td>\n      <td>0.001521</td>\n      <td>0.001856</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5910 rows × 14 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"!pip install  torch-geometric","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:44.863555Z","iopub.execute_input":"2024-08-20T12:51:44.864191Z","iopub.status.idle":"2024-08-20T12:51:57.393676Z","shell.execute_reply.started":"2024-08-20T12:51:44.864164Z","shell.execute_reply":"2024-08-20T12:51:57.392438Z"},"trusted":true},"execution_count":343,"outputs":[{"name":"stdout","text":"Requirement already satisfied: torch-geometric in /opt/conda/lib/python3.10/site-packages (2.5.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (4.66.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.26.4)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.11.4)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2024.3.0)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.2)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.9.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (2.31.0)\nRequirement already satisfied: pyparsing in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (3.1.1)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (1.2.2)\nRequirement already satisfied: psutil>=5.8.0 in /opt/conda/lib/python3.10/site-packages (from torch-geometric) (5.9.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->torch-geometric) (4.0.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch-geometric) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->torch-geometric) (2024.2.2)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (1.3.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->torch-geometric) (3.2.0)\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\n\n\n\n# Features and Labels\nX_snli = high_confidence_snli.drop(['True_Label','majority_vote'], axis=1).values\ny_snli = high_confidence_snli['True_Label'].values\n\n# Features and Labels\nX_mnli_matched = high_confidence_mnli_matched.drop(['True_Label','majority_vote'], axis=1).values\ny_mnli_matched = high_confidence_mnli_matched['True_Label'].values\n\n# Features and Labels\nX_mnli_mismatched = high_confidence_mnli_mismatched.drop(['True_Label','majority_vote'], axis=1).values\ny_mnli_mismatched = high_confidence_mnli_mismatched['True_Label'].values\n\n# Features and Labels\nX_anli_r1 = high_confidence_anli_r1.drop(['True_Label','majority_vote'], axis=1).values\ny_anli_r1 = high_confidence_anli_r1['True_Label'].values\n\n# Features and Labels\nX_anli_r2 = high_confidence_anli_r2.drop(['True_Label','majority_vote'], axis=1).values\ny_anli_r2 = high_confidence_anli_r2['True_Label'].values\n\n# Features and Labels\nX_anli_r3 = high_confidence_anli_r3.drop(['True_Label','majority_vote'], axis=1).values\ny_anli_r3 = high_confidence_anli_r3['True_Label'].values\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:57.395479Z","iopub.execute_input":"2024-08-20T12:51:57.395865Z","iopub.status.idle":"2024-08-20T12:51:57.413053Z","shell.execute_reply.started":"2024-08-20T12:51:57.395831Z","shell.execute_reply":"2024-08-20T12:51:57.412159Z"},"trusted":true},"execution_count":344,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\nfrom torch_geometric.nn import GATConv\nfrom torch_geometric.data import Data\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\n\n# Function to create graph data\ndef create_graph(data, labels):\n    num_nodes = data.shape[0]\n    edge_index = torch.tensor([range(num_nodes), range(num_nodes)], dtype=torch.long)  # Self-loops as edges\n    graph_data = Data(x=torch.tensor(data, dtype=torch.float), edge_index=edge_index, y=torch.tensor(labels, dtype=torch.long))\n    return graph_data\n\n\n# Define the enhanced GAT model with residual connections and multi-head attention\nclass EnhancedGAT(torch.nn.Module):\n    def __init__(self, input_dim, hidden_dim=32, heads=4, output_heads=1, dropout=0.3):\n        super(EnhancedGAT, self).__init__()\n        # First GAT layer with multi-head attention\n        self.conv1 = GATConv(input_dim, hidden_dim, heads=heads, dropout=dropout, concat=True)\n        # Second GAT layer with single attention head\n        self.conv2 = GATConv(hidden_dim * heads, 3, heads=output_heads, dropout=dropout, concat=False)\n        self.dropout = dropout\n        self.residual = input_dim == 3  # Residual connection if input dimension matches output dimension\n\n    def forward(self, data):\n        x, edge_index = data.x, data.edge_index\n        # Apply first GAT layer with attention dropout\n        x = self.conv1(x, edge_index)\n        x = F.relu(x)\n        x = F.dropout(x, p=self.dropout, training=self.training)  # Dropout for regularization\n        # Apply second GAT layer and residual connection if needed\n        x = self.conv2(x, edge_index)\n        if self.residual:\n            x += data.x  # Add residual connection\n        return F.log_softmax(x, dim=1)  # Log-softmax for classification\n\n# Define the early stopping class\nclass EarlyStopping:\n    def __init__(self, patience=10, min_delta=0.001):\n        self.patience = patience\n        self.min_delta = min_delta\n        self.best_loss = None\n        self.wait = 0\n        self.stop_training = False\n\n    def __call__(self, loss):\n        if self.best_loss is None or loss < self.best_loss - self.min_delta:\n            self.best_loss = loss\n            self.wait = 0\n        else:\n            self.wait += 1\n            if self.wait >= self.patience:\n                self.stop_training = True\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:57.414456Z","iopub.execute_input":"2024-08-20T12:51:57.414751Z","iopub.status.idle":"2024-08-20T12:51:57.430337Z","shell.execute_reply.started":"2024-08-20T12:51:57.414724Z","shell.execute_reply":"2024-08-20T12:51:57.429231Z"},"trusted":true},"execution_count":345,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom sklearn.metrics import f1_score, recall_score\n\n# Function to train and evaluate the enhanced GAT with F1-score and Recall\ndef train_and_evaluate_enhanced_gat(train_data, test_data, name, num_epochs=100, patience=10, learning_rate=0.005, weight_decay=5e-4):\n    input_dim = train_data.x.shape[1]  # Determine the input dimension\n    model = EnhancedGAT(input_dim)  # Instantiate Enhanced GAT model with the correct input dimension\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n    criterion = torch.nn.CrossEntropyLoss()\n    \n    # Initialize early stopping\n    early_stopping = EarlyStopping(patience=patience)\n    \n    # Variables for tracking metrics\n    train_losses = []\n    val_losses = []\n    train_f1_scores = []\n    val_f1_scores = []\n    train_recalls = []\n    val_recalls = []\n    \n    # Training with validation and early stopping\n    for epoch in range(num_epochs):\n        model.train()  # Set to training mode\n        optimizer.zero_grad()  # Reset gradients\n        out = model(train_data)  # Forward pass\n        loss = criterion(out, train_data.y)  # Calculate loss\n        loss.backward()  # Backward pass\n        optimizer.step()  # Update weights\n        train_losses.append(loss.item())  # Store training loss\n        \n        # Calculate training F1-score and recall\n        train_pred = out.argmax(dim=1)  # Get predicted labels\n        train_f1 = f1_score(train_data.y.cpu(), train_pred.cpu(), average='weighted')\n        train_recall = recall_score(train_data.y.cpu(), train_pred.cpu(), average='weighted')\n        train_f1_scores.append(train_f1)\n        train_recalls.append(train_recall)\n        \n        # Validation loop\n        model.eval()  # Set to evaluation mode\n        with torch.no_grad():\n            val_out = model(test_data)  # Forward pass for validation\n            val_loss = criterion(val_out, test_data.y)  # Validation loss\n            val_losses.append(val_loss.item())\n            val_pred = val_out.argmax(dim=1)  # Get predicted labels\n            val_f1 = f1_score(test_data.y.cpu(), val_pred.cpu(), average='weighted')\n            val_recall = recall_score(test_data.y.cpu(), val_pred.cpu(), average='weighted')\n            val_f1_scores.append(val_f1)\n            val_recalls.append(val_recall)\n        \n        # Check for early stopping\n        early_stopping(val_loss.item())\n        if early_stopping.stop_training:\n            print(\"Early stopping triggered\")\n            break\n        \n        print(f\"Epoch {epoch + 1}/{num_epochs}, Train Loss: {loss:.4f}, Val Loss: {val_loss:.4f}, Val F1: {val_f1:.4f}, Val Recall: {val_recall:.4f}\")\n    \n    # Final validation F1 and Recall\n    final_val_f1 = val_f1_scores[-1]\n    final_val_recall = val_recalls[-1]\n    \n    print(f\"Final Validation F1 on {name}: {final_val_f1:.4f}\")\n    print(f\"Final Validation Recall on {name}: {final_val_recall:.4f}\")\n    \n    return model, final_val_f1, final_val_recall","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:57.431645Z","iopub.execute_input":"2024-08-20T12:51:57.431955Z","iopub.status.idle":"2024-08-20T12:51:57.446531Z","shell.execute_reply.started":"2024-08-20T12:51:57.431917Z","shell.execute_reply":"2024-08-20T12:51:57.445578Z"},"trusted":true},"execution_count":346,"outputs":[]},{"cell_type":"code","source":"X_train_snli, X_test_snli, y_train_snli, y_test_snli = train_test_split(X_snli, y_snli, test_size=0.2, random_state=42)\n\n# Create graph data for SNLI\ntrain_graph_snli = create_graph(X_train_snli, y_train_snli)\ntest_graph_snli = create_graph(X_test_snli, y_test_snli)\n\n# Call the function to train and evaluate for SNLI\nmodel_snli, val_loss_snli, val_accuracy_snli = train_and_evaluate_enhanced_gat(\n    train_graph_snli, test_graph_snli, 'SNLI ENHANCED GAT'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:57.447741Z","iopub.execute_input":"2024-08-20T12:51:57.448054Z","iopub.status.idle":"2024-08-20T12:51:58.706964Z","shell.execute_reply.started":"2024-08-20T12:51:57.448015Z","shell.execute_reply":"2024-08-20T12:51:58.705891Z"},"trusted":true},"execution_count":347,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 0.9795, Val Loss: 0.8037, Val F1: 0.9205, Val Recall: 0.9437\nEpoch 2/100, Train Loss: 0.8561, Val Loss: 0.6831, Val F1: 0.9205, Val Recall: 0.9437\nEpoch 3/100, Train Loss: 0.7486, Val Loss: 0.5815, Val F1: 0.9205, Val Recall: 0.9437\nEpoch 4/100, Train Loss: 0.6780, Val Loss: 0.4980, Val F1: 0.9205, Val Recall: 0.9437\nEpoch 5/100, Train Loss: 0.6090, Val Loss: 0.4296, Val F1: 0.9205, Val Recall: 0.9437\nEpoch 6/100, Train Loss: 0.5669, Val Loss: 0.3743, Val F1: 0.9205, Val Recall: 0.9437\nEpoch 7/100, Train Loss: 0.5461, Val Loss: 0.3304, Val F1: 0.9205, Val Recall: 0.9437\nEpoch 8/100, Train Loss: 0.5201, Val Loss: 0.2964, Val F1: 0.9213, Val Recall: 0.9445\nEpoch 9/100, Train Loss: 0.5169, Val Loss: 0.2702, Val F1: 0.9213, Val Recall: 0.9445\nEpoch 10/100, Train Loss: 0.4860, Val Loss: 0.2495, Val F1: 0.9213, Val Recall: 0.9445\nEpoch 11/100, Train Loss: 0.4716, Val Loss: 0.2334, Val F1: 0.9213, Val Recall: 0.9445\nEpoch 12/100, Train Loss: 0.4777, Val Loss: 0.2207, Val F1: 0.9213, Val Recall: 0.9445\nEpoch 13/100, Train Loss: 0.4785, Val Loss: 0.2108, Val F1: 0.9213, Val Recall: 0.9445\nEpoch 14/100, Train Loss: 0.4571, Val Loss: 0.2027, Val F1: 0.9213, Val Recall: 0.9445\nEpoch 15/100, Train Loss: 0.4610, Val Loss: 0.1963, Val F1: 0.9213, Val Recall: 0.9445\nEpoch 16/100, Train Loss: 0.4500, Val Loss: 0.1911, Val F1: 0.9272, Val Recall: 0.9470\nEpoch 17/100, Train Loss: 0.4440, Val Loss: 0.1871, Val F1: 0.9291, Val Recall: 0.9478\nEpoch 18/100, Train Loss: 0.4368, Val Loss: 0.1840, Val F1: 0.9293, Val Recall: 0.9478\nEpoch 19/100, Train Loss: 0.4314, Val Loss: 0.1818, Val F1: 0.9351, Val Recall: 0.9495\nEpoch 20/100, Train Loss: 0.4347, Val Loss: 0.1802, Val F1: 0.9444, Val Recall: 0.9536\nEpoch 21/100, Train Loss: 0.4395, Val Loss: 0.1789, Val F1: 0.9458, Val Recall: 0.9536\nEpoch 22/100, Train Loss: 0.4396, Val Loss: 0.1779, Val F1: 0.9464, Val Recall: 0.9528\nEpoch 23/100, Train Loss: 0.4325, Val Loss: 0.1767, Val F1: 0.9476, Val Recall: 0.9536\nEpoch 24/100, Train Loss: 0.4205, Val Loss: 0.1753, Val F1: 0.9482, Val Recall: 0.9536\nEpoch 25/100, Train Loss: 0.4488, Val Loss: 0.1741, Val F1: 0.9482, Val Recall: 0.9536\nEpoch 26/100, Train Loss: 0.4260, Val Loss: 0.1728, Val F1: 0.9475, Val Recall: 0.9528\nEpoch 27/100, Train Loss: 0.4253, Val Loss: 0.1718, Val F1: 0.9475, Val Recall: 0.9528\nEpoch 28/100, Train Loss: 0.4056, Val Loss: 0.1709, Val F1: 0.9462, Val Recall: 0.9511\nEpoch 29/100, Train Loss: 0.4205, Val Loss: 0.1702, Val F1: 0.9462, Val Recall: 0.9511\nEpoch 30/100, Train Loss: 0.4136, Val Loss: 0.1697, Val F1: 0.9462, Val Recall: 0.9511\nEpoch 31/100, Train Loss: 0.4249, Val Loss: 0.1693, Val F1: 0.9457, Val Recall: 0.9511\nEpoch 32/100, Train Loss: 0.4217, Val Loss: 0.1690, Val F1: 0.9470, Val Recall: 0.9528\nEpoch 33/100, Train Loss: 0.4317, Val Loss: 0.1690, Val F1: 0.9470, Val Recall: 0.9528\nEpoch 34/100, Train Loss: 0.4233, Val Loss: 0.1692, Val F1: 0.9482, Val Recall: 0.9536\nEpoch 35/100, Train Loss: 0.4082, Val Loss: 0.1696, Val F1: 0.9475, Val Recall: 0.9528\nEpoch 36/100, Train Loss: 0.4123, Val Loss: 0.1701, Val F1: 0.9482, Val Recall: 0.9536\nEpoch 37/100, Train Loss: 0.4228, Val Loss: 0.1707, Val F1: 0.9482, Val Recall: 0.9536\nEpoch 38/100, Train Loss: 0.4218, Val Loss: 0.1712, Val F1: 0.9482, Val Recall: 0.9536\nEpoch 39/100, Train Loss: 0.4266, Val Loss: 0.1717, Val F1: 0.9469, Val Recall: 0.9519\nEarly stopping triggered\nFinal Validation F1 on SNLI ENHANCED GAT: 0.9469\nFinal Validation Recall on SNLI ENHANCED GAT: 0.9519\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train_mnli_matched, X_test_mnli_matched, y_train_mnli_matched, y_test_mnli_matched = train_test_split(X_mnli_matched, y_mnli_matched, test_size=0.2, random_state=42)\n\n# Create graph data for MNLI Matched\ntrain_graph_mnli_matched = create_graph(X_train_mnli_matched, y_train_mnli_matched)\ntest_graph_mnli_matched = create_graph(X_test_mnli_matched, y_test_mnli_matched)\n\n# Call the function to train and evaluate for MNLI Matched\nmodel_mnli_matched, val_loss_mnli_matched, val_accuracy_mnli_matched = train_and_evaluate_enhanced_gat(\n    train_graph_mnli_matched, test_graph_mnli_matched, 'MNLI Matched ENHANCED GAT'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:58.708410Z","iopub.execute_input":"2024-08-20T12:51:58.708718Z","iopub.status.idle":"2024-08-20T12:51:59.754402Z","shell.execute_reply.started":"2024-08-20T12:51:58.708690Z","shell.execute_reply":"2024-08-20T12:51:59.753382Z"},"trusted":true},"execution_count":348,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.1852, Val Loss: 0.9660, Val F1: 0.9462, Val Recall: 0.9608\nEpoch 2/100, Train Loss: 1.0009, Val Loss: 0.8033, Val F1: 0.9453, Val Recall: 0.9608\nEpoch 3/100, Train Loss: 0.8607, Val Loss: 0.6657, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 4/100, Train Loss: 0.7401, Val Loss: 0.5523, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 5/100, Train Loss: 0.6618, Val Loss: 0.4605, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 6/100, Train Loss: 0.6004, Val Loss: 0.3882, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 7/100, Train Loss: 0.5503, Val Loss: 0.3323, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 8/100, Train Loss: 0.5312, Val Loss: 0.2895, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 9/100, Train Loss: 0.5052, Val Loss: 0.2575, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 10/100, Train Loss: 0.4936, Val Loss: 0.2337, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 11/100, Train Loss: 0.4844, Val Loss: 0.2160, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 12/100, Train Loss: 0.4804, Val Loss: 0.2031, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 13/100, Train Loss: 0.4829, Val Loss: 0.1938, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 14/100, Train Loss: 0.4643, Val Loss: 0.1866, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 15/100, Train Loss: 0.4643, Val Loss: 0.1810, Val F1: 0.9462, Val Recall: 0.9616\nEpoch 16/100, Train Loss: 0.4687, Val Loss: 0.1765, Val F1: 0.9453, Val Recall: 0.9608\nEpoch 17/100, Train Loss: 0.4698, Val Loss: 0.1728, Val F1: 0.9453, Val Recall: 0.9608\nEpoch 18/100, Train Loss: 0.4659, Val Loss: 0.1697, Val F1: 0.9453, Val Recall: 0.9608\nEpoch 19/100, Train Loss: 0.4460, Val Loss: 0.1669, Val F1: 0.9453, Val Recall: 0.9608\nEpoch 20/100, Train Loss: 0.4525, Val Loss: 0.1644, Val F1: 0.9453, Val Recall: 0.9608\nEpoch 21/100, Train Loss: 0.4532, Val Loss: 0.1622, Val F1: 0.9453, Val Recall: 0.9608\nEpoch 22/100, Train Loss: 0.4465, Val Loss: 0.1601, Val F1: 0.9453, Val Recall: 0.9608\nEpoch 23/100, Train Loss: 0.4363, Val Loss: 0.1584, Val F1: 0.9453, Val Recall: 0.9608\nEpoch 24/100, Train Loss: 0.4464, Val Loss: 0.1571, Val F1: 0.9453, Val Recall: 0.9608\nEpoch 25/100, Train Loss: 0.4277, Val Loss: 0.1563, Val F1: 0.9474, Val Recall: 0.9616\nEpoch 26/100, Train Loss: 0.4441, Val Loss: 0.1562, Val F1: 0.9474, Val Recall: 0.9616\nEpoch 27/100, Train Loss: 0.4475, Val Loss: 0.1567, Val F1: 0.9492, Val Recall: 0.9616\nEpoch 28/100, Train Loss: 0.4490, Val Loss: 0.1578, Val F1: 0.9511, Val Recall: 0.9625\nEpoch 29/100, Train Loss: 0.4190, Val Loss: 0.1591, Val F1: 0.9580, Val Recall: 0.9659\nEpoch 30/100, Train Loss: 0.4275, Val Loss: 0.1600, Val F1: 0.9580, Val Recall: 0.9659\nEpoch 31/100, Train Loss: 0.4274, Val Loss: 0.1603, Val F1: 0.9580, Val Recall: 0.9659\nEpoch 32/100, Train Loss: 0.4358, Val Loss: 0.1598, Val F1: 0.9574, Val Recall: 0.9650\nEpoch 33/100, Train Loss: 0.4288, Val Loss: 0.1586, Val F1: 0.9580, Val Recall: 0.9659\nEarly stopping triggered\nFinal Validation F1 on MNLI Matched ENHANCED GAT: 0.9580\nFinal Validation Recall on MNLI Matched ENHANCED GAT: 0.9659\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train_mnli_mismatched, X_test_mnli_mismatched, y_train_mnli_mismatched, y_test_mnli_mismatched = train_test_split(X_mnli_mismatched, y_mnli_mismatched, test_size=0.2, random_state=42)\n\n# Create graph data for MNLI mismatched\ntrain_graph_mnli_mismatched = create_graph(X_train_mnli_mismatched, y_train_mnli_mismatched)\ntest_graph_mnli_mismatched = create_graph(X_test_mnli_mismatched, y_test_mnli_mismatched)\n\n# Call the function to train and evaluate for MNLI Matched\nmodel_mnli_mismatched, val_loss_mnli_mismatched, val_accuracy_mnli_mismatched = train_and_evaluate_enhanced_gat(\n    train_graph_mnli_mismatched, test_graph_mnli_mismatched, 'MNLI Mismatched ENHANCED GAT'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:51:59.755875Z","iopub.execute_input":"2024-08-20T12:51:59.756274Z","iopub.status.idle":"2024-08-20T12:52:01.149362Z","shell.execute_reply.started":"2024-08-20T12:51:59.756236Z","shell.execute_reply":"2024-08-20T12:52:01.148483Z"},"trusted":true},"execution_count":349,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.0972, Val Loss: 0.8878, Val F1: 0.9483, Val Recall: 0.9636\nEpoch 2/100, Train Loss: 0.9371, Val Loss: 0.7458, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 3/100, Train Loss: 0.7978, Val Loss: 0.6242, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 4/100, Train Loss: 0.7111, Val Loss: 0.5232, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 5/100, Train Loss: 0.6451, Val Loss: 0.4407, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 6/100, Train Loss: 0.5785, Val Loss: 0.3734, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 7/100, Train Loss: 0.5517, Val Loss: 0.3197, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 8/100, Train Loss: 0.5155, Val Loss: 0.2769, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 9/100, Train Loss: 0.4896, Val Loss: 0.2432, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 10/100, Train Loss: 0.4957, Val Loss: 0.2175, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 11/100, Train Loss: 0.4804, Val Loss: 0.1979, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 12/100, Train Loss: 0.4633, Val Loss: 0.1832, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 13/100, Train Loss: 0.4541, Val Loss: 0.1722, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 14/100, Train Loss: 0.4636, Val Loss: 0.1639, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 15/100, Train Loss: 0.4658, Val Loss: 0.1573, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 16/100, Train Loss: 0.4439, Val Loss: 0.1521, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 17/100, Train Loss: 0.4483, Val Loss: 0.1478, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 18/100, Train Loss: 0.4456, Val Loss: 0.1442, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 19/100, Train Loss: 0.4474, Val Loss: 0.1413, Val F1: 0.9508, Val Recall: 0.9662\nEpoch 20/100, Train Loss: 0.4363, Val Loss: 0.1391, Val F1: 0.9529, Val Recall: 0.9670\nEpoch 21/100, Train Loss: 0.4331, Val Loss: 0.1375, Val F1: 0.9529, Val Recall: 0.9670\nEpoch 22/100, Train Loss: 0.4373, Val Loss: 0.1367, Val F1: 0.9528, Val Recall: 0.9662\nEpoch 23/100, Train Loss: 0.4321, Val Loss: 0.1365, Val F1: 0.9518, Val Recall: 0.9645\nEpoch 24/100, Train Loss: 0.4166, Val Loss: 0.1368, Val F1: 0.9537, Val Recall: 0.9653\nEpoch 25/100, Train Loss: 0.4358, Val Loss: 0.1371, Val F1: 0.9517, Val Recall: 0.9619\nEpoch 26/100, Train Loss: 0.4256, Val Loss: 0.1367, Val F1: 0.9534, Val Recall: 0.9628\nEpoch 27/100, Train Loss: 0.4168, Val Loss: 0.1358, Val F1: 0.9524, Val Recall: 0.9611\nEpoch 28/100, Train Loss: 0.4140, Val Loss: 0.1343, Val F1: 0.9507, Val Recall: 0.9602\nEpoch 29/100, Train Loss: 0.4293, Val Loss: 0.1326, Val F1: 0.9507, Val Recall: 0.9602\nEpoch 30/100, Train Loss: 0.4295, Val Loss: 0.1311, Val F1: 0.9512, Val Recall: 0.9611\nEpoch 31/100, Train Loss: 0.4236, Val Loss: 0.1295, Val F1: 0.9522, Val Recall: 0.9628\nEpoch 32/100, Train Loss: 0.4140, Val Loss: 0.1282, Val F1: 0.9522, Val Recall: 0.9628\nEpoch 33/100, Train Loss: 0.4181, Val Loss: 0.1272, Val F1: 0.9532, Val Recall: 0.9645\nEpoch 34/100, Train Loss: 0.4161, Val Loss: 0.1266, Val F1: 0.9542, Val Recall: 0.9662\nEpoch 35/100, Train Loss: 0.4069, Val Loss: 0.1262, Val F1: 0.9523, Val Recall: 0.9653\nEpoch 36/100, Train Loss: 0.4099, Val Loss: 0.1260, Val F1: 0.9523, Val Recall: 0.9653\nEpoch 37/100, Train Loss: 0.4114, Val Loss: 0.1259, Val F1: 0.9523, Val Recall: 0.9653\nEpoch 38/100, Train Loss: 0.4143, Val Loss: 0.1260, Val F1: 0.9528, Val Recall: 0.9662\nEpoch 39/100, Train Loss: 0.4161, Val Loss: 0.1262, Val F1: 0.9523, Val Recall: 0.9653\nEpoch 40/100, Train Loss: 0.4041, Val Loss: 0.1265, Val F1: 0.9523, Val Recall: 0.9653\nEpoch 41/100, Train Loss: 0.4127, Val Loss: 0.1269, Val F1: 0.9523, Val Recall: 0.9653\nEpoch 42/100, Train Loss: 0.4066, Val Loss: 0.1273, Val F1: 0.9523, Val Recall: 0.9653\nEpoch 43/100, Train Loss: 0.4098, Val Loss: 0.1278, Val F1: 0.9523, Val Recall: 0.9653\nEarly stopping triggered\nFinal Validation F1 on MNLI Mismatched ENHANCED GAT: 0.9518\nFinal Validation Recall on MNLI Mismatched ENHANCED GAT: 0.9645\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train_anli_r1, X_test_anli_r1, y_train_anli_r1, y_test_anli_r1 = train_test_split(X_anli_r1, y_anli_r1, test_size=0.2, random_state=42)\n\n# Create graph data for ANLI r1\ntrain_graph_anli_r1 = create_graph(X_train_anli_r1, y_train_anli_r1)\ntest_graph_anli_r1 = create_graph(X_test_anli_r1, y_test_anli_r1)\n\n# Call the function to train and evaluate for ANLI r1\nmodel_anli_r1, val_loss_anli_r1, val_accuracy_anli_r1 = train_and_evaluate_enhanced_gat(\n    train_graph_anli_r1, test_graph_anli_r1, 'ANLI R1 ENHANCED GAT'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:52:01.150633Z","iopub.execute_input":"2024-08-20T12:52:01.151500Z","iopub.status.idle":"2024-08-20T12:52:01.612874Z","shell.execute_reply.started":"2024-08-20T12:52:01.151468Z","shell.execute_reply":"2024-08-20T12:52:01.611845Z"},"trusted":true},"execution_count":350,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.1204, Val Loss: 1.0080, Val F1: 0.7551, Val Recall: 0.7558\nEpoch 2/100, Train Loss: 1.0365, Val Loss: 0.9118, Val F1: 0.8114, Val Recall: 0.8140\nEpoch 3/100, Train Loss: 0.9536, Val Loss: 0.8315, Val F1: 0.7985, Val Recall: 0.8140\nEpoch 4/100, Train Loss: 0.8903, Val Loss: 0.7630, Val F1: 0.8197, Val Recall: 0.8372\nEpoch 5/100, Train Loss: 0.8580, Val Loss: 0.7052, Val F1: 0.8197, Val Recall: 0.8372\nEpoch 6/100, Train Loss: 0.8516, Val Loss: 0.6578, Val F1: 0.8356, Val Recall: 0.8488\nEpoch 7/100, Train Loss: 0.8417, Val Loss: 0.6192, Val F1: 0.8356, Val Recall: 0.8488\nEpoch 8/100, Train Loss: 0.8040, Val Loss: 0.5868, Val F1: 0.8356, Val Recall: 0.8488\nEpoch 9/100, Train Loss: 0.7704, Val Loss: 0.5596, Val F1: 0.8356, Val Recall: 0.8488\nEpoch 10/100, Train Loss: 0.7848, Val Loss: 0.5374, Val F1: 0.8401, Val Recall: 0.8488\nEpoch 11/100, Train Loss: 0.7772, Val Loss: 0.5201, Val F1: 0.8203, Val Recall: 0.8256\nEpoch 12/100, Train Loss: 0.8132, Val Loss: 0.5076, Val F1: 0.8239, Val Recall: 0.8256\nEpoch 13/100, Train Loss: 0.7179, Val Loss: 0.4972, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 14/100, Train Loss: 0.7461, Val Loss: 0.4889, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 15/100, Train Loss: 0.7217, Val Loss: 0.4821, Val F1: 0.8397, Val Recall: 0.8372\nEpoch 16/100, Train Loss: 0.7545, Val Loss: 0.4774, Val F1: 0.8397, Val Recall: 0.8372\nEpoch 17/100, Train Loss: 0.7600, Val Loss: 0.4715, Val F1: 0.8397, Val Recall: 0.8372\nEpoch 18/100, Train Loss: 0.7401, Val Loss: 0.4632, Val F1: 0.8397, Val Recall: 0.8372\nEpoch 19/100, Train Loss: 0.7736, Val Loss: 0.4553, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 20/100, Train Loss: 0.7183, Val Loss: 0.4495, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 21/100, Train Loss: 0.6998, Val Loss: 0.4447, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 22/100, Train Loss: 0.6956, Val Loss: 0.4424, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 23/100, Train Loss: 0.7333, Val Loss: 0.4426, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 24/100, Train Loss: 0.7681, Val Loss: 0.4426, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 25/100, Train Loss: 0.7666, Val Loss: 0.4448, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 26/100, Train Loss: 0.7415, Val Loss: 0.4460, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 27/100, Train Loss: 0.6841, Val Loss: 0.4466, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 28/100, Train Loss: 0.7001, Val Loss: 0.4449, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 29/100, Train Loss: 0.7091, Val Loss: 0.4450, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 30/100, Train Loss: 0.7226, Val Loss: 0.4446, Val F1: 0.8372, Val Recall: 0.8372\nEpoch 31/100, Train Loss: 0.7156, Val Loss: 0.4439, Val F1: 0.8372, Val Recall: 0.8372\nEarly stopping triggered\nFinal Validation F1 on ANLI R1 ENHANCED GAT: 0.8372\nFinal Validation Recall on ANLI R1 ENHANCED GAT: 0.8372\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train_anli_r2, X_test_anli_r2, y_train_anli_r2, y_test_anli_r2 = train_test_split(X_anli_r2, y_anli_r2, test_size=0.2, random_state=42)\n\n# Create graph data for ANLI r2\ntrain_graph_anli_r2 = create_graph(X_train_anli_r2, y_train_anli_r2)\ntest_graph_anli_r2 = create_graph(X_test_anli_r2, y_test_anli_r2)\n\n# Call the function to train and evaluate for ANLI r2\nmodel_anli_r2, val_loss_anli_r2, val_accuracy_anli_r2 = train_and_evaluate_enhanced_gat(\n    train_graph_anli_r2, test_graph_anli_r2, 'ANLI R2 ENHANCED GAT'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:52:01.614130Z","iopub.execute_input":"2024-08-20T12:52:01.614442Z","iopub.status.idle":"2024-08-20T12:52:02.041823Z","shell.execute_reply.started":"2024-08-20T12:52:01.614416Z","shell.execute_reply":"2024-08-20T12:52:02.040651Z"},"trusted":true},"execution_count":351,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.2348, Val Loss: 1.1210, Val F1: 0.1545, Val Recall: 0.3146\nEpoch 2/100, Train Loss: 1.1574, Val Loss: 1.0460, Val F1: 0.3436, Val Recall: 0.4270\nEpoch 3/100, Train Loss: 1.0672, Val Loss: 0.9838, Val F1: 0.5400, Val Recall: 0.6292\nEpoch 4/100, Train Loss: 1.0160, Val Loss: 0.9306, Val F1: 0.5603, Val Recall: 0.6517\nEpoch 5/100, Train Loss: 0.9951, Val Loss: 0.8856, Val F1: 0.5705, Val Recall: 0.6629\nEpoch 6/100, Train Loss: 0.9411, Val Loss: 0.8482, Val F1: 0.5705, Val Recall: 0.6629\nEpoch 7/100, Train Loss: 0.9299, Val Loss: 0.8161, Val F1: 0.5705, Val Recall: 0.6629\nEpoch 8/100, Train Loss: 0.8991, Val Loss: 0.7885, Val F1: 0.5705, Val Recall: 0.6629\nEpoch 9/100, Train Loss: 0.8925, Val Loss: 0.7655, Val F1: 0.5705, Val Recall: 0.6629\nEpoch 10/100, Train Loss: 0.8905, Val Loss: 0.7457, Val F1: 0.6702, Val Recall: 0.7079\nEpoch 11/100, Train Loss: 0.8545, Val Loss: 0.7287, Val F1: 0.7047, Val Recall: 0.7191\nEpoch 12/100, Train Loss: 0.8485, Val Loss: 0.7149, Val F1: 0.7191, Val Recall: 0.7303\nEpoch 13/100, Train Loss: 0.8686, Val Loss: 0.7048, Val F1: 0.7087, Val Recall: 0.7191\nEpoch 14/100, Train Loss: 0.8627, Val Loss: 0.6971, Val F1: 0.7081, Val Recall: 0.7191\nEpoch 15/100, Train Loss: 0.8190, Val Loss: 0.6905, Val F1: 0.7218, Val Recall: 0.7303\nEpoch 16/100, Train Loss: 0.8182, Val Loss: 0.6846, Val F1: 0.7218, Val Recall: 0.7303\nEpoch 17/100, Train Loss: 0.8529, Val Loss: 0.6807, Val F1: 0.7218, Val Recall: 0.7303\nEpoch 18/100, Train Loss: 0.8103, Val Loss: 0.6783, Val F1: 0.7218, Val Recall: 0.7303\nEpoch 19/100, Train Loss: 0.8272, Val Loss: 0.6774, Val F1: 0.7218, Val Recall: 0.7303\nEpoch 20/100, Train Loss: 0.8351, Val Loss: 0.6782, Val F1: 0.7218, Val Recall: 0.7303\nEpoch 21/100, Train Loss: 0.8130, Val Loss: 0.6787, Val F1: 0.7218, Val Recall: 0.7303\nEpoch 22/100, Train Loss: 0.8296, Val Loss: 0.6792, Val F1: 0.7218, Val Recall: 0.7303\nEpoch 23/100, Train Loss: 0.8170, Val Loss: 0.6799, Val F1: 0.7218, Val Recall: 0.7303\nEpoch 24/100, Train Loss: 0.8138, Val Loss: 0.6828, Val F1: 0.7328, Val Recall: 0.7416\nEpoch 25/100, Train Loss: 0.8569, Val Loss: 0.6874, Val F1: 0.7446, Val Recall: 0.7528\nEpoch 26/100, Train Loss: 0.8302, Val Loss: 0.6919, Val F1: 0.7446, Val Recall: 0.7528\nEpoch 27/100, Train Loss: 0.7743, Val Loss: 0.6943, Val F1: 0.7557, Val Recall: 0.7640\nEarly stopping triggered\nFinal Validation F1 on ANLI R2 ENHANCED GAT: 0.7446\nFinal Validation Recall on ANLI R2 ENHANCED GAT: 0.7528\n","output_type":"stream"}]},{"cell_type":"code","source":"X_train_anli_r3, X_test_anli_r3, y_train_anli_r3, y_test_anli_r3 = train_test_split(X_anli_r3, y_anli_r3, test_size=0.2, random_state=42)\n\n# Create graph data for ANLI r3\ntrain_graph_anli_r3 = create_graph(X_train_anli_r3, y_train_anli_r3)\ntest_graph_anli_r3 = create_graph(X_test_anli_r3, y_test_anli_r3)\n\n# Call the function to train and evaluate for ANLI r3\nmodel_anli_r3, val_loss_anli_r3, val_accuracy_anli_r3 = train_and_evaluate_enhanced_gat(\n    train_graph_anli_r3, test_graph_anli_r3, 'ANLI R3 ENHANCED GAT'\n)","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:52:02.049110Z","iopub.execute_input":"2024-08-20T12:52:02.049397Z","iopub.status.idle":"2024-08-20T12:52:02.450130Z","shell.execute_reply.started":"2024-08-20T12:52:02.049372Z","shell.execute_reply":"2024-08-20T12:52:02.449077Z"},"trusted":true},"execution_count":352,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.0767, Val Loss: 0.9906, Val F1: 0.5812, Val Recall: 0.6887\nEpoch 2/100, Train Loss: 1.0143, Val Loss: 0.9307, Val F1: 0.5812, Val Recall: 0.6887\nEpoch 3/100, Train Loss: 0.9881, Val Loss: 0.8821, Val F1: 0.5837, Val Recall: 0.6698\nEpoch 4/100, Train Loss: 0.9333, Val Loss: 0.8423, Val F1: 0.6485, Val Recall: 0.6887\nEpoch 5/100, Train Loss: 0.9328, Val Loss: 0.8096, Val F1: 0.6883, Val Recall: 0.6981\nEpoch 6/100, Train Loss: 0.8713, Val Loss: 0.7805, Val F1: 0.6944, Val Recall: 0.6981\nEpoch 7/100, Train Loss: 0.8834, Val Loss: 0.7561, Val F1: 0.7016, Val Recall: 0.7075\nEpoch 8/100, Train Loss: 0.8413, Val Loss: 0.7338, Val F1: 0.6940, Val Recall: 0.7075\nEpoch 9/100, Train Loss: 0.8534, Val Loss: 0.7155, Val F1: 0.7026, Val Recall: 0.7170\nEpoch 10/100, Train Loss: 0.8372, Val Loss: 0.7010, Val F1: 0.7112, Val Recall: 0.7264\nEpoch 11/100, Train Loss: 0.8467, Val Loss: 0.6902, Val F1: 0.7198, Val Recall: 0.7358\nEpoch 12/100, Train Loss: 0.8402, Val Loss: 0.6830, Val F1: 0.7160, Val Recall: 0.7358\nEpoch 13/100, Train Loss: 0.7986, Val Loss: 0.6766, Val F1: 0.7033, Val Recall: 0.7264\nEpoch 14/100, Train Loss: 0.8012, Val Loss: 0.6709, Val F1: 0.6961, Val Recall: 0.7170\nEpoch 15/100, Train Loss: 0.8219, Val Loss: 0.6669, Val F1: 0.7086, Val Recall: 0.7264\nEpoch 16/100, Train Loss: 0.8214, Val Loss: 0.6646, Val F1: 0.7009, Val Recall: 0.7170\nEpoch 17/100, Train Loss: 0.8499, Val Loss: 0.6650, Val F1: 0.7367, Val Recall: 0.7453\nEpoch 18/100, Train Loss: 0.8365, Val Loss: 0.6674, Val F1: 0.7282, Val Recall: 0.7358\nEpoch 19/100, Train Loss: 0.8091, Val Loss: 0.6700, Val F1: 0.7282, Val Recall: 0.7358\nEpoch 20/100, Train Loss: 0.8218, Val Loss: 0.6722, Val F1: 0.7367, Val Recall: 0.7453\nEpoch 21/100, Train Loss: 0.7954, Val Loss: 0.6737, Val F1: 0.7253, Val Recall: 0.7358\nEpoch 22/100, Train Loss: 0.8206, Val Loss: 0.6755, Val F1: 0.7137, Val Recall: 0.7264\nEpoch 23/100, Train Loss: 0.7855, Val Loss: 0.6773, Val F1: 0.6890, Val Recall: 0.7075\nEpoch 24/100, Train Loss: 0.8113, Val Loss: 0.6790, Val F1: 0.6890, Val Recall: 0.7075\nEpoch 25/100, Train Loss: 0.8101, Val Loss: 0.6801, Val F1: 0.6759, Val Recall: 0.6981\nEarly stopping triggered\nFinal Validation F1 on ANLI R3 ENHANCED GAT: 0.7086\nFinal Validation Recall on ANLI R3 ENHANCED GAT: 0.7264\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\n\n# Convert NumPy arrays to PyTorch tensors\nX_train_snli_tensor = torch.tensor(X_train_snli)\nX_train_mnli_matched_tensor = torch.tensor(X_train_mnli_matched)\nX_train_mnli_mismatched_tensor = torch.tensor(X_train_mnli_mismatched)\nX_train_anli_r1_tensor = torch.tensor(X_train_anli_r1)\nX_train_anli_r2_tensor = torch.tensor(X_train_anli_r2)\nX_train_anli_r3_tensor = torch.tensor(X_train_anli_r3)\n\ny_train_snli_tensor = torch.tensor(y_train_snli)\ny_train_mnli_matched_tensor = torch.tensor(y_train_mnli_matched)\ny_train_mnli_mismatched_tensor = torch.tensor(y_train_mnli_mismatched)\ny_train_anli_r1_tensor = torch.tensor(y_train_anli_r1)\ny_train_anli_r2_tensor = torch.tensor(y_train_anli_r2)\ny_train_anli_r3_tensor = torch.tensor(y_train_anli_r3)\n\n# Concatenate X_train and y_train for all tasks\nX_train_all = torch.cat([X_train_snli_tensor, X_train_mnli_matched_tensor, X_train_mnli_mismatched_tensor, X_train_anli_r1_tensor, X_train_anli_r2_tensor, X_train_anli_r3_tensor], dim=0)\ny_train_all = torch.cat([y_train_snli_tensor, y_train_mnli_matched_tensor, y_train_mnli_mismatched_tensor, y_train_anli_r1_tensor, y_train_anli_r2_tensor, y_train_anli_r3_tensor], dim=0)\n\n# Convert NumPy arrays to PyTorch tensors for test data\nX_test_snli_tensor = torch.tensor(X_test_snli)\nX_test_mnli_matched_tensor = torch.tensor(X_test_mnli_matched)\nX_test_mnli_mismatched_tensor = torch.tensor(X_test_mnli_mismatched)\nX_test_anli_r1_tensor = torch.tensor(X_test_anli_r1)\nX_test_anli_r2_tensor = torch.tensor(X_test_anli_r2)\nX_test_anli_r3_tensor = torch.tensor(X_test_anli_r3)\n\ny_test_snli_tensor = torch.tensor(y_test_snli)\ny_test_mnli_matched_tensor = torch.tensor(y_test_mnli_matched)\ny_test_mnli_mismatched_tensor = torch.tensor(y_test_mnli_mismatched)\ny_test_anli_r1_tensor = torch.tensor(y_test_anli_r1)\ny_test_anli_r2_tensor = torch.tensor(y_test_anli_r2)\ny_test_anli_r3_tensor = torch.tensor(y_test_anli_r3)\n\n# Concatenate X_test and y_test for all tasks\nX_test_all = torch.cat([X_test_snli_tensor, X_test_mnli_matched_tensor, X_test_mnli_mismatched_tensor, X_test_anli_r1_tensor, X_test_anli_r2_tensor, X_test_anli_r3_tensor], dim=0)\ny_test_all = torch.cat([y_test_snli_tensor, y_test_mnli_matched_tensor, y_test_mnli_mismatched_tensor, y_test_anli_r1_tensor, y_test_anli_r2_tensor, y_test_anli_r3_tensor], dim=0)\n\n# Create graph data for ALL combined data\ntrain_graph_all = create_graph(X_train_all, y_train_all)\ntest_graph_all = create_graph(X_test_all, y_test_all)\n\n# Train and evaluate the model on the combined dataset\nmodel_all, final_val_loss_all, final_val_accuracy_all = train_and_evaluate_enhanced_gat(\n    train_graph_all, test_graph_all, 'Combined Tasks with ENHANCED GAT'\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-20T12:52:02.451651Z","iopub.execute_input":"2024-08-20T12:52:02.451947Z","iopub.status.idle":"2024-08-20T12:52:05.881215Z","shell.execute_reply.started":"2024-08-20T12:52:02.451920Z","shell.execute_reply":"2024-08-20T12:52:05.880079Z"},"trusted":true},"execution_count":353,"outputs":[{"name":"stdout","text":"Epoch 1/100, Train Loss: 1.0941, Val Loss: 0.9142, Val F1: 0.8771, Val Recall: 0.9011\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_34/1454212303.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  graph_data = Data(x=torch.tensor(data, dtype=torch.float), edge_index=edge_index, y=torch.tensor(labels, dtype=torch.long))\n","output_type":"stream"},{"name":"stdout","text":"Epoch 2/100, Train Loss: 0.9437, Val Loss: 0.7784, Val F1: 0.9061, Val Recall: 0.9310\nEpoch 3/100, Train Loss: 0.8263, Val Loss: 0.6614, Val F1: 0.9083, Val Recall: 0.9334\nEpoch 4/100, Train Loss: 0.7312, Val Loss: 0.5631, Val F1: 0.9103, Val Recall: 0.9355\nEpoch 5/100, Train Loss: 0.6582, Val Loss: 0.4827, Val F1: 0.9113, Val Recall: 0.9365\nEpoch 6/100, Train Loss: 0.6145, Val Loss: 0.4184, Val F1: 0.9113, Val Recall: 0.9365\nEpoch 7/100, Train Loss: 0.5828, Val Loss: 0.3678, Val F1: 0.9118, Val Recall: 0.9370\nEpoch 8/100, Train Loss: 0.5511, Val Loss: 0.3286, Val F1: 0.9116, Val Recall: 0.9368\nEpoch 9/100, Train Loss: 0.5335, Val Loss: 0.2985, Val F1: 0.9116, Val Recall: 0.9368\nEpoch 10/100, Train Loss: 0.5264, Val Loss: 0.2754, Val F1: 0.9116, Val Recall: 0.9368\nEpoch 11/100, Train Loss: 0.5146, Val Loss: 0.2580, Val F1: 0.9116, Val Recall: 0.9368\nEpoch 12/100, Train Loss: 0.5125, Val Loss: 0.2445, Val F1: 0.9116, Val Recall: 0.9368\nEpoch 13/100, Train Loss: 0.5096, Val Loss: 0.2341, Val F1: 0.9116, Val Recall: 0.9368\nEpoch 14/100, Train Loss: 0.5031, Val Loss: 0.2257, Val F1: 0.9116, Val Recall: 0.9368\nEpoch 15/100, Train Loss: 0.5027, Val Loss: 0.2189, Val F1: 0.9116, Val Recall: 0.9368\nEpoch 16/100, Train Loss: 0.4918, Val Loss: 0.2132, Val F1: 0.9115, Val Recall: 0.9365\nEpoch 17/100, Train Loss: 0.4862, Val Loss: 0.2085, Val F1: 0.9147, Val Recall: 0.9375\nEpoch 18/100, Train Loss: 0.4860, Val Loss: 0.2047, Val F1: 0.9167, Val Recall: 0.9383\nEpoch 19/100, Train Loss: 0.4812, Val Loss: 0.2019, Val F1: 0.9190, Val Recall: 0.9391\nEpoch 20/100, Train Loss: 0.4777, Val Loss: 0.2002, Val F1: 0.9287, Val Recall: 0.9420\nEpoch 21/100, Train Loss: 0.4818, Val Loss: 0.1999, Val F1: 0.9336, Val Recall: 0.9427\nEpoch 22/100, Train Loss: 0.4787, Val Loss: 0.2006, Val F1: 0.9382, Val Recall: 0.9443\nEpoch 23/100, Train Loss: 0.4695, Val Loss: 0.2018, Val F1: 0.9364, Val Recall: 0.9417\nEpoch 24/100, Train Loss: 0.4804, Val Loss: 0.2024, Val F1: 0.9376, Val Recall: 0.9417\nEpoch 25/100, Train Loss: 0.4730, Val Loss: 0.2017, Val F1: 0.9377, Val Recall: 0.9417\nEpoch 26/100, Train Loss: 0.4786, Val Loss: 0.2000, Val F1: 0.9377, Val Recall: 0.9417\nEpoch 27/100, Train Loss: 0.4697, Val Loss: 0.1976, Val F1: 0.9380, Val Recall: 0.9422\nEpoch 28/100, Train Loss: 0.4738, Val Loss: 0.1950, Val F1: 0.9386, Val Recall: 0.9433\nEpoch 29/100, Train Loss: 0.4646, Val Loss: 0.1927, Val F1: 0.9375, Val Recall: 0.9433\nEpoch 30/100, Train Loss: 0.4626, Val Loss: 0.1908, Val F1: 0.9369, Val Recall: 0.9438\nEpoch 31/100, Train Loss: 0.4631, Val Loss: 0.1896, Val F1: 0.9367, Val Recall: 0.9453\nEpoch 32/100, Train Loss: 0.4682, Val Loss: 0.1889, Val F1: 0.9352, Val Recall: 0.9451\nEpoch 33/100, Train Loss: 0.4558, Val Loss: 0.1885, Val F1: 0.9349, Val Recall: 0.9453\nEpoch 34/100, Train Loss: 0.4619, Val Loss: 0.1883, Val F1: 0.9344, Val Recall: 0.9453\nEpoch 35/100, Train Loss: 0.4531, Val Loss: 0.1882, Val F1: 0.9346, Val Recall: 0.9456\nEpoch 36/100, Train Loss: 0.4568, Val Loss: 0.1882, Val F1: 0.9353, Val Recall: 0.9459\nEpoch 37/100, Train Loss: 0.4598, Val Loss: 0.1883, Val F1: 0.9357, Val Recall: 0.9461\nEpoch 38/100, Train Loss: 0.4574, Val Loss: 0.1885, Val F1: 0.9355, Val Recall: 0.9459\nEpoch 39/100, Train Loss: 0.4599, Val Loss: 0.1891, Val F1: 0.9365, Val Recall: 0.9464\nEpoch 40/100, Train Loss: 0.4615, Val Loss: 0.1898, Val F1: 0.9359, Val Recall: 0.9451\nEpoch 41/100, Train Loss: 0.4543, Val Loss: 0.1907, Val F1: 0.9360, Val Recall: 0.9448\nEpoch 42/100, Train Loss: 0.4584, Val Loss: 0.1917, Val F1: 0.9376, Val Recall: 0.9459\nEarly stopping triggered\nFinal Validation F1 on Combined Tasks with ENHANCED GAT: 0.9378\nFinal Validation Recall on Combined Tasks with ENHANCED GAT: 0.9459\n","output_type":"stream"}]}]}